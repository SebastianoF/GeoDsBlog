[
  {
    "objectID": "posts/index_gds.html",
    "href": "posts/index_gds.html",
    "title": "Geospatial Data Science",
    "section": "",
    "text": "Python datetimes manipulations\n\n\nCode fragments for timestamps and timezones manipulation\n\n\n\n\n\nJan 24, 2024\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\nHaversine’s distance mathematics\n\n\nComputing the distance between two points on a sphere\n\n\n\n\n\nJan 13, 2024\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\nUsing polygons\n\n\nGeospatial data science basic operations and measurements\n\n\n\n\n\nOct 3, 2022\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\nHow well positioned is your office?\n\n\nTime to question your workplace location\n\n\n\n\n\nJul 29, 2022\n\n\n\n\n\n  \n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/gds-2024-01-24-timestamp/index.html#summary",
    "href": "posts/gds-2024-01-24-timestamp/index.html#summary",
    "title": "Python datetimes manipulations",
    "section": "Summary",
    "text": "Summary\nThis blog post collects in one place a range of methods for converting datetime objects across types in python and pandas.\nThe code snippet and the subsequent diagram summarise the data casting between datetime, int, str, pd.Timestamp.\nThe remaining sections offer more details and examples about each of those methods. At the end of the post you will find a section about some snippets to manipulate timezones in all the examined formats.\n\nimport datetime as dt\nimport pandas as pd\n\n# create a datetime\ndt_event = dt.datetime(year=2024, month=1, day=22, hour=13, minute=55, second=12)\n\n# convert to string with specific format code\nFORMAT_CODE = \"%m/%d/%Y, %H:%M:%S\"\ndt_event_str = dt_event.strftime(FORMAT_CODE)\n\n# convert from string to datetime\ndt_event = dt.datetime.strptime(dt_event_str, FORMAT_CODE)\n\n# convert from datetime to integer (milliseconds)\nmsec_event = int(dt_event.timestamp() * 1000)\n\n# convert from milliseconds to integer\ndt_event = dt.datetime.fromtimestamp(msec_event / 1000)\n\n# create a list of timestamps\ndt_start, dt_end = dt.datetime(2022, 3, 1), dt.datetime(2022, 3, 10)\n \nlist_events = []\n\nwhile dt_start &lt;= dt_end:\n    list_events.append(dt_start)\n    dt_start += dt.timedelta(days=1)\n\n# convert list of timestamps to pandas DatetimeIdex of pd.Timestamps\nindex_events = pd.to_datetime(list_events)\n\n# convert pandas DatetimeIdex of pd.Timestamps to list of timestamps\nlist_events = [dt.datetime.combine(x.date(), x.time()) for x in index_events]\n\n\n# convert pandas DatetimeIdex of pd.Timestamps to an Index of integers (nanoseconds)\nindex_events_int = index_events.astype(int)\n\n# convert pandas Index of integers (nanoseconds) to DatetimeIdex of pd.Timestamps\nindex_events = pd.to_datetime(index_events_int)\n\n# convert pandas DatetimeIdex of pd.Timestamps to an Index of strings\nindex_events_str = index_events.astype(str)\n\n# convert pandas Index of strings to DatetimeIdex of pd.Timestamps\nindex_events = pd.to_datetime(index_events_str)"
  },
  {
    "objectID": "posts/gds-2024-01-24-timestamp/index.html#introduction",
    "href": "posts/gds-2024-01-24-timestamp/index.html#introduction",
    "title": "Python datetimes manipulations",
    "section": "Introduction",
    "text": "Introduction\nWhen first started working with time series, a recurrent task was for me to google up ways to convert datetimes across python and pandas data types. It took me some times to get to have a grasp about the possibilities and conversion methods, as well as how to set and change timezones without making mistakes.\nThis blog post is for you to get the same information I got browsing for a while around various stackoverflows and documentation pages, all in one place.\nAs it is for manipulations of polygons, handling datetimes is another recurring task in geospatial data science.\nIn python there are multiple ways to encode a point in time according to a range of criteria:\n\nReadability\nStoring space\nOut of the box methods\n\nBefore delving into some examples, let’s warm up with a code snippet showing what can go wrong if dates and times are manipulated carelessly.\n\nimport datetime as dt\n\nFORMAT = \"%H:%M:%S%z %Y-%m-%d\"\n\nevent_1 = dt.datetime.strptime(\"22:15:00+00:00 2022-05-01\", FORMAT)\nevent_2 = dt.datetime.strptime(\"18:30:00+00:00 2022-06-01\", FORMAT)\n\nprint(f\"Events as type {type(event_1)}\")\nprint(f\"Event 1: {event_1}\")\nprint(f\"Event 2: {event_2}\")\n\nif event_1 &lt; event_2: \n    print(f\"\\nDatetime {event_1} comes before {event_2}\")\n\n\nevent_1 = event_1.strftime(FORMAT)\nevent_2 = event_2.strftime(FORMAT)\n\nprint(f\"\\nEvents as type {type(event_1)}\")\nprint(f\"Event 1: {event_1}\")\nprint(f\"Event 2: {event_2}\")\n\n\nif event_1 &gt; event_2: \n    print(f\"\\nDatetime {event_1} comes after {event_2}.\")\n    print(\"Oh, Really?\")\n\nEvents as type &lt;class 'datetime.datetime'&gt;\nEvent 1: 2022-05-01 22:15:00+00:00\nEvent 2: 2022-06-01 18:30:00+00:00\n\nDatetime 2022-05-01 22:15:00+00:00 comes before 2022-06-01 18:30:00+00:00\n\nEvents as type &lt;class 'str'&gt;\nEvent 1: 22:15:00+0000 2022-05-01\nEvent 2: 18:30:00+0000 2022-06-01\n\nDatetime 22:15:00+0000 2022-05-01 comes after 18:30:00+0000 2022-06-01.\nOh, Really?\n\n\nCan you see what went wrong?\nIn this example we started from two datetimes (any representation of a date and a time) in string format. These were converted to instances of datetime.datetime from the standard library datetime.\nThe first odd thing, that becomes evident when calling type(dt.datetime.strptime(\"22:15:00+00:00 2022-05-01\", FORMAT)) is that datetime.datetime is not a class. It is a type, like int and str, so it does not follow the camel case convention1.\nAlso that the type name and the library name are the same. So, in some modules you will find from datetime import datetime and in other simply import datetime, a confusion that I avoid usually aliasing the library datetime to dt, as done in the previous example.\nFrom the diagram shown, we can see another source of possible confusions: in the python world the term timestamp refers to a datetime object converted to an integer — more details in the section later about how this is done. In the pandas world instead, the term timestamp is used as the class that encodes the pandas datetime object. So there is a change in the naming convention if you are using the python standards libraries, or if you are using pandas.\nStarting from the center of the diagram, in each section of this post we will explore a conversion and the specifications of the given types, leaving pandas.Timestamp for last."
  },
  {
    "objectID": "posts/gds-2024-01-24-timestamp/index.html#datetime.datetime-to-str-and-back",
    "href": "posts/gds-2024-01-24-timestamp/index.html#datetime.datetime-to-str-and-back",
    "title": "Python datetimes manipulations",
    "section": "datetime.datetime to str and back",
    "text": "datetime.datetime to str and back\nThere are many formats to represent a datetime into a string. One of the standard is specified in ISO8601.\n\n# create a datetime - you need at least year, month and day.\ndt_event = dt.datetime(year=2024, month=1, day=22)\ndt_event\n\ndatetime.datetime(2024, 1, 22, 0, 0)\n\n\n\n# create a datetime with hour, minutes and seconds\ndt_event = dt.datetime(year=2024, month=1, day=22, hour=13, minute=55, second=12)\ndt_event\n\ndatetime.datetime(2024, 1, 22, 13, 55, 12)\n\n\n\n# convert to string with a range of formats\nprint(dt_event.strftime(\"%m/%d/%Y, %H:%M:%S\"))\nprint(dt_event.strftime(\"%Y-%m-%d, %H:%M\"))\nprint(dt_event.strftime(\"%H:%M\"))\n\n01/22/2024, 13:55:12\n2024-01-22, 13:55\n13:55\n\n\nOnce we have the string we can move back to the timestamp object, passing the same format code (this operation is not bijective if some of the information is not included in the chosen format code).\nMore info about the format codes and what each letter means is available at the official documentation https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior.\n\n# Create a datetime object\ndt_event = dt.datetime(year=2024, month=1, day=22, hour=13, minute=55, second=12, microsecond=120)\nprint(f\"Datetime: {dt_event}\")\n\n# convert to string\nFORMAT_CODE = \"%m/%d/%Y, %H:%M:%S\"\nstr_event = dt_event.strftime(FORMAT_CODE)\nprint(f\"String:   {dt_event}\")\n\n# string to datetime\nprint(f\"Datetime: {dt.datetime.strptime(str_event, FORMAT_CODE)}\")\n\nDatetime: 2024-01-22 13:55:12.000120\nString:   2024-01-22 13:55:12.000120\nDatetime: 2024-01-22 13:55:12\n\n\nIn addition to timestamp.timestamp you can create only the time, the date and a timedelta.\n\nprint(dt.time(hour=4))\nprint(dt.date(year=2024, month=3, day=11))\nprint(dt.timedelta(hours=3))  \n\n# we can not sum a timedelta to a time\ntry:\n    print(dt.time(hour=4) + dt.time(hour=4))\nexcept TypeError as err:\n    print(f\"Error raised: TypeError {err}\")\n\n# nor to a date\ntry:\n    print(dt.date(year=2024, month=3, day=11) + dt.time(hour=4))\nexcept TypeError as err:\n    print(f\"Error raised: TypeError {err}\")\n\n# but we can sum it to a timestamp\nprint(dt.datetime(year=2024, month=3, day=11, hour=3) + dt.timedelta(hours=4))\n\n04:00:00\n2024-03-11\n3:00:00\nError raised: TypeError unsupported operand type(s) for +: 'datetime.time' and 'datetime.time'\nError raised: TypeError unsupported operand type(s) for +: 'datetime.date' and 'datetime.time'\n2024-03-11 07:00:00\n\n\n\n# if we quickly want to have the timestamp converted to a string in isoformat we can use:\ndt_event.isoformat()\n\n'2024-01-22T13:55:12.000120'"
  },
  {
    "objectID": "posts/gds-2024-01-24-timestamp/index.html#datetime.datetime-to-int-and-back",
    "href": "posts/gds-2024-01-24-timestamp/index.html#datetime.datetime-to-int-and-back",
    "title": "Python datetimes manipulations",
    "section": "datetime.datetime to int and back",
    "text": "datetime.datetime to int and back\nThe integer representation of a datetime is called timestamp in the python world. We note again that it is not the same as the the pandas Timestamp object.\nA timestamp is a numerical representation of a date in the Unix epoch (or Unix time or POSIX time or Unix timestamp). It is the number of seconds that have elapsed since January 1, 1970 (midnight UTC/GMT), not counting leap seconds (using the ISO 8601 convention it is 1970-01-01T00:00:00Z).\n\ndt_event = dt.datetime(year=2024, month=3, day=11, hour=3)\ndt_event.timestamp?\n\nDocstring: Return POSIX timestamp as float.\nType:      builtin_function_or_method\n\n\nOut of the box, the python timestamp is in seconds, and encoded as a float. In some dataset, and to save memory, it can be encoded in milliseconds or even in microseconds, and encoded as an integer.\n\nprint(f\"Datetime:         {dt_event}\")\nprint(f\"Python timestamp: {dt_event.timestamp()}\")\n\nDatetime:         2024-03-11 03:00:00\nPython timestamp: 1710126000.0\n\n\n\nISO_FORMAT_CODE = \"%Y-%m-%dT%H:%M:%S%z\"\ndt_event_zero = dt.datetime.strptime(\"1970-01-01T00:00:00Z\", ISO_FORMAT_CODE)\nprint(f\"Datetime:         {dt_event_zero}\")\nprint(f\"Python timestamp: {dt_event_zero.timestamp()}\")\n\nDatetime:         1970-01-01 00:00:00+00:00\nPython timestamp: 0.0\n\n\n\ndt_event = dt.datetime(year=2024, month=3, day=11, hour=3, minute=13, second=45)\nsec_event = int(dt_event.timestamp())\nmsec_event = int(dt_event.timestamp() * 1000)  # convert to milliseconds before casting\nback_to_dt_event = dt.datetime.fromtimestamp(msec_event / 1000)\n\nprint(f\"Datetime:              {dt_event}\")\nprint(f\"Python timestamp (s):  {sec_event}\")\nprint(f\"Python timestamp (ms): {msec_event}\")\nprint(f\"Back to datetime:      {back_to_dt_event}\")\n\nDatetime:              2024-03-11 03:13:45\nPython timestamp (s):  1710126825\nPython timestamp (ms): 1710126825000\nBack to datetime:      2024-03-11 03:13:45"
  },
  {
    "objectID": "posts/gds-2024-01-24-timestamp/index.html#datetime.datetime-str-and-int-to-pandas.timestamp-and-back",
    "href": "posts/gds-2024-01-24-timestamp/index.html#datetime.datetime-str-and-int-to-pandas.timestamp-and-back",
    "title": "Python datetimes manipulations",
    "section": "datetime.datetime, str and int to pandas.Timestamp and back",
    "text": "datetime.datetime, str and int to pandas.Timestamp and back\nIn pandas we typically don’t deal with a single datetime, but with a list of those. With timeseries, pandas becomes particularly efficient when timeseries are used as data frames indexes.\nHere we see some examples of conversion of lists of datetime.datetime into pandas pd.Timestamp indexes, and into int and string.\n\ndef print_events(iterable_events):\n    \"\"\"helper to print info about iterable events len and types\"\"\"\n    print(f\"{len(iterable_events)} days: \")\n    print(\"[\")\n    for ev in iterable_events:\n        print(f\"    {ev}\")\n    print(\"]\")\n    print(f\"This is a {type(iterable_events)} of {type(iterable_events[0])} elements.\")\n\n\n# Create a list of datetime.datetime\ndt_start = dt.datetime(2022, 3, 1)\ndt_end = dt.datetime(2022, 3, 10)\n \nlist_events = []\n \nwhile dt_start &lt;= dt_end:\n    list_events.append(dt_start)\n    dt_start += dt.timedelta(days=1)\n\nprint_events(list_events)\n\n10 days: \n[\n    2022-03-01 00:00:00\n    2022-03-02 00:00:00\n    2022-03-03 00:00:00\n    2022-03-04 00:00:00\n    2022-03-05 00:00:00\n    2022-03-06 00:00:00\n    2022-03-07 00:00:00\n    2022-03-08 00:00:00\n    2022-03-09 00:00:00\n    2022-03-10 00:00:00\n]\nThis is a &lt;class 'list'&gt; of &lt;class 'datetime.datetime'&gt; elements.\n\n\n\n# Create the same list as above, with pandas.date_range https://pandas.pydata.org/docs/reference/api/pandas.date_range.html\nimport pandas as pd\nimport numpy as np\n\ndt_start = dt.datetime(2022, 3, 1)\ndt_end = dt.datetime(2022, 3, 10)\n \nindex_events = pd.date_range(start=dt_start, end=dt_end)\n\nprint_events(index_events)\n\n10 days: \n[\n    2022-03-01 00:00:00\n    2022-03-02 00:00:00\n    2022-03-03 00:00:00\n    2022-03-04 00:00:00\n    2022-03-05 00:00:00\n    2022-03-06 00:00:00\n    2022-03-07 00:00:00\n    2022-03-08 00:00:00\n    2022-03-09 00:00:00\n    2022-03-10 00:00:00\n]\nThis is a &lt;class 'pandas.core.indexes.datetimes.DatetimeIndex'&gt; of &lt;class 'pandas._libs.tslibs.timestamps.Timestamp'&gt; elements.\n\n\n\n# Assign the index to a dataframe\nimport numpy as np\ndf = pd.DataFrame(np.random.choice(list(\"random\"), size=len(index_events))).set_index(index_events)\ndf\n\n\n\n\n\n\n\n\n0\n\n\n\n\n2022-03-01\nm\n\n\n2022-03-02\nr\n\n\n2022-03-03\nn\n\n\n2022-03-04\no\n\n\n2022-03-05\nr\n\n\n2022-03-06\nd\n\n\n2022-03-07\nm\n\n\n2022-03-08\nn\n\n\n2022-03-09\na\n\n\n2022-03-10\nd\n\n\n\n\n\n\n\n\n# Same pandas index_events created above can be created directly with strings\nindex_events = pd.date_range(start='3/1/2022', end='3/10/2022')\n\nprint_events(index_events)\n\n10 days: \n[\n    2022-03-01 00:00:00\n    2022-03-02 00:00:00\n    2022-03-03 00:00:00\n    2022-03-04 00:00:00\n    2022-03-05 00:00:00\n    2022-03-06 00:00:00\n    2022-03-07 00:00:00\n    2022-03-08 00:00:00\n    2022-03-09 00:00:00\n    2022-03-10 00:00:00\n]\nThis is a &lt;class 'pandas.core.indexes.datetimes.DatetimeIndex'&gt; of &lt;class 'pandas._libs.tslibs.timestamps.Timestamp'&gt; elements.\n\n\n\n# Convert the list of datetimes to the pandas index\nindex_events_converted = pd.to_datetime(list_events)\n\nprint_events(index_events)\n\nif not pd.testing.assert_index_equal(index_events_converted, index_events):\n    print()\n    print(\"The two index events are not considered equal! Why?\")\n\n10 days: \n[\n    2022-03-01 00:00:00\n    2022-03-02 00:00:00\n    2022-03-03 00:00:00\n    2022-03-04 00:00:00\n    2022-03-05 00:00:00\n    2022-03-06 00:00:00\n    2022-03-07 00:00:00\n    2022-03-08 00:00:00\n    2022-03-09 00:00:00\n    2022-03-10 00:00:00\n]\nThis is a &lt;class 'pandas.core.indexes.datetimes.DatetimeIndex'&gt; of &lt;class 'pandas._libs.tslibs.timestamps.Timestamp'&gt; elements.\n\nThe two index events are not considered equal! Why?\n\n\nHere something interesting have happened. Even if the two series have the same values, they are not equal.\nPrinting them individually will show why:\n\nindex_events\n\nDatetimeIndex(['2022-03-01', '2022-03-02', '2022-03-03', '2022-03-04',\n               '2022-03-05', '2022-03-06', '2022-03-07', '2022-03-08',\n               '2022-03-09', '2022-03-10'],\n              dtype='datetime64[ns]', freq='D')\n\n\n\nindex_events_converted\n\nDatetimeIndex(['2022-03-01', '2022-03-02', '2022-03-03', '2022-03-04',\n               '2022-03-05', '2022-03-06', '2022-03-07', '2022-03-08',\n               '2022-03-09', '2022-03-10'],\n              dtype='datetime64[ns]', freq=None)\n\n\nThe freq= value of the converted frequency is different from the frequency of the original one, that is defined by days. Under the hood they are different objects, as the first one contains first and last element and the frequency, and the second contains every single individual element.\nEven assigning the frequency to index_events_converted afterwards does not make the two DateteimeIndex equal:\n\nindex_events_converted.freq = \"D\"\nindex_events_converted\n\nDatetimeIndex(['2022-03-01', '2022-03-02', '2022-03-03', '2022-03-04',\n               '2022-03-05', '2022-03-06', '2022-03-07', '2022-03-08',\n               '2022-03-09', '2022-03-10'],\n              dtype='datetime64[ns]', freq='D')\n\n\n\nif not pd.testing.assert_index_equal(index_events_converted, index_events):\n    print()\n    print(\"The two index events are still not equal!\")\n\n\nThe two index events are still not equal!\n\n\nWhen working with pd.Timestamp we have to keep in mind that different constructors creating the same numerical results produce different objects.\nWhat about the memory? Also this is the same for both objects:\n\nprint(index_events.memory_usage())\nprint(index_events_converted.memory_usage())\n\n80\n80\n\n\nNow we can convert back from a pandas DatetimeIndex to a list of datetime.datetime objects.\nA pd.Timestamp object can not be converted directly into a datetime.datetime object, but it can be converted into a datetime.date and a datetime.time object.\nSo we can use a list comprehension and combine the two exported formats:\n\n# Create a Pandas Timestamp object\npd_event = pd.Timestamp('2022-06-18 12:34:56')\n\n# Convert the Timestamp to a Python datetime.date object\ndate = pd_event.date()\ntime = pd_event.time()\ndt_event = dt.datetime.combine(date, time)\n\nprint(f\"Event {pd_event} of type {type(pd_event)}\")\nprint(f\"Event {dt_event} of type {type(dt_event)}\")\n\nEvent 2022-06-18 12:34:56 of type &lt;class 'pandas._libs.tslibs.timestamps.Timestamp'&gt;\nEvent 2022-06-18 12:34:56 of type &lt;class 'datetime.datetime'&gt;\n\n\n\nindex_event_converted_back = [dt.datetime.combine(x.date(), x.time()) for x in index_events_converted]\nprint_events(index_event_converted_back)\n\n10 days: \n[\n    2022-03-01 00:00:00\n    2022-03-02 00:00:00\n    2022-03-03 00:00:00\n    2022-03-04 00:00:00\n    2022-03-05 00:00:00\n    2022-03-06 00:00:00\n    2022-03-07 00:00:00\n    2022-03-08 00:00:00\n    2022-03-09 00:00:00\n    2022-03-10 00:00:00\n]\nThis is a &lt;class 'list'&gt; of &lt;class 'datetime.datetime'&gt; elements.\n\n\n\nPandas Timestamp to int\nTo convert a pandas DatetimeIndex from pd.Timestamp to integer, we have to use the .astype method of a pandas series.\n\nprint(index_events)\nindex_events_int = index_events.astype(int)\nprint()\nprint(index_events_int)\n\nDatetimeIndex(['2022-03-01', '2022-03-02', '2022-03-03', '2022-03-04',\n               '2022-03-05', '2022-03-06', '2022-03-07', '2022-03-08',\n               '2022-03-09', '2022-03-10'],\n              dtype='datetime64[ns]', freq='D')\n\nIndex([1646092800000000000, 1646179200000000000, 1646265600000000000,\n       1646352000000000000, 1646438400000000000, 1646524800000000000,\n       1646611200000000000, 1646697600000000000, 1646784000000000000,\n       1646870400000000000],\n      dtype='int64')\n\n\nThe timestamps are now in nanoseconds, as specified in the dtype of the DatetimeIndex index_events. You have to divide it by 1e9 to have them in seconds:\n\nindex_events_int_sec = (index_events_int / 1e9).astype(int)\nindex_events_int_sec\n\nIndex([1646092800, 1646179200, 1646265600, 1646352000, 1646438400, 1646524800,\n       1646611200, 1646697600, 1646784000, 1646870400],\n      dtype='int64')\n\n\n\n# convert back to `DatetimeIndex`\npd.to_datetime(index_events_int)\n\nDatetimeIndex(['2022-03-01', '2022-03-02', '2022-03-03', '2022-03-04',\n               '2022-03-05', '2022-03-06', '2022-03-07', '2022-03-08',\n               '2022-03-09', '2022-03-10'],\n              dtype='datetime64[ns]', freq=None)\n\n\n\n# convert back to `DatetimeIndex`. Pandas expects nanoseconds\npd.to_datetime(index_events_int_sec)\n\nDatetimeIndex(['1970-01-01 00:00:01.646092800',\n               '1970-01-01 00:00:01.646179200',\n               '1970-01-01 00:00:01.646265600',\n                  '1970-01-01 00:00:01.646352',\n               '1970-01-01 00:00:01.646438400',\n               '1970-01-01 00:00:01.646524800',\n               '1970-01-01 00:00:01.646611200',\n               '1970-01-01 00:00:01.646697600',\n                  '1970-01-01 00:00:01.646784',\n               '1970-01-01 00:00:01.646870400'],\n              dtype='datetime64[ns]', freq=None)\n\n\n\n\nPandas Timestamp to str\nConversion to strings works in the same way as with int.\n\n# convert from DatetimeIndex to string.\nprint(index_events)\nindex_events_str = index_events.astype(str)\nprint()\nprint(index_events_str)\n\nDatetimeIndex(['2022-03-01', '2022-03-02', '2022-03-03', '2022-03-04',\n               '2022-03-05', '2022-03-06', '2022-03-07', '2022-03-08',\n               '2022-03-09', '2022-03-10'],\n              dtype='datetime64[ns]', freq='D')\n\nIndex(['2022-03-01', '2022-03-02', '2022-03-03', '2022-03-04', '2022-03-05',\n       '2022-03-06', '2022-03-07', '2022-03-08', '2022-03-09', '2022-03-10'],\n      dtype='object')\n\n\n\n# convert back from string to DatetimeIndex\npd.to_datetime(index_events_str)\n\nDatetimeIndex(['2022-03-01', '2022-03-02', '2022-03-03', '2022-03-04',\n               '2022-03-05', '2022-03-06', '2022-03-07', '2022-03-08',\n               '2022-03-09', '2022-03-10'],\n              dtype='datetime64[ns]', freq=None)"
  },
  {
    "objectID": "posts/gds-2024-01-24-timestamp/index.html#timezones",
    "href": "posts/gds-2024-01-24-timestamp/index.html#timezones",
    "title": "Python datetimes manipulations",
    "section": "Timezones",
    "text": "Timezones\nSome basic concepts first:\n\nUTC universal time coordinates: is the standard to assign to each country in the world a timezone, that is an offset in respect to the timezone zero, also called Zulu or GTM.\nGMT Greenwich Mean Time: is the time zone at the meridian zero, passing at Greenwich in England. The UTC standard tells you how many time zone you are away from Greenwich.\nZulu for the NATO alphabet letter Z, was originally indicating the letter z as in “zero meridian”. This is an equivalent of GTM and it is commonly used in the army and in the civilian aviation. The final “zulu” in a radio message refers to the fact that the time is not read in local time. For example 02:00am at GMT, would read “zero two hundred zulu” in a radio communication.\n\nThe names of the timezones are typically the UTC offsets, written as suffix to the timestamps, such as +02:00 or -05:00, though some particularly recurring timezones have their names, such as:\n\nEastern Standard Time (EST): UTC-0500 or 5 hours behind GMT.\nCentral Standard Time (CST): UTC-0600 or 6 hours behind GMT.\nMountain Standard Time (MST): UTC-0700 or 7 hours behind GMT.\nPacific Standard Time (PST): UTC-0800 or 8 hours behind GMT.\n\n\nTimezones in timestamps\nWhen timezones are written as offsets and appended to the timestamps, the are also summed or subtracted to the datetime itself.\nFor example to write 2023-02-21T06:07:21+03:30 in GMT, we have to set the offset to zero and subtract the offset: 2023-02-21T02:37:21+00:00, that in the ISO 8601 becomes 2023-02-21T02:37:21Z, with in the Zulu timezone, and in a convention that is often found in public datasets as 2023-02-21 02:37:21 UTC, conflating the concepts of GMT and UTC.\nAnother way of encoding the timezones is with the name of a representative city where the timezone belongs to. +03:30 does not tell much about where we are, if compared to \"Asia/Tehran\".\n\n\nNo timezone given\nWhen a timezone is not specified, the timestamp is said to be naive, or not timezone-aware.\nIn this case it is important to check with the data provider if the datetime are given in local time or at GMT. If the datetime are given as integer UNIX timestamps, then they are at GMT, unless there was a mistake in the data ingestion.\n\nimport datetime as dt\nfrom dateutil import tz\n\n# naive\nstr_event = \"2023-02-21 02:37:21\"\n\ndt_event = dt.datetime.strptime(str_event, \"%Y-%m-%d %H:%M:%S\")\nprint(f\"naive datetime:  {dt_event.isoformat()}\")\n\n# GTM\nstr_event = \"2023-02-21 02:37:21 GMT\"  # works also with UTC instead of GMT\n\ndt_event = dt.datetime.strptime(str_event, \"%Y-%m-%d %H:%M:%S %Z\")\nprint(f\"GTM datetime:    {dt_event.isoformat()}\")\n\n# zero offset\nstr_event = \"2023-02-21 02:37:21+00:00\"\n\ndt_event = dt.datetime.strptime(str_event, \"%Y-%m-%d %H:%M:%S%z\")\nprint(f\"datetime +00:00: {dt_event.isoformat()}\")\n\n# +1h offset\nstr_event = \"2023-02-21 03:37:21+01:00\"\n\ndt_event = dt.datetime.strptime(str_event, \"%Y-%m-%d %H:%M:%S%z\")\nprint(f\"datetime +01:00: {dt_event.isoformat()}\")\n\nnaive datetime:  2023-02-21T02:37:21\nGTM datetime:    2023-02-21T02:37:21\ndatetime +00:00: 2023-02-21T02:37:21+00:00\ndatetime +01:00: 2023-02-21T03:37:21+01:00\n\n\nThe code above should be enough to convince you that when the offset is not specified, then we can assume the datetime is at GMT, as it was at GMT when we passed as a string.\n\n\nTimezones as capital cities\nBelow, with dateutil we can also pass the offset with the capital city based offset, such as “Asia/Tehran”.\n\nfrom dateutil import tz\n\n# country/city offset\nstr_event = \"2023-02-21 03:37:21\"\nstr_offset = \"Asia/Tehran\"\n\ndt_event = dt.datetime.strptime(str_event, \"%Y-%m-%d %H:%M:%S\")\nprint(f\"datetime: {dt_event.isoformat()}\")\ndt_event = dt_event.astimezone(tz.gettz(str_offset))\nprint(f\"datetime: {dt_event.isoformat()}\")\n\ndatetime: 2023-02-21T03:37:21\ndatetime: 2023-02-21T07:07:21+03:30\n\n\nIf you use this solution be aware that there is no validation, and if the string is misspelled or non existing, the datetime will be assigned the default +00:00 offset.\n\ndt_event = dt.datetime.strptime(str_event, \"%Y-%m-%d %H:%M:%S\")\nprint(f\"datetime: {dt_event.isoformat()}\")\n\ncorrect = \"Asia/Tehran\"\ndt_event = dt_event.astimezone(tz.gettz(correct))\nprint(f\"datetime in {correct}: {dt_event.isoformat()}\")\n\nmisspelled = \"Asia/Theran\"\ndt_event = dt_event.astimezone(tz.gettz(misspelled))\nprint(f\"datetime in {misspelled}: {dt_event.isoformat()}\")\n\nplain_wrong = \"MiddleEarth/County\"\ndt_event = dt_event.astimezone(tz.gettz(plain_wrong))\nprint(f\"datetime in {plain_wrong}: {dt_event.isoformat()}\")\n\ndatetime: 2023-02-21T03:37:21\ndatetime in Asia/Tehran: 2023-02-21T07:07:21+03:30\ndatetime in Asia/Theran: 2023-02-21T03:37:21+00:00\ndatetime in MiddleEarth/County: 2023-02-21T03:37:21+00:00\n\n\nNot raising an error for non existing capitals is not ideal. So rather than assigning the timezone dateutil I would recommend the method timezone from pytz.\n\nfrom pytz import timezone\n\ndt_event = dt.datetime.strptime(str_event, \"%Y-%m-%d %H:%M:%S\")\n\ncorrect = \"Asia/Tehran\"\ndt_event = dt_event.astimezone(tz.gettz(correct))\nprint(f\"datetime in {correct}: {dt_event.isoformat()}\")\n\nplain_wrong = \"MiddleEarth/County\"\ntry:\n    dt_event.astimezone(timezone(plain_wrong))\n    print(f\"datetime in {plain_wrong}: {dt_event.isoformat()}\")\nexcept Exception as err:\n    print(f\"Error raised: {type(err)} {err}\")\n\ndatetime in Asia/Tehran: 2023-02-21T07:07:21+03:30\nError raised: &lt;class 'pytz.exceptions.UnknownTimeZoneError'&gt; 'MiddleEarth/County'\n\n\n\n\nTimezones in pandas\nAlso to the pd.Timestamp object can be assigned a timezone with tz_localize and can be converted from one timezone to another with tz_convert (this last works only with the non-naive timestamps).\n\nindex_events = pd.date_range(start='3/1/2022', end='3/10/2022')\n\n\ntry:\n    index_events.tz_convert('US/Pacific')\nexcept TypeError as err:\n    print(f\"We can not use tz_convert directly. Error raised: {err}\")\n\nWe can not use tz_convert directly. Error raised: Cannot convert tz-naive timestamps, use tz_localize to localize\n\n\n\nindex_events_localised = index_events.tz_localize('US/Pacific')\nprint_events(index_events_localised)  # note that the datetimes are not shifted! They are considered in local time at the specified location\n\n10 days: \n[\n    2022-03-01 00:00:00-08:00\n    2022-03-02 00:00:00-08:00\n    2022-03-03 00:00:00-08:00\n    2022-03-04 00:00:00-08:00\n    2022-03-05 00:00:00-08:00\n    2022-03-06 00:00:00-08:00\n    2022-03-07 00:00:00-08:00\n    2022-03-08 00:00:00-08:00\n    2022-03-09 00:00:00-08:00\n    2022-03-10 00:00:00-08:00\n]\nThis is a &lt;class 'pandas.core.indexes.datetimes.DatetimeIndex'&gt; of &lt;class 'pandas._libs.tslibs.timestamps.Timestamp'&gt; elements.\n\n\n\nprint_events(index_events_localised.tz_convert('GMT'))\n\n10 days: \n[\n    2022-03-01 08:00:00+00:00\n    2022-03-02 08:00:00+00:00\n    2022-03-03 08:00:00+00:00\n    2022-03-04 08:00:00+00:00\n    2022-03-05 08:00:00+00:00\n    2022-03-06 08:00:00+00:00\n    2022-03-07 08:00:00+00:00\n    2022-03-08 08:00:00+00:00\n    2022-03-09 08:00:00+00:00\n    2022-03-10 08:00:00+00:00\n]\nThis is a &lt;class 'pandas.core.indexes.datetimes.DatetimeIndex'&gt; of &lt;class 'pandas._libs.tslibs.timestamps.Timestamp'&gt; elements.\n\n\nFor a given dataset of datetime in GMT, we have first to localise them over Greenwich, and then convert to the local timezone.\n\nindex_events = pd.date_range(start='3/1/2022', end='3/10/2022').tz_localize(\"GMT\")\nprint_events(index_events.tz_convert('US/Pacific'))\n\n10 days: \n[\n    2022-02-28 16:00:00-08:00\n    2022-03-01 16:00:00-08:00\n    2022-03-02 16:00:00-08:00\n    2022-03-03 16:00:00-08:00\n    2022-03-04 16:00:00-08:00\n    2022-03-05 16:00:00-08:00\n    2022-03-06 16:00:00-08:00\n    2022-03-07 16:00:00-08:00\n    2022-03-08 16:00:00-08:00\n    2022-03-09 16:00:00-08:00\n]\nThis is a &lt;class 'pandas.core.indexes.datetimes.DatetimeIndex'&gt; of &lt;class 'pandas._libs.tslibs.timestamps.Timestamp'&gt; elements."
  },
  {
    "objectID": "posts/gds-2024-01-24-timestamp/index.html#footnotes",
    "href": "posts/gds-2024-01-24-timestamp/index.html#footnotes",
    "title": "Python datetimes manipulations",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn Python &gt;3 the concepts of class and type was unified, and pure type objects are now only the native ones.↩︎"
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Posts",
    "section": "",
    "text": "Python datetimes manipulations\n\n\nCode fragments for timestamps and timezones manipulation\n\n\n\n\n\nJan 24, 2024\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\nHaversine’s distance mathematics\n\n\nComputing the distance between two points on a sphere\n\n\n\n\n\nJan 13, 2024\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\nUsing polygons\n\n\nGeospatial data science basic operations and measurements\n\n\n\n\n\nOct 3, 2022\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\nHow well positioned is your office?\n\n\nTime to question your workplace location\n\n\n\n\n\nJul 29, 2022\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\nHow to sign your commits\n\n\nSetting up gpg authentication while keeping separate work and personal projects\n\n\n\n\n\nJul 23, 2022\n\n\n  \n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/gds-2022-10-03-polygons-part-1/index.html",
    "href": "posts/gds-2022-10-03-polygons-part-1/index.html",
    "title": "Using polygons",
    "section": "",
    "text": "This article was first published on the 10 October 2022 on geods.hashnode.dev. The version presented here is the maintained one."
  },
  {
    "objectID": "posts/gds-2022-10-03-polygons-part-1/index.html#topics-of-this-part-and-the-next-two",
    "href": "posts/gds-2022-10-03-polygons-part-1/index.html#topics-of-this-part-and-the-next-two",
    "title": "Using polygons",
    "section": "Topics of this part and the next two",
    "text": "Topics of this part and the next two\nThis is the first article of a series of three. The topics covered in the three parts are:\n\nPart 1: Definitions: intersections, operations and measurements of polygons\n\nPolygons intersecting water and land.\n…and with water and land nearby.\nWhich country intersect this polygon?\nDistances between countries.\nSame country, different borders: how different are they?\n\nPart 2: Reference systems for polygons\n\nChanging reference system\nPolygons crossing the antimeridian\nWhat at the poles.\n\nPart 3: Random polygons and wandering polygons\n\nCreating random polygons\nFloating polygons\nPolygons deformed by a vector field"
  },
  {
    "objectID": "posts/gds-2022-10-03-polygons-part-1/index.html#setting-up-the-environment",
    "href": "posts/gds-2022-10-03-polygons-part-1/index.html#setting-up-the-environment",
    "title": "Using polygons",
    "section": "Setting up the environment",
    "text": "Setting up the environment\nCreate a python 3.9 environment called venv and activate it. There are several options, the code below is to create an environment via virtualenv, as quicker than conda, and more than enough for these small experiments:\nvirtualenv venv -p python3.9\nsource venv/bin/activate\nWith the environment activated, install the requirements below. You can install each library individually with pip pip install &lt;copy paste each line here&gt;, or you can copy paste all the requirements in a file in the root of your project requirements.txt and install them all in one go via pip install -r requirements.txt.\n# requirements.txt\ngeopandas==0.11.1\njupyter==1.0.0\nkeplergl==0.3.2\nmatplotlib==3.6.0\nosmnx==1.2.2\nshapely==1.8.4"
  },
  {
    "objectID": "posts/gds-2022-10-03-polygons-part-1/index.html#polygons-intersecting-water-and-land",
    "href": "posts/gds-2022-10-03-polygons-part-1/index.html#polygons-intersecting-water-and-land",
    "title": "Using polygons",
    "section": "Polygons intersecting water and land",
    "text": "Polygons intersecting water and land\nLand and water on a map are typically modelled by polygons through the segmentation of the underlying geographical features.\nGiven a polygon or a bounding box drawn on a map as input, the goal is to answer the question\n\nWhat is the percentage of land and water that the polygon drawn on the map intersects?\n\nFor the example below we will use a polygon drawn around the city of Genova, Italy, (44.405551, 8.904023). As usual you are invited to reproduce the results using a different city.\n\nfrom copy import deepcopy\n\nfrom IPython.display import Image\n\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport osmnx\nimport shapely\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action='ignore', category=DeprecationWarning)\n\nimport pandas as pd\n\nfrom geopy import distance\nfrom keplergl import KeplerGl\nfrom shapely.geometry import Polygon, Point, shape\n\n\nKEPLER_OUTPUT = False  # Render kepler output if True. Produces a screenshot for the blog article otherwise\n\n\nif KEPLER_OUTPUT:\n    map_0 = KeplerGl(height=800)\n    display(map_0)\nelse:\n    display(Image(\"images/map0.png\"))\n\n\n\n\n\n\n\n\nIn an interactive session the cell above produces creates an empty map where to draw a polygon and copy paste it in the cell below. In the static blog version this is not reproduced, as the variable KEPLER_OUTPUT is set to False.\nYou can get at the source notebook used to create this blog post and create a different polygon, using the drawing pencil (top right), then right click on it to copy the values and paste it in a dictionary.\n\ndict_roi = {\"type\":\"Polygon\",\"coordinates\":[[[8.987382009236233,44.54680601507832],[8.841126105836173,44.550231465826634],[8.723022747221052,44.53065474626796],[8.621398927017932,44.48511343018655],[8.591873087363503,44.3865638718846],[8.64817817693631,44.307506289856086],[8.75666847147733,44.33796344890003],[8.83906616353428,44.35613196160054],[8.950989695244745,44.3531860988552],[9.023087675794352,44.30554076888227],[9.079392765366244,44.274083482671955],[9.120591611394707,44.288339652995674],[9.143937624144632,44.33108740800724],[9.141877681842741,44.39784910785984],[9.090379124307875,44.49344084451141],[8.987382009236233,44.54680601507832]]]}\n\nsh_roi = shape(dict_roi)\n\nconfig_map1 = {\n    'version': 'v1',\n    'config': {\n        'mapState': {\n            \"bearing\": 0,\n            \"dragRotate\": False,\n            \"latitude\": 44.41404004333898,\n            \"longitude\": 8.871549089955757,\n            \"pitch\": 0,\n            \"zoom\": 10.347869665266995,\n            \"isSplit\": False,\n        }\n    }\n}\n\ngdf_roi =  gpd.GeoDataFrame({\"name\":[\"ROI\"], \"geometry\": [sh_roi]})\ngdf_roi = gdf_roi.set_crs('4326')  # reproject to WGS84 (the only one supported by kepler)\n\n# To reproduce exactly the same images shown in the article you will have to copy the config\n# file from the linked repo and save it in the save it in a configs.py file.\n# You can still follow the tutorial and run all the lines of code without the config.\ntry:\n    from configs import config_map1\nexcept ImportError:\n    config_map1 = dict()\n    \nif KEPLER_OUTPUT:\n    map_1 = KeplerGl(data=deepcopy({\"roi\": gdf_roi}), config=config_map1, height=800)\n    display(map_1)\nelse:\n    display(Image(\"images/map1.png\"))\n\n\n\n\n\n\n\n\nThe copy pasted geometry from the kepler app is a dictionary with a type and a list of coordinates following a conventional GeoJSON object. GeoJSON is a format for encoding data about geographic features using JavaScript Object Notation (json), established in 2015.\nA geojson to model a single point appears as:\n{\n  \"type\": \"Feature\",\n  \"geometry\": {\n    \"type\": \"Point\",\n    \"coordinates\": [125.6, 10.1]\n  },\n  \"properties\": {\n    \"name\": \"Dinagat Islands\"\n  }\n}\nWhere \"type\" can be Feature for single objects, or FeatureCollections for multiple objects, and where the geometry.type can be a Point, LineString, Polygon, MultiPoint, MultiLineString, and MultiPolygon.\nTo get the “water” within the selected geometry, we will use the osmnx library.\nThere are a few options to obtain land and water intersecting a region. It is possible to get the rivers, the sea (or bay), and the administrative regions and then consider to subtract the rivers to the administrative regions, and their intersection with the selected ROI. This option may leave some empty spaces, as in OSM there may be multiple annotators and the boundaries may not collimate. A different approach is to assume that all what is not water is land. So we query the rivers and the sea, we consider their union and their intersection with the ROI, and we call this new region “water”. The land will be the set difference between water and the ROI.\n\\[\n\\text{water}_{\\text{roi}} = (\\text{sea} \\cup \\text{river}) \\cap \\text{roi}\\\\\n\\text{land}_{\\text{roi}} = \\text{roi} \\setminus \\text{water}_{\\text{roi}}\n\\]\nSo we need to perform the binary polygon operations of union, intersection and subtraction of polygons, as well as to get the polygons of sea and rivers involved in the selected roi.\n\n# how to get tags:\n# https://wiki.openstreetmap.org/wiki/Map_features#Water_related\n\n# ~5 mins the first run (2 seconds after the first run, as results are stored in the local folder `cache`).\ngdf_rivers = osmnx.geometries_from_polygon(sh_roi, tags={'natural': 'water'})\ngdf_sea = osmnx.geometries_from_polygon(sh_roi, tags={'natural': 'bay'})\n\n# check crs is already set to WGS84\nassert gdf_rivers.crs == 4326\nassert gdf_sea.crs == 4326\n\n# Dissolve the \"geodataframe\" so that all of its rows are conflated into a single observation\ngdf_rivers = gdf_rivers.dissolve()\ngdf_sea = gdf_sea.dissolve()\n\n# Compute water_roi and land_roi\ngse_sea_union_river = gdf_rivers.union(gdf_sea)  # The union of two geodataframes is a geoseries (the union is only on the geometries)\n\ngse_water_roi = gse_sea_union_river.intersection(gdf_roi)\ngse_land_roi = gdf_roi.difference(gse_water_roi)\n\nyou can also explore the administrative boundaries for an alternative route to find the polygons segmenting land.\ngdf_region = ox.geometries_from_polygon(sh_poly, tags={'boundary': 'administrative'})\nBut this route is not explored here, you are invited to investigate and plot it.\n\n# Call deepcopy when passing an object prevents issue \"AttributeError: 'str' object has no attribute '_geom'\".\n# https://github.com/keplergl/kepler.gl/issues/1240\n\ngdf_genova = gpd.GeoDataFrame(\n    {\n        \"name\": [\"roi\", \"water\", \"land\"], \n        \"geometry\":[deepcopy(gdf_roi.geometry.iloc[0]), deepcopy(gse_water_roi.iloc[0]), deepcopy(gse_land_roi.iloc[0])]\n    }\n)\n\ngdf_genova = gdf_genova.set_crs('4326')\n\ndisplay(gdf_genova.head())\n\n\n\n\n\n\n\n\nname\ngeometry\n\n\n\n\n0\nroi\nPOLYGON ((8.98738 44.54681, 8.84113 44.55023, ...\n\n\n1\nwater\nMULTIPOLYGON (((8.62514 44.37508, 8.62505 44.3...\n\n\n2\nland\nMULTIPOLYGON (((8.98738 44.54681, 9.04759 44.5...\n\n\n\n\n\n\n\n\nkepler_data = {}\nfor _, row in gdf_genova.iterrows():\n    kepler_data.update({row[\"name\"]: gdf_genova[gdf_genova[\"name\"] == row[\"name\"]].copy()})\n\n\ntry:\n    from configs import config_map2\nexcept ImportError:\n    config_map2 = dict()\n\n\nif KEPLER_OUTPUT:\n    map_2 = KeplerGl(\n        data=deepcopy(kepler_data), \n        height=800,\n        config=config_map2,\n    )\n    display(map_2)\nelse:\n    display(Image(\"images/map2.png\"))\n\n\n\n\n\n\n\n\n\n# Reproject to cylindrical equal area projectcion\ngdf_genova[\"areas (Km2)\"] = gdf_genova.to_crs({'proj':'cea'}).area/ 10**6\ndisplay(gdf_genova[[\"name\", \"areas (Km2)\"]])\n\n\n\n\n\n\n\n\nname\nareas (Km2)\n\n\n\n\n0\nroi\n920.966148\n\n\n1\nwater\n330.635071\n\n\n2\nland\n590.333976"
  },
  {
    "objectID": "posts/gds-2022-10-03-polygons-part-1/index.html#polygons-intersecting-water-and-land-nearby",
    "href": "posts/gds-2022-10-03-polygons-part-1/index.html#polygons-intersecting-water-and-land-nearby",
    "title": "Using polygons",
    "section": "Polygons intersecting water and land… nearby!",
    "text": "Polygons intersecting water and land… nearby!\nThere are situations when the manually delineated region may not be as accurate as it is required. It may be required to get some information of the areas “nearby” as well. “Nearby” can have different definitions, and here we get three of them: 1. Convex hull 2. Bounding Box 3. Buffer\nThe convex hull of the polygon \\(\\Omega\\) is the smallest convex polygon containing \\(\\Omega\\). The bounding box is the smallest rectangle containing \\(\\Omega\\) with edges parallel to the axis of the reference system. The buffer consists of enlarging the boundaries of \\(\\Omega\\) in a direction perpendicular to its sides. The analogous operation for raster objects is called dilation and it is one of the two most common morphological operations along with erosion.\nBut first we transform the operations we wrote so far into a function, taking dict_roi as input and returning the equivalent of gdf_genova for the selected area, which we can try for a different geometry. Then we test that the functions work for a different region. We chose the Viverone Lake, you are invited to select another one.\n\ndef water_and_land(roi: dict) -&gt; gpd.GeoDataFrame:\n    \"\"\"from a polygon embedded in a geojson dict to a geodataframe with water, land and respective areas\"\"\"\n    sh_roi = shape(roi)\n    gdf_roi =  gpd.GeoDataFrame({\"name\":[\"ROI\"], \"geometry\": [sh_roi]}).set_crs('4326')\n    gdf_rivers = osmnx.geometries_from_polygon(sh_roi, tags={'natural': 'water'}).dissolve()\n    gdf_sea = osmnx.geometries_from_polygon(sh_roi, tags={'natural': 'bay'}).dissolve()\n    gse_sea_union_river = gdf_rivers.union(gdf_sea) if len(gdf_sea) else gdf_rivers\n    gse_water_roi = gse_sea_union_river.intersection(gdf_roi)\n    gse_land_roi = gdf_roi.difference(gse_water_roi)\n    \n    gdf = gpd.GeoDataFrame(\n        {\n            \"name\": [\"roi\", \"water\", \"land\"], \n            \"geometry\":[deepcopy(gdf_roi.geometry.iloc[0]), deepcopy(gse_water_roi.iloc[0]), deepcopy(gse_land_roi.iloc[0])]\n        }\n    ).set_crs('4326')\n    \n    gdf[\"areas (Km2)\"] = gdf.to_crs({'proj':'cea'}).area/ 10**6\n    \n    return gdf\n\n\ndef split_gdf_to_layers(gdf: gpd.GeoDataFrame, column_name: str = \"name\") -&gt; dict:\n    kepler_data = {}\n    for _, row in gdf.iterrows():\n        kepler_data.update({row[column_name]: gdf[gdf[column_name] == row[column_name]].copy()})\n        \n    return kepler_data\n\n\ntry:\n    from configs import config_map3\nexcept ImportError:\n    config_map3 = dict()\n\nif True:\n    dict_viverone = {\"type\":\"Polygon\",\"coordinates\":[[[8.098304589093475,45.44917237554611],[8.061826507846979,45.42337850571526],[8.086238762219843,45.3926467614535],[8.015246804101336,45.37550064157892],[7.985783738478874,45.42121207166915],[8.02282302097566,45.44838495097526],[8.098304589093475,45.44917237554611]]]}\n\n\nif KEPLER_OUTPUT:\n    # ~ 5 minutes first run - data are downloaded and cached\n    map_3 = KeplerGl(\n        data=deepcopy(split_gdf_to_layers(water_and_land(dict_viverone))), \n        height=800,\n        config =config_map3,\n    )\n    display(map_3)\nelse:\n    display(Image(\"images/map3.png\"))\n\n\n\n\n\n\n\n\nNow we can consider the three “nearby” operations. For simplicity all the three operations we are interested in are embedded in a class.\n\n\nclass GeoOperations:\n\n    @staticmethod\n    def _data_to_gdf(roi: dict) -&gt; gpd.GeoDataFrame:\n        \"\"\" input value `data` is a dumped geojson to a string format \"\"\"\n        sh_roi = shape(roi)\n        return gpd.GeoDataFrame({\"name\":[\"ROI\"], \"geometry\": [sh_roi]}).set_crs(4326)\n    \n    @staticmethod\n    def _add_area_column(gdf: gpd.GeoDataFrame) -&gt; None:\n        gdf[\"areas (Km2)\"] = gdf.to_crs({'proj':'cea'}).area/ 10**6\n        \n    @staticmethod\n    def _get_polygon_box(row):\n        c_hull = row.convex_hull  # make sure the input is a polygon \n        x, y, X, Y = c_hull.bounds\n        return shapely.geometry.Polygon([(x, y), (X, y), (X, Y), (x, Y), (x, y)])\n    \n    def buffer(self, data: str, buffer_meters:float = 10) -&gt; gpd.GeoDataFrame:\n        gdf = self._data_to_gdf(data)\n        gdf.geometry = gdf.to_crs(crs=3857)['geometry'].buffer(buffer_meters).to_crs(4326)\n        self._add_area_column(gdf)\n        return gdf\n\n    def convex_hull(self, data: str) -&gt; gpd.GeoDataFrame:\n        gdf = self._data_to_gdf(data)\n        gdf.geometry = gdf['geometry'].convex_hull\n        self._add_area_column(gdf)\n        return gdf\n        \n    def bbox(self, data: str) -&gt; gpd.GeoDataFrame:\n        gdf = self._data_to_gdf(data)\n        gdf['geometry'] = gdf['geometry'].apply(lambda x: self._get_polygon_box(x))\n        self._add_area_column(gdf)\n        return gdf\n\n\nkepler_data = split_gdf_to_layers(water_and_land(dict_roi))\n\nge_ops = GeoOperations()\nkepler_data.update({\"buffer\": ge_ops.buffer(dict_roi, 5_000), \"convex hull\": ge_ops.convex_hull(dict_roi), \"bbox\": ge_ops.bbox(dict_roi)})\n\n\ntry:\n    from configs import config_map4\nexcept ImportError:\n    config_map4 = dict()\n\n\nif KEPLER_OUTPUT:\n    map_4 = KeplerGl(\n        data=deepcopy(kepler_data),\n        height=800,\n        config=deepcopy(config_map4),\n    )\n    map_4\nelse:\n    display(Image(\"images/map4.png\"))\n\n\n\n\n\n\n\n\nIn the picture above the bounding box, buffer and convex hull were added to the selected geometry, but the land and sea was not computed for these new polygons. This is left for the reader for practice. For example the water and land within the bounding box should look like the figure below:\n\ndict_bbox = ge_ops.bbox(dict_roi)[\"geometry\"].iloc[0].__geo_interface__\n\nif KEPLER_OUTPUT:\n    map_5 = KeplerGl(\n        data=split_gdf_to_layers(water_and_land(dict_bbox)), \n        height=800,\n        config=deepcopy(config_map4), # we can re-use the same config as the previous map\n    )\n    map_5\nelse:\n    display(Image(\"images/map5.png\"))\n\n\n\n\n\n\n\n\n1.3 Which country intersects this polygon?\nImagine now to draw a polygon on the map with the goal of solving the following problem:\n\nGet the list with the names of all the countries intersecting the given polygon.\n\nThere are two options we can see think of to solve this problem with osmnx: 1. Create a GeoDataFrame in memory with all the countries in the world, and then compute the intersection between the dataframe and the given polygon. 2. Feed the given polygon to osmnx directly, and use the right tag to get all the countries intersecting it.\nThe first one is very slow to initialise, though fast each time a new polygon is passed to the intersect method. The second one is relatively slow each time a new polygon is given.\nHere, out of laziness we will not use any of the methods above. The toy dataset provided geopandas contains already all the information that we need, so we will use this one instead of the one provided by osmnx.\nAs usual the first step is to draw a polygon on the first empty map of the notebook. Then copy paste the polygon geojson below, to be saved in the variable dict_roi. We selected a rectangular region around Myanmar, the reader is free to act according to preference.\n\ndict_roi = {\"type\":\"Polygon\",\"coordinates\":[[[95.27405518168942,28.464895473156595],[88.19328456832459,26.69900877882346],[87.15109322263424,21.154320639876016],[91.6876908450501,14.831845554981104],[98.52323996531177,13.52423467533735],[103.97941818686449,15.157545455928199],[104.43920848643478,20.868173454079436],[104.0713762467781,26.45228108415989],[95.27405518168942,28.464895473156595]]]}\n\nsh_roi = shape(dict_roi)\n\ngdf_roi =  gpd.GeoDataFrame({\"name\":[\"ROI\"], \"geometry\": [sh_roi]})\ngdf_roi = gdf_roi.set_crs('4326')\n\ntry:\n    from configs import config_map6\nexcept ImportError:\n    config_map6 = dict()\n\nif KEPLER_OUTPUT:\n    map_6 = KeplerGl(data=deepcopy({\"roi\": gdf_roi}), height=800, config=config_map6)\n    display(map_6)\nelse:\n    display(Image(\"images/map6.png\"))\n\n\n\n\n\n\n\n\n\nimport geopandas as gpd\n\ngdf_countries = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\ngdf_countries[\"areas (Km2)\"] = gdf_countries.to_crs({'proj':'cea'}).area/ 10**6\n\nax = gdf_countries.plot(figsize=(15, 15), column='areas (Km2)', cmap='Greens')\nax.axis('off')  # images/map_naturalearth.png\n\n\n\n\n\n\n\n\n\ndict_roi = {\"type\":\"Polygon\",\"coordinates\":[[[95.27405518168942,28.464895473156595],[88.19328456832459,26.69900877882346],[87.15109322263424,21.154320639876016],[91.6876908450501,14.831845554981104],[98.52323996531177,13.52423467533735],[103.97941818686449,15.157545455928199],[104.43920848643478,20.868173454079436],[104.0713762467781,26.45228108415989],[95.27405518168942,28.464895473156595]]]}\n\nsh_roi = shape(dict_roi)\n\n# note gdf_countries.intersects(sh_roi) returns a boolean series.\n\ngdf_intersection_countries = gdf_countries[gdf_countries.intersects(sh_roi)].reset_index(drop=True)\ngdf_intersection_countries[[\"name\", 'areas (Km2)']].head(10)\n\ndisplay(gdf_intersection_countries.head())\n\n\n\n\n\n\n\n\npop_est\ncontinent\nname\niso_a3\ngdp_md_est\ngeometry\nareas (Km2)\n\n\n\n\n0\n68414135\nAsia\nThailand\nTHA\n1161000.0\nPOLYGON ((105.21878 14.27321, 104.28142 14.416...\n5.101229e+05\n\n\n1\n7126706\nAsia\nLaos\nLAO\n40960.0\nPOLYGON ((107.38273 14.20244, 106.49637 14.570...\n2.290354e+05\n\n\n2\n55123814\nAsia\nMyanmar\nMMR\n311100.0\nPOLYGON ((100.11599 20.41785, 99.54331 20.1866...\n6.795988e+05\n\n\n3\n96160163\nAsia\nVietnam\nVNM\n594900.0\nPOLYGON ((104.33433 10.48654, 105.19991 10.889...\n3.360194e+05\n\n\n4\n1281935911\nAsia\nIndia\nIND\n8721000.0\nPOLYGON ((97.32711 28.26158, 97.40256 27.88254...\n3.142772e+06\n\n\n\n\n\n\n\nLeft to the reader is to add a column with the information of the the percentage of the area covered by the selected roi over each country (e.g. Myanmar will be 100% covered by the ROI. What about China?)."
  },
  {
    "objectID": "posts/gds-2022-10-03-polygons-part-1/index.html#distances-between-countries",
    "href": "posts/gds-2022-10-03-polygons-part-1/index.html#distances-between-countries",
    "title": "Using polygons",
    "section": "Distances between countries",
    "text": "Distances between countries\nNow let’s say you have two polygons representing two countries. How to compute their distance in Km on the surface of the earth?\n\nlist_countries = sorted(list(gdf_countries.name))\nlist_countries[:10]\n\n['Afghanistan',\n 'Albania',\n 'Algeria',\n 'Angola',\n 'Antarctica',\n 'Argentina',\n 'Armenia',\n 'Australia',\n 'Austria',\n 'Azerbaijan']\n\n\n\ngdf_countries.to_crs(epsg=27700,inplace=True) # more about 27700 on the next article! The impatient reader can look under https://epsg.io/27700\n\nsh_france = gdf_countries[gdf_countries.name == \"France\"].geometry.iloc[0]\nsh_italy = gdf_countries[gdf_countries.name == 'Italy'].geometry.iloc[0]\nsh_uk = gdf_countries[gdf_countries.name == 'United Kingdom'].geometry.iloc[0]\n\nprint(f\"Distance between France and UK polygons: {round(sh_france.distance(sh_uk) / 1_000, 3)} Km\")\n\nDistance between France and UK polygons: 37.14 Km\n\n\nA quick investigation using google seems to not confirm this number. Google says the distance is approx 32Km. Did we do something wrong? Let’s use google maps and pick the two point that are visually the closest to each others on the coasts, copy paste them to the notebook and see what is their distance:\n\na = (51.108648, 1.286524)\nb = (50.869575, 1.583608)\n\nprint(f\"Distance between France and UK by naked eye: {round(distance.distance(a, b).km, 3)} Km\")\n\nDistance between France and UK by naked eye: 33.801 Km\n\n\nThis number looks way closer to the one found with the shapely distance method. So Why the discrepancy? Is it a problem with the coordinates? Maybe the 27700 is not the optimal one in this case?\nLooking at the data we can find that the accuracy of the countries segmentation is way less that the actual underlying coast. The accuracy is so low that the distance between the polygon is actually wider!\n\ntry:\n    from configs import config_map7\nexcept ImportError:\n    config_map7 = dict()\n\nif KEPLER_OUTPUT:\n    map_7 = KeplerGl(data=deepcopy({\"rois\": gdf_countries.to_crs('4326')}), height=800, config=config_map7)\n    display(map_7)\nelse:\n    display(Image(\"images/map7.png\"))\n\n\n\n\n\n\n\n\nNOTE: the artefacts that you can see in the map above are caused by polygons crossing the antimeridian. This topic will be discussed in the next blog post of the polygon series “reference systems for polygons”.\nThis section also confirmed numerically the importance of the quality of the segmentation in taking measurements, which lead us straight to the next (and last!) section of this first tutorial about polygons."
  },
  {
    "objectID": "posts/gds-2022-10-03-polygons-part-1/index.html#same-country-different-borders-how-different-are-they",
    "href": "posts/gds-2022-10-03-polygons-part-1/index.html#same-country-different-borders-how-different-are-they",
    "title": "Using polygons",
    "section": "Same country, different borders: how different are they?",
    "text": "Same country, different borders: how different are they?\nThe problem we want to address in this section is to quantify the differences between segmentations of the same region. As often happens in geospatial data science (and not only there) a ground truth is not available. Also, as in the case of natural phenomena, the true segmentation varies over time, and an approximation is all what we can get. Quantifying the differences between two shapes consists of reducing the dimensionality of the problem, from 2D to 1D. The methods here presented can be generalised from ND to 1D, which can happen in case altitude and time are taken into account in the segmentation process.\nWe will take three segmentations of a the Laguna Honda reservoir in San Francisco, CA: a most accurate, a less accurate, and an absurdly inaccurate one, then we measure their distances with a range of four metrics.\n\ndict_lh_accurate = {\"type\":\"Polygon\",\"coordinates\":[[[-122.46336870734235,37.75382312075932],[-122.46361657885608,37.75373534452417],[-122.4635831171001,37.75365833165909],[-122.46316396885612,37.75301643279386],[-122.46300520664897,37.752840487963525],[-122.46230161881246,37.7522463019286],[-122.46211503445602,37.75203693263166],[-122.46178081988387,37.75186782984793],[-122.46148855443528,37.75184019740165],[-122.46133288449474,37.75201341062547],[-122.4612061599761,37.752084737992796],[-122.46103218224685,37.75214417741294],[-122.46077014171657,37.752179841041794],[-122.46037947391036,37.752188392148845],[-122.46021718629451,37.75217343769624],[-122.4600201930734,37.75211950941181],[-122.45991172291727,37.75207281219638],[-122.45983991818758,37.752077610017345],[-122.45994274713122,37.75230912513596],[-122.45996851594319,37.75240437677724],[-122.46006708164848,37.7524522572354],[-122.46084756619754,37.752511150412346],[-122.46122218790283,37.75259302382214],[-122.46131143485962,37.75262966326064],[-122.46167022567394,37.7526676568636],[-122.46188775902073,37.75272257149743],[-122.46205025380969,37.752816858793715],[-122.462376261119,37.75308710607375],[-122.46251137863547,37.753166819591506],[-122.46262051201403,37.75320708721178],[-122.46268905680037,37.75326534110075],[-122.46270650192626,37.75330849493771],[-122.46273109980764,37.753422092415555],[-122.46277517530947,37.75345753879545],[-122.4628296309809,37.7534771096768],[-122.46294915057157,37.75346704465253],[-122.4630682867433,37.753516920096686],[-122.46336870734235,37.75382312075932]]]}\n\ndict_lh_less_accurate = {\"type\":\"Polygon\",\"coordinates\":[[[-122.46336953589098,37.753832862738975],[-122.4636233738959,37.75373251334907],[-122.4631072366194,37.75293640336153],[-122.46177458709447,37.75186264481159],[-122.46148267338904,37.75183922934248],[-122.4612246047509,37.752080073813296],[-122.46047155200345,37.75218711554916],[-122.4599004164929,37.752066693585995],[-122.45984118762517,37.752080073813296],[-122.45996387599406,37.752411233681165],[-122.46131767868594,37.752618625974755],[-122.46194804306447,37.75272232190355],[-122.46268840391157,37.75324748937671],[-122.46272647961233,37.753431464361086],[-122.46314954295359,37.75359536896148],[-122.46336953589098,37.753832862738975]]]}\n\n\ndict_lh_inaccurate = {\"type\":\"Polygon\",\"coordinates\":[[[-122.46176576455443,37.75368924488866],[-122.46242948841775,37.75334575268072],[-122.4631052799878,37.752296183276115],[-122.46372073302484,37.75144697531703],[-122.46259843631039,37.75127522420765],[-122.46204332180642,37.75167597617579],[-122.46192264474033,37.752410682480374],[-122.46095722821178,37.752954551279636],[-122.4605348584804,37.75364153773237],[-122.46176576455443,37.75368924488866]]]}\n\n\nsh_lh_accurate = shape(dict_lh_accurate)\nsh_lh_less_accurate = shape(dict_lh_less_accurate)\nsh_lh_inaccurate = shape(dict_lh_inaccurate)\n\ngdf_lh_accurate = gpd.GeoDataFrame({\"name\":[\"Accurate\"], \"geometry\": [sh_lh_accurate]}).set_crs('4326')\ngdf_lh_less_accurate = gpd.GeoDataFrame({\"name\":[\"Less accurate\"], \"geometry\": [sh_lh_less_accurate]}).set_crs('4326')\ngdf_lh_inaccurate = gpd.GeoDataFrame({\"name\":[\"Inaccurate\"], \"geometry\": [sh_lh_inaccurate]}).set_crs('4326')\n\n\ntry:\n    from configs import config_map8\nexcept ImportError:\n    config_map8 = dict()\n\nif KEPLER_OUTPUT:\n    map_8 = KeplerGl(\n        data={\n            \"Laguna Honda accurate\": deepcopy(gdf_lh_accurate),\n            \"Laguna Honda less accurate\": deepcopy(gdf_lh_less_accurate),\n            \"Laguna Honda inaccurate\": deepcopy(gdf_lh_inaccurate),\n        }, \n        height=800,\n        config=config_map8\n    )\n    display(map_8)\nelse:\n    display(Image(\"images/map8.png\"))\n\n\n\n\n\n\n\n\n\nDice’s Score\nAfter Lee R. Dice, the Dice’s score measures the area of the overlap between two polygons normalised to the sum of the individual areas of the two polygons. Therefore if the polygon are perfectly congruent, then the dice equals 1, if there is no intersection between the two elements, the measure is zero.\nFormally: let \\(P_1\\) and \\(P_2\\) be two simple (i.e. where their area can be computed uniquely) polygons drawn on a map, \\(\\mathcal{A}(P_1)\\) and \\(\\mathcal{A}(P_2)\\) their area, and \\(P_1 \\cap P_2\\) their intersection. The Dice’s score of \\(P_1\\) and \\(P_2\\) is defined as: \\[\n\\text{Dice}(P_1, P_2) = \\frac{2 \\mathcal{A}(P_1 \\cap P_2) }{ \\mathcal{A}(P_1) + \\mathcal{A}(P_2)}~.\n\\]\nThis definition does not correspond to the intuitive definition of a distance that is zero when the measured object as as close to each others as possible.\n\n\nDice’s Distance\nThe Dice’s score can be turned into a distance simply modifying its formula as: \\[\n\\text{DiceDist}(P_1, P_2) = 1 - \\frac{2 \\mathcal{A}(P_1 \\cap P_2) }{ \\mathcal{A}(P_1) + \\mathcal{A}(P_2)}~,\n\\] so that the distance between two identical polygons is zero.\n\n\nCovariance Distance\nWe can consider the polygon’s vertex as a cloud of points, and compare the principal components of their distribution encoded as a Symmetric Positive Definite matrix through the covariance matrix.\nThe covariance distance is given by the sum of the 2 eigenvalues of the product of the covariance matrices, normalised by the sum of the products of squared eigenvalues of each covariance matrix. As done for the dice score, to have zero when the two shapes are identica, we take \\(1\\) minus the value obtained.\nFormally: with the same notation as above the covariance distance is defined as \\[\n\\text{CovDist}(P_1, P_2) = \\alpha \\left( 1 - \\frac{ \\text{Tr}( \\text{c}(P_1) \\text{c}(P_2) )  }{ \\text{Fro}(P_1) + \\text{Fro}(P_2)} \\right) ~,\n\\] where \\(\\alpha\\) is a multiplicative factor to scale the data (it can be one if the points are normalised with standard deviation = 1, or computed as the reciprocal of the average standard deviations of the clouds distributions), and \\(\\text{Tr}\\) and \\(\\text{Fro}\\) are the trace and Frobenius norm respectively.\n\n\nHausdoroff Distance\nThe main feature of the covariance distance is that only the principal components of the two geometries are considered, and the noise is not taken into account. The Hausdoroff distance instead aims at being very sensitive to the misplacement of a single vertex between the two geometries.\nIt’s definition is based on as maximal distance between the vertex of one polygon the distance between the \\[\n\\text{HausDist}(P_1, P_2) = \\text{max} \\left\\{ \\text{max}_{p_i \\in \\partial P_1} d(p_i, \\partial P_2) , \\text{max}_{p_j \\in \\partial P_2} d(p_j, \\partial P_1) \\right\\} ~,\n\\] where \\(\\partial P_1\\) is the contour, or perimeter of the polygon \\(P_i\\) and \\(d\\) is the Euclidean distance (or in the case of the curved surface, the geodesic distance).\n\n\nNormalised Symmetric Contour distance\nThe contour distance between \\(P_1\\) and \\(P_2\\) is given by the the sum of the distances of all the vertex of \\(P_1\\) to the contour of \\(P_2\\).\nAs contour distance so defined is not symmetric (the contour distance between \\(P_1\\) and \\(P_2\\) is not the same of the contour distance between \\(P_2\\) and \\(P_1\\)), we can make it symmetric considering the sum of the contour distance between \\(P_1\\) and \\(P_2\\) with the contour distance between \\(P_2\\) and \\(P_1\\).\nThe distance can be normalised considering a distance factor encompassing both geometries in respect to their distances, which can be the lengths of both perimeters.\nFormally: \\[\n\\text{NSCD}(P_1, P_2) = \\frac{  S(P_1, P_2) + S(P_1, P_2) }{ \\vert \\partial P_1 \\vert + \\vert \\partial P_2 \\vert } ~,\n\\] where the length of the contour is indicated with \\(\\vert \\partial P_i \\vert\\), and \\(S(P_1, P_2)\\) is the contour distance, computed as: \\[\nS(P_1, P_2) = \\sum_{p_i \\in \\partial P_1} d(p_i, \\partial P_2)~.\n\\]\nBelow we implement the four distances in a class and then we use this tool to measure the distances between the polygon we created above: sh_lh_accurate, sh_lh_less_accurate and sh_lh_inaccurate.\nAs usual you are invited to try it out on your own, on different geometries, to check the behaviours of the distances.\n\nclass Dist:\n    def __init__(self, p1: Polygon, p2: Polygon):\n        self.p1 = p1\n        self.p2 = p2\n        self.prec = 6\n        \n    def dice(self) -&gt; float:\n        assert self.p1.area + self.p2.area &gt; 0, \"Polygons must have positive areas.\"\n        return np.round(1 - 2 * self.p1.intersection(self.p2).area / (self.p1.area + self.p2.area), self.prec)\n    \n    def cov(self) -&gt; float:\n        x1, y1 = self.p1.exterior.coords.xy[0], self.p1.exterior.coords.xy[1]\n        x2, y2 = self.p2.exterior.coords.xy[0], self.p2.exterior.coords.xy[1]\n        cov1 = np.cov(np.array([(x1 - np.mean(x1)) /  np.std(x1), (y1 - np.mean(y1)) / np.std(y1) ]))\n        cov2 = np.cov(np.array([(x2 - np.mean(x2)) /  np.std(x2), (y2 - np.mean(y2)) / np.std(y2) ]))\n        return np.round((1 - (np.trace(cov1.dot(cov2)) / (np.linalg.norm(cov1, ord='fro') * np.linalg.norm(cov2, ord='fro')))), self.prec)\n    \n    def hau(self) -&gt; float:\n        \"\"\" measured in degrees !!\"\"\"\n        return np.round(self.p1.hausdorff_distance(self.p2), self.prec)\n    \n    def nsc(self) -&gt; float:\n        \"\"\" measured in degrees !!\"\"\"\n        def bd(p1, p2):\n            return np.round(sum([p1.boundary.distance(Point(x, y)) for x,y in zip(p2.exterior.coords.xy[0], p2.exterior.coords.xy[1])]), self.prec)\n        \n        return (bd(self.p1, self.p2) + bd(self.p2, self.p1)) / (self.p1.length + self.p2.length)\n    \n    def d(self, selected_measure: str) -&gt; float:\n        map_measures = {\n            \"dice\": self.dice,\n            \"cov\": self.cov,\n            \"haus\": self.hau,\n            \"nsc\": self.nsc,\n        }\n        return map_measures[selected_measure]()\n        \n        \n\n\n# simplify the name according to the polygons colour in the image above\ngreen = sh_lh_accurate\nyellow = sh_lh_less_accurate\nred = sh_lh_inaccurate\n\nmeas = [ \"dice\", \"cov\", \"haus\", \"nsc\"]\n\ndf_dist = pd.DataFrame(\n    {\n        \"green - green\": [Dist(green, green).d(m) for m in meas],\n        \"green - yellow\": [Dist(green, yellow).d(m) for m in meas],\n        \"green - red\": [Dist(green, red).d(m) for m in meas],\n        \"yellow - red\": [Dist(yellow, red).d(m) for m in meas],    \n    },\n    index=meas\n)  \n\ndisplay(df_dist)\n\n\n\n\n\n\n\n\ngreen - green\ngreen - yellow\ngreen - red\nyellow - red\n\n\n\n\ndice\n-0.0\n0.039633\n0.760388\n0.783997\n\n\ncov\n0.0\n0.000104\n0.692591\n0.706379\n\n\nhaus\n0.0\n0.000078\n0.001526\n0.001545\n\n\nnsc\n0.0\n0.044710\n1.657110\n1.038613\n\n\n\n\n\n\n\n\nNote 1:\nThe considered Hausdorff and nsc distances are in degrees and not in meters. As we want to have a measure of the differences for geometries (specifically geometries within comparable latitude intervals) this is not invalidating the results. Though there are cases when these metrics should return the results in meters. This is left as an exercise for the reader (you can leave a comment below for a discussion about possible solutions!).\n\n\nNote 2:\nThe same methods can be used for comparing the differences in shape of two objects that are expected to be similar or two measure the variability of two shapes that are changing over time, assuming that their segmentation is accurate and unbiased.\nAlso the same method can be used to measure the accuracy of a segmentation of two different people segmenting the same object (inter-rater variability) or the same person segmenting twice the same object (intra-rater variability). If the variability is assessed without the rater knowing he is segmenting twice the same shape (out of a stack of different shapes to undergo segmentation, one is repeated twice, re-labelled and re-shuffled in the stack), then this experiment is called test re-test."
  },
  {
    "objectID": "posts/gds-2022-10-03-polygons-part-1/index.html#appendix-list-of-topics-and-further-theoretical-ideas",
    "href": "posts/gds-2022-10-03-polygons-part-1/index.html#appendix-list-of-topics-and-further-theoretical-ideas",
    "title": "Using polygons",
    "section": "Appendix: list of topics and further theoretical ideas",
    "text": "Appendix: list of topics and further theoretical ideas\nIn solving three recurring problems we went through a relatively long list of topics. To maintain an hands on approach, the theory was barely touched. In the list below you can find a summary reporting the list of the topics touched and some hints to dig further into the theory:\n\nPolygons and sets\n\npolygon as a list of vectors\npolygon as a set on a surface\noperations between sets: union, intersection, difference, disjoint union.\nCan you express union and difference as combinations of intersection and disjoint union?\nDe Morgan laws\nMathematical concept of Algebra and sigma algebra. Are the polygons on the surface of a sphere with the operation of intersection and disjoint union a sigma algebra?\nGeojson format to represent a polygon\nGeojson to shapely, and shapely operations\nAdding metadata to a polygon with geopandas\nOsmnx library\nVisualisation with KeplerGl and WGS84\n\nMorphological operations in GIS\n\nDilation (or buffer) and erosion\nBounding box\nConvex hull\nClass as a toolbox\n\nDistance between polygons and projections\n\n“Optimal Algorithms for Computing the Minimum Distance Between Two Finite Planar Sets” Toussaint, Bhattacharya\nMinkovsky sum\nHaversine distance\nVincentry distance\n\nShape differences\n\nSegmentation\nDifference quantification as a dimensionality reduction problem\nIntra-rater and inter-rater variability\nDice’s score\nCovariance of a polygon (of a set of 2d points) and covariance distance\nHausdoroff distance\nNormalized symmetric contour distance"
  },
  {
    "objectID": "posts/gds-2022-10-03-polygons-part-1/index.html#resources",
    "href": "posts/gds-2022-10-03-polygons-part-1/index.html#resources",
    "title": "Using polygons",
    "section": "Resources",
    "text": "Resources\n\nGeopandas and shapely basics https://www.learndatasci.com/tutorials/geospatial-data-python-geopandas-shapely/\nShapely manual https://shapely.readthedocs.io/en/stable/manual.html\nGeopandas crs usage https://geopandas.org/en/stable/docs/user_guide/projections.html\nintersection polygon algorithm https://www.swtestacademy.com/intersection-convex-polygons-algorithm/\nBuffer (dilation) to remove holes https://gis.stackexchange.com/questions/409340/removing-small-holes-from-the-polygon/409398\nmaps projections https://automating-gis-processes.github.io/2017/lessons/L2/projections.html\nTrace and covariance https://online.stat.psu.edu/stat505/lesson/1/1.5"
  },
  {
    "objectID": "posts/gds-2022-10-03-polygons-part-1/index.html#inspired-by",
    "href": "posts/gds-2022-10-03-polygons-part-1/index.html#inspired-by",
    "title": "Using polygons",
    "section": "Inspired by",
    "text": "Inspired by\n\nQiusheng Wu\nMaxime Labonne"
  },
  {
    "objectID": "posts/index_bp.html",
    "href": "posts/index_bp.html",
    "title": "Code development",
    "section": "",
    "text": "How to sign your commits\n\n\nSetting up gpg authentication while keeping separate work and personal projects\n\n\n\n\n\nJul 23, 2022\n\n\n  \n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A Geospatial Data Science Blog",
    "section": "",
    "text": "Python datetimes manipulations\n\n\nCode fragments for timestamps and timezones manipulation\n\n\n\n\n\nJan 24, 2024\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\nHaversine’s distance mathematics\n\n\nComputing the distance between two points on a sphere\n\n\n\n\n\nJan 13, 2024\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\nUsing polygons\n\n\nGeospatial data science basic operations and measurements\n\n\n\n\n\nOct 3, 2022\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\nHow well positioned is your office?\n\n\nTime to question your workplace location\n\n\n\n\n\nJul 29, 2022\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\nHow to sign your commits\n\n\nSetting up gpg authentication while keeping separate work and personal projects\n\n\n\n\n\nJul 23, 2022\n\n\n  \n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/bp.html",
    "href": "posts/bp.html",
    "title": "Code development",
    "section": "",
    "text": "How to sign your commits\n\n\nSetting up gpg authentication while keeping separate work and personal projects\n\n\n\n\n\nJul 23, 2022\n\n\n  \n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/all.html",
    "href": "posts/all.html",
    "title": "Posts",
    "section": "",
    "text": "Python datetimes manipulations\n\n\nCode fragments for timestamps and timezones manipulation\n\n\n\n\n\nJan 24, 2024\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\nHaversine’s distance mathematics\n\n\nComputing the distance between two points on a sphere\n\n\n\n\n\nJan 13, 2024\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\nUsing polygons\n\n\nGeospatial data science basic operations and measurements\n\n\n\n\n\nOct 3, 2022\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\nHow well positioned is your office?\n\n\nTime to question your workplace location\n\n\n\n\n\nJul 29, 2022\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\nHow to sign your commits\n\n\nSetting up gpg authentication while keeping separate work and personal projects\n\n\n\n\n\nJul 23, 2022\n\n\n  \n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/gds.html",
    "href": "posts/gds.html",
    "title": "Geospatial Data Science",
    "section": "",
    "text": "Python datetimes manipulations\n\n\nCode fragments for timestamps and timezones manipulation\n\n\n\n\n\nJan 24, 2024\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\nHaversine’s distance mathematics\n\n\nComputing the distance between two points on a sphere\n\n\n\n\n\nJan 13, 2024\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\nUsing polygons\n\n\nGeospatial data science basic operations and measurements\n\n\n\n\n\nOct 3, 2022\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\nHow well positioned is your office?\n\n\nTime to question your workplace location\n\n\n\n\n\nJul 29, 2022\n\n\n\n\n\n  \n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#apresentações-recentes-recent-talks",
    "href": "index.html#apresentações-recentes-recent-talks",
    "title": "A Geospatial Data Science Blog",
    "section": "Apresentações recentes / Recent talks",
    "text": "Apresentações recentes / Recent talks\nVeja todas as apresentações aqui! / See all talks here!"
  },
  {
    "objectID": "index.html#textos-recentes-recent-posts",
    "href": "index.html#textos-recentes-recent-posts",
    "title": "A Geospatial Data Science Blog",
    "section": "Textos recentes / Recent posts",
    "text": "Textos recentes / Recent posts\n\n\n\n\n\n\n\n\n\ncolab\n\n\n \n\n\n\n\ngithub\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncolab\n\n\n \n\n\n\n\ngithub\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncolab\n\n\n \n\n\n\n\ngithub\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncolab\n\n\n \n\n\n\n\ngithub\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncolab\n\n\n \n\n\n\n\ngithub\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncolab\n\n\n \n\n\n\n\ngithub\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncolab\n\n\n \n\n\n\n\ngithub\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncolab\n\n\n \n\n\n\n\ngithub\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncolab\n\n\n \n\n\n\n\ngithub\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncolab\n\n\n \n\n\n\n\ngithub\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncolab\n\n\n \n\n\n\n\ngithub\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncolab\n\n\n \n\n\n\n\ngithub\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython datetimes manipulations\n\n\nCode fragments for timestamps and timezones manipulation\n\n\n\nJan 24, 2024\n\n\n\n\n\ncolab\n\n\n\n\n\n\n\ngithub\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHaversine’s distance mathematics\n\n\nComputing the distance between two points on a sphere\n\n\n\nJan 13, 2024\n\n\n\n\n\ncolab\n\n\n\n\n\n\n\ngithub\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing polygons\n\n\nGeospatial data science basic operations and measurements\n\n\n\nOct 3, 2022\n\n\n\n\n\ncolab\n\n\n\n\n\n\n\ngithub\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow well positioned is your office?\n\n\nTime to question your workplace location\n\n\n\nJul 29, 2022\n\n\n\n\n\ncolab\n\n\n\n\n\n\n\ngithub\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to sign your commits\n\n\nSetting up gpg authentication while keeping separate work and personal projects\n\n\n\nJul 23, 2022\n\n\n\n\n\ncolab\n\n\n \n\n\n\n\ngithub\n\n\n  \n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#quarto",
    "href": "index.html#quarto",
    "title": "A Geospatial Data Science Blog",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished presentation."
  },
  {
    "objectID": "index.html#bullets",
    "href": "index.html#bullets",
    "title": "A Geospatial Data Science Blog",
    "section": "Bullets",
    "text": "Bullets\nWhen you click the Render button a document will be generated that includes:\n\nContent authored with markdown\nOutput from executable code"
  }
]