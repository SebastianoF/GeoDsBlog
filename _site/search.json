[
  {
    "objectID": "posts/index_gds.html",
    "href": "posts/index_gds.html",
    "title": "Geospatial Data Science",
    "section": "",
    "text": "Python datetimes manipulations\n\n\nCode fragments for timestamps and timezones manipulation\n\n\n\n\n\nJan 24, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHaversine’s distance mathematics\n\n\nComputing the distance between two points on a sphere\n\n\n\n\n\nJan 13, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nUsing polygons\n\n\nGeospatial data science basic operations and measurements\n\n\n\n\n\nOct 3, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nHow well positioned is your office?\n\n\nTime to question your workplace location\n\n\n\n\n\nJul 29, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/bp-2022-07-23-gpg/index.html",
    "href": "posts/bp-2022-07-23-gpg/index.html",
    "title": "How to sign your commits",
    "section": "",
    "text": "This short blog post introduces the reader to the PGP key signatures, and suggests a way of signing commits with different accounts - e.g. personal and work settings - and to swap between them with a single CLI command."
  },
  {
    "objectID": "posts/bp-2022-07-23-gpg/index.html#requirements",
    "href": "posts/bp-2022-07-23-gpg/index.html#requirements",
    "title": "How to sign your commits",
    "section": "Requirements",
    "text": "Requirements\n\nYou are familiar with git command line and gitlab / github interface.\nYou installed the gpg1 command line interface (Mac, Linux or PowerShell - Windows)."
  },
  {
    "objectID": "posts/bp-2022-07-23-gpg/index.html#create-and-add-a-pgp-key-signature-to-github-or-gitlab",
    "href": "posts/bp-2022-07-23-gpg/index.html#create-and-add-a-pgp-key-signature-to-github-or-gitlab",
    "title": "How to sign your commits",
    "section": "Create and add a PGP key signature to github or gitlab",
    "text": "Create and add a PGP key signature to github or gitlab\nThe procedure of creating and adding a PGP key signature is similar to the one to add an RSA key to your github or gitlab account. The difference is that instead of using the ssh-agent you will be using the gpg CLI, and you will be asked to pair the configurations of your PGP key with the RSA key.\n\nCreate a new RSA (read only) key-pair with fingerprint (PGP key signature) via gpg the CLI command\n\ngpg --full-generate-key\nThe command above will bring you to a prompt asking you to input the kind of key that you want and its expiry period, followed by your email, username, and a passphrase. Recommended settings for a key to sign commits is RSA (sign only) followed by 4096 for the keysize (in the appendix of this article you can find some instructions to undo the key creation).\n\nIf you are using gitlab, please add the pgp public key to the page https://gitlab.com/-/profile/gpg_keys. If you are using github, go instead to use the page https://github.com/settings/keys.\n\nYou can see the list of created key pairs with pgp -k, and you can export the public key to a file with gpg --export -a \"email@address.com\" &gt; public.key, then copy the content of public.key to clipboard to export it to github/gitlab.\n\nNow you have created your RSA key pair, and the pgp public key. The missing step is to tell git that it has to start using the created key to sign the commits. To achive this goal, change the user setting on the local machine with\n\ngit config --global user.email &lt;same email used when creating the key-pair&gt;\ngit config --global user.name &lt;same username used when creating the key-pair&gt;\ngit config --global user.signingkey &lt;your fingerprint - copy paste from git(lab-hub) gpg page&gt;\ngit config --global commit.gpgSign true"
  },
  {
    "objectID": "posts/bp-2022-07-23-gpg/index.html#swap-between-work-and-personal-profile",
    "href": "posts/bp-2022-07-23-gpg/index.html#swap-between-work-and-personal-profile",
    "title": "How to sign your commits",
    "section": "Swap between work and personal profile",
    "text": "Swap between work and personal profile\nAt this point you may also have the problem that I had: swapping between multiple gitub/gitlab profiles, for work, personal project, etc. With the commands above I can create as many gpg keys with as many username and emails. To quickly tell git to swap between them, the solution that I suggest here is to create a bash script for each profile in the sub-folder ~/.git-accounts-settings/:\n# ~/.git-accounts-settings/company_A.sh\ngit config --global user.email &lt;same email used when creating the key-pair&gt;\ngit config --global user.name &lt;same username used when creating the key-pair&gt;\ngit config --global user.signingkey &lt;your fingerprint - copy paste from gitlab gpg page&gt;\ngit config --global commit.gpgSign true\n# ~/.git-accounts-settings/company_B.sh\ngit config --global user.email &lt;another email, same used when creating the key-pair&gt;\ngit config --global user.name &lt;another username used when creating the key-pair&gt;\ngit config --global user.signingkey &lt;your fingerprint - copy paste from gitlab gpg page&gt;\ngit config --global commit.gpgSign true\n# ~/.git-accounts-settings/personal.sh\ngit config --global user.email &lt;personal email&gt;\ngit config --global user.name &lt;personal user&gt;\ngit config --global commit.gpgSign false\nEach time you want to swap across profiles, you can simply call the script with bash ~/.git-accounts-settings/&lt;my profile&gt;.sh.\nAnd to quickly swap between them, the commands can be aliased with shorter variable names:\n# in the .bashrc\nalias set_git_config_company_A=\"bash ~/.git-accounts-settings/company_A.sh\"\nalias set_git_config_company_B=\"bash ~/.git-accounts-settings/company_B.sh\"\nalias set_git_config_personal=\"bash ~/.git-accounts-settings/personal.sh\""
  },
  {
    "objectID": "posts/bp-2022-07-23-gpg/index.html#cache-the-passphrases",
    "href": "posts/bp-2022-07-23-gpg/index.html#cache-the-passphrases",
    "title": "How to sign your commits",
    "section": "Cache the passphrases",
    "text": "Cache the passphrases\nTo avoid typing the passphrase each time a commit requires to be signed, it is possible to specify a caching duration the gpp agent config file, under ~/.gnupg/gpg-agent.conf.\nFor caching the passphrase for 400 days, create the config file with these two lines, where 34560000 is 400 times the number of seconds in a day.\ndefault-cache-ttl 34560000\nmaximum-cache-ttl 34560000\n\n\n\nkey-and-nib"
  },
  {
    "objectID": "posts/bp-2022-07-23-gpg/index.html#appendix-0-list-key-creation",
    "href": "posts/bp-2022-07-23-gpg/index.html#appendix-0-list-key-creation",
    "title": "How to sign your commits",
    "section": "Appendix 0: list key creation",
    "text": "Appendix 0: list key creation\nTo undo the key creation of step 1 you can retrieve the list of existing keys with gpg -k, hen copy the key public id to clipboard, that is a string like this dummy 43525435HJJH5K2H3KJHK3452KJH65NBMBV in the output\npub   ed25519 2022-02-18 [SC] [expires: 2024-02-18]\n      43525435HJJH5K2H3KJHK3452KJH65NBMBV\nuid           [ultimate] Sebastiano Ferraris <seb@email.com>\nsub   cv25519 2022-02-18 [E] [expires: 2024-02-18]\nFinally delete public and private key with:\ngpg --delete-secret-key 43525435HJJH5K2H3KJHK3452KJH65NBMBV\ngpg --delete-key 43525435HJJH5K2H3KJHK3452KJH65NBMBV"
  },
  {
    "objectID": "posts/bp-2022-07-23-gpg/index.html#appendix-1-troubleshooting-on-mac",
    "href": "posts/bp-2022-07-23-gpg/index.html#appendix-1-troubleshooting-on-mac",
    "title": "How to sign your commits",
    "section": "Appendix 1: Troubleshooting on mac",
    "text": "Appendix 1: Troubleshooting on mac\nIf git commit fails to authenticate the git commit, with the following error message\nThen you have to redirect the GPG_TTY key to the local tty with:\nexport GPG_TTY=$(tty)\nIf this solves the problem, you will have to append it to your .bashrc."
  },
  {
    "objectID": "posts/bp-2022-07-23-gpg/index.html#appendix-2-export-public-and-private-keys",
    "href": "posts/bp-2022-07-23-gpg/index.html#appendix-2-export-public-and-private-keys",
    "title": "How to sign your commits",
    "section": "Appendix 2: Export public and private keys",
    "text": "Appendix 2: Export public and private keys\nHow do I know my public key?\nThis command will export an ascii armored version of the public key:\ngpg --output public.pgp --armor --export username@email\nAlso to export the secret key, there is a similar command:\ngpg --output private.pgp --armor --export-secret-key username@email"
  },
  {
    "objectID": "posts/bp-2022-07-23-gpg/index.html#appendix-3-trigger-gpg-passphrase-linux",
    "href": "posts/bp-2022-07-23-gpg/index.html#appendix-3-trigger-gpg-passphrase-linux",
    "title": "How to sign your commits",
    "section": "Appendix 3: Trigger gpg passphrase linux",
    "text": "Appendix 3: Trigger gpg passphrase linux\nOn some linux distributions, the prompt to insert the gpg passphrase does not pop up when you create a new commit. Git simply refuses to add a new non-signed commit.\nSo you will be in the situation where you have some code to commit, you know your passphrase, you have the gpg-agent on so you not need to retype the passphrase each time, but nobody is asking you for your passphrase when you are committing a message.\nA workaround is to trigger gpg to ask you for your passphrase for another reason (e.g. signing a file), after which your passphrase is stored in the gpg-agent and you will be free to create signed commits, as the passphrase is automatically retrieved from cache. The list of commands would be:\ngit commit -am \"new stuff\"  # this commit is not added and does not trigger the passphrase prompt\ncd \ntouch z_tmp.txt  # creating a dummy file to authenticate\ngpg -s z_tmp.txt  # this trigger the passphrase (and a y/n question to confirm your choice)\ncd &lt;repo you were working&gt;\ngit commit -am \"new stuff\"\nTo turn this workaround into a oneliner, you can create the dummy file z_tmp.txt in the root directory (as above), and then add the following line to your bash profile:\nalias gpg_trigger='gpg -s ~/z_tmp.txt'\nSo the next time, instead of running all the commands of the previous block, you can simply call the newly created alias gpg_trigger."
  },
  {
    "objectID": "posts/index_al.html",
    "href": "posts/index_al.html",
    "title": "Algorithms",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "posts/index_bp.html",
    "href": "posts/index_bp.html",
    "title": "Code development",
    "section": "",
    "text": "How to sign your commits\n\n\nSetting up gpg authentication while keeping separate work and personal projects\n\n\n\n\n\nJul 23, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Posts",
    "section": "",
    "text": "Python datetimes manipulations\n\n\nCode fragments for timestamps and timezones manipulation\n\n\n\n\n\nJan 24, 2024\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\nHaversine’s distance mathematics\n\n\nComputing the distance between two points on a sphere\n\n\n\n\n\nJan 13, 2024\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\nUsing polygons\n\n\nGeospatial data science basic operations and measurements\n\n\n\n\n\nOct 3, 2022\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\nHow well positioned is your office?\n\n\nTime to question your workplace location\n\n\n\n\n\nJul 29, 2022\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\nHow to sign your commits\n\n\nSetting up gpg authentication while keeping separate work and personal projects\n\n\n\n\n\nJul 23, 2022\n\n\n  \n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/index_bl.html",
    "href": "posts/index_bl.html",
    "title": "Blogging",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "posts/gds-2022-07-29-office-pos/index.html",
    "href": "posts/gds-2022-07-29-office-pos/index.html",
    "title": "How well positioned is your office?",
    "section": "",
    "text": "Employees location respect to their office - Kepler and seaborn\nThis article was first published on the 24 April 2022 on medium. A second attempt to render both LaTeX and code was tried over geods.hashnode.dev. The current version here on quarto is the maintained one."
  },
  {
    "objectID": "posts/gds-2022-07-29-office-pos/index.html#problem-statement",
    "href": "posts/gds-2022-07-29-office-pos/index.html#problem-statement",
    "title": "How well positioned is your office?",
    "section": "Problem statement",
    "text": "Problem statement\nGeospatial data science is the branch of data science transforming geospatial data into information, insights and knowledge through the application of the scientific method and algorithms development. In this tutorial we use its principles to find the answer to a very simple question:\n\nIs your company office optimally located in respect to the position of its employees?\n\nDiscovering that all the employees are located South in respect to the position of the office, would inform the decision of saving everyone time and money.\nYou will be using the following components:\n\nA python interpreter with the required libraries pre-installed — virtualenv or conda environment.\nKepler GL, for results visualisation and user queries\nThe ukcommute dataset from Keper Gl website.\nosmnx to narrow down the dataset’s offices located within the borders of London.\nA handful of Python functions to compute distances and angles on the surface of a sphere and to visualise some results.\n\n(all the links are provided in the next section)."
  },
  {
    "objectID": "posts/gds-2022-07-29-office-pos/index.html#setup-and-requirements",
    "href": "posts/gds-2022-07-29-office-pos/index.html#setup-and-requirements",
    "title": "How well positioned is your office?",
    "section": "Setup and requirements",
    "text": "Setup and requirements\nThere are several options to create a python 3.9 environment. The code below uses conda:\nconda create -n geods python=3.9\nconda activate geods\nWith the environment activated, install the requirements listed below:\n# requirements.txt\ngeopandas==0.10.2\njupyter==1.0.0\nkeplergl==0.3.2\nmatplotlib==3.5.1\nosmnx==1.1.2\npandas==1.4.2\nseaborn==0.11.2\nYou can install each library individually with the pip command\npip install &lt;copy paste each line here&gt;\nor you can copy paste all the requirements in a file in the root of your project requirements.txt and install them all in one go via:\npip install -r requirements.txt\nNow all should be ready and set up to go!\nBefore going to the actual code, as the interactive maps of Kepler can not be embedded in the static webpage of quarto (yet!) we set a notebook variable to suppress the KeplerGl outputs and return a screenshots instead.\nThis is to improve the appearance of the article published on the blog. You can turn the variable to True when running the code from the notebook, and have access to the interactive maps.\n\nKEPLER_OUTPUT = False  # Render kepler output if True. Produces a screenshot otherwise"
  },
  {
    "objectID": "posts/gds-2022-07-29-office-pos/index.html#download-and-visualise-the-dataset",
    "href": "posts/gds-2022-07-29-office-pos/index.html#download-and-visualise-the-dataset",
    "title": "How well positioned is your office?",
    "section": "Download and visualise the dataset",
    "text": "Download and visualise the dataset\nAfter importing the required library with…\n\nfrom IPython.display import Image\n\nimport contextily as cx\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport osmnx\nimport seaborn as sns\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nimport pandas as pd\n\nfrom keplergl import KeplerGl\nfrom shapely.geometry import shape\n\n…we can download the chosen dataset using pandas read_csv method:\n\n# About 3 seconds\n!curl https://raw.githubusercontent.com/keplergl/kepler.gl-data/master/ukcommute/data.csv -o ./z_commuters_data.csv\ndf_commuter = pd.read_csv(\"./z_commuters_data.csv\")\n\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100 28.1M  100 28.1M    0     0  8369k      0  0:00:03  0:00:03 --:--:-- 8378k\n\n\n\ndf_commuter.head()\n\n\n\n\n\n\n\n\nworkplace_lng\nworkplace_lat\nresidence_lng\nall_flows\nresidence_lat\n\n\n\n\n0\n-2.992287\n53.410907\n-3.016558\n256\n53.487704\n\n\n1\n-3.016558\n53.487704\n-3.016558\n236\n53.487704\n\n\n2\n-2.993589\n53.450009\n-3.016558\n209\n53.487704\n\n\n3\n-3.026089\n53.478898\n-3.016558\n188\n53.487704\n\n\n4\n-2.977089\n53.406397\n-3.016558\n142\n53.487704\n\n\n\n\n\n\n\n\nconfig = {\n    'version': 'v1',\n    'config': {\n        'mapState': {\n            'latitude': 51.536265,\n            'longitude': -0.039740,\n            'zoom': 10\n        }\n    }\n}\n\nif KEPLER_OUTPUT:\n    map_1 = KeplerGl(data={'commuters': df_commuter}, config=config, height=800)\n    display(map_1)\nelse:\n    display(Image(\"images/map1.png\"))\n\n\n\n\n\n\n\n\nAt this point, you are invited to create the map on your own Jupyter notebook, to look into Kepler GL. This tool is developed by Uber, and if you have not used it before it is worth spending some time playing around with layers, colours and filters, and changing the map tiles (dark, light, satellite, etc…).\nKepler automatically recognises and formats the input DataFrame into independent layers, whose colour and features can be toggled and modified in the sidebar. In this case the input DataFrame is passed as a value of the input dictionary with key commuters, and it is, for now, the only layer.\nFor each row the dataset has two sets of points, workplace and residence, and each edge between two points on the same row is weighted by the integer column all_flows, that we can interpret as the number of times the commuter walked through the path over the period of the data collection. Note that this dataset has no timestamp, and from the dataset alone we can not tell the period the data was collected for.\nAlso, by observing the precision in the locations for multiple travels, and by the fact that there are workplaces in impossible locations, such as the centre of Richmond Park, we may suspect this is not real data (browse around the dataset to find more). Or if it is real data, some heavy pre-processing have happened beforehand, such as outlier detection, complete case analysis, and clustering or averaging of sets of nearby locations.\n\nA note on Kepler settings\nAfter changing the settings in the Kepler map, you can export them as a json config file with map_1.config, and re-import it when creating a new map. To limit the length of this post we have omitted the config used to re-create the images appearing across the post, but you can retrieve them from the config folder on the linked repo at:\nhttps://github.com/SebastianoF/GeoDsBlog/tree/master/posts/gds-2022-07-29-office-pos\nIf you are not following the code from the repo, or the configs from the kepler_config.py module are not imported, then the cells below will be loading the previous config file as its default."
  },
  {
    "objectID": "posts/gds-2022-07-29-office-pos/index.html#narrow-the-dataset-to-the-city-of-london",
    "href": "posts/gds-2022-07-29-office-pos/index.html#narrow-the-dataset-to-the-city-of-london",
    "title": "How well positioned is your office?",
    "section": "Narrow the dataset to the city of London",
    "text": "Narrow the dataset to the city of London\nEarly stage explorations indicate that the dataset covers the whole of England and Wales. Also toggling the layers, or making the workplace radii smaller and selecting a different colorscale than the residences, we can see that in many cases some some workplaces are coincident with the residences. For the moment, and for the goals of this post, this problem is of no concern.\nAs what we want to do is to assess the optimality of a location within the boundaries of London, the first thing to do is to download the city boundaries and then reduce the dataset to the offices located within these boundaries.\nThe geometry of a boundary can be encoded as a shapely polygon, and embedded into a geopandas dataframe: a pandas dataframe equipped with a column named geometry containing a shapely object. We can download the polygon from Open Street Map, the open source initiative providing the users with a free editable geographic database of the world.\nFear not, you are not asked to open a new tab on the browser and download the maps from the osm website! The maps can be downloaded programmatically via the very handy osmnx python library, and via its method osmnx.geocode_to_gdf.\nNote that we will query both London and the City of London geometries, in order to filter the workplace points in our dataset appearing in the union of these two regions.\n\nosmnx.config(use_cache=True, log_console=False)\n\ndef gdf_concat(list_gdf: list):\n    return gpd.GeoDataFrame( pd.concat(list_gdf, ignore_index=True)) \n\nquery_city = {'city': 'City of London'}\nquery_london = {'city': 'London'}\n\ngdf = gdf_concat([osmnx.geocode_to_gdf(query_city), osmnx.geocode_to_gdf(query_london)])\n\ngdf.head()\n\n\n\n\n\n\n\n\ngeometry\nbbox_north\nbbox_south\nbbox_east\nbbox_west\nplace_id\nosm_type\nosm_id\nlat\nlon\nclass\ntype\nplace_rank\nimportance\naddresstype\nname\ndisplay_name\n\n\n\n\n0\nPOLYGON ((-0.11383 51.51826, -0.11380 51.51812...\n51.523312\n51.506871\n-0.072762\n-0.113830\n243916402\nrelation\n51800\n51.515618\n-0.091998\nboundary\nadministrative\n12\n0.586511\ncity\nCity of London\nCity of London, Greater London, England, Unite...\n\n\n1\nPOLYGON ((-0.51038 51.46809, -0.51036 51.46795...\n51.691874\n51.286760\n0.334016\n-0.510375\n243408926\nrelation\n65606\n51.507446\n-0.127765\nplace\ncity\n16\n0.830783\ncity\nLondon\nLondon, Greater London, England, United Kingdom\n\n\n\n\n\n\n\nBefore continuing, it would be good to visualise the polygons we just downloaded. There are multiple ways to do this. For example we can use contextility as a basemap and use the embedded method plot of a geopandas dataframe.\n\ngdf_epsg = gdf.to_crs(epsg=3857)\nax = gdf_epsg.plot(figsize=(10, 10), alpha=0.5, edgecolor='k')\ncx.add_basemap(ax, source=cx.providers.CartoDB.DarkMatterNoLabels)\n\n\n\n\n\n\n\n\n\ntry:\n    from kepler_config import config_map_2\nexcept ImportError:\n    config = config_map_2\n\n\nif KEPLER_OUTPUT:\n    map_2 = KeplerGl(data={'london' :gdf_epsg}, config=config, height=800)  # kepler knows what to do when fed with a geodataframe\n    display(map_2)\nelse:\n    display(Image(\"images/map2.png\"))\n\n\n\n\n\n\n\n\nTo narrow the commuter dataset to only the offices within the boundary of London, first we cast df_commuter into a geopandas DataFrame with the geometry column having for each entry the shapely point of the workplace coordinate.\nThen we use this new geodataframe to obtain the mask of the point within the boundaries, and finally we apply the mask to the initial df_commuter dataset. Since we are here, we also take a look at the percentage of the offices that are in London over the full dataset (you are of course invited to look into other statistics).\n\n# -- about 17 seconds --\ngdf_commuters_workplace = gpd.GeoDataFrame(df_commuter.copy(), geometry=gpd.points_from_xy(df_commuter.workplace_lng, df_commuter.workplace_lat))\n\n# -- about 120 seconds: points in polygon \nmask_points_in_city = gdf_commuters_workplace.intersects(gdf.geometry.iloc[0])\nmask_points_in_london = gdf_commuters_workplace.intersects(gdf.geometry.iloc[1])\n\n\nnum_total_rows = len(gdf_commuters_workplace)\nnum_rows_in_city = len(mask_points_in_city[mask_points_in_city == True])\nnum_rows_in_london = len(mask_points_in_london[mask_points_in_london == True])\nprint(f\"Number of rows for offices in the city {num_rows_in_city} ({100 * num_rows_in_city / num_total_rows} %)\")\nprint(f\"Number of rows for offices in london {num_rows_in_london} ({100 * num_rows_in_london / num_total_rows} %)\")\n\nmask_union = mask_points_in_city | mask_points_in_london\nnum_rows_in_union = mask_union.sum()\nprint(f\"Number of offices in the union of London and the City {num_rows_in_union} ({100 * num_rows_in_union / num_total_rows} %)\")\n\n# Sanity check\nassert num_rows_in_union == num_rows_in_city + num_rows_in_london\n\ndf_commuter_london_office = df_commuter[mask_union]\ndf_commuter_london_office.reset_index(inplace=True, drop=True)\n\ndf_commuter_london_office.head()\n\nNumber of rows for offices in the city 4507 (0.663142267337411 %)\nNumber of rows for offices in london 139512 (20.52724739311668 %)\nNumber of offices in the union of London and the City 144019 (21.19038966045409 %)\n\n\n\n\n\n\n\n\n\nworkplace_lng\nworkplace_lat\nresidence_lng\nall_flows\nresidence_lat\n\n\n\n\n0\n-0.096124\n51.502618\n-2.258075\n14\n53.406096\n\n\n1\n-0.064385\n51.490614\n-1.633650\n11\n54.380373\n\n\n2\n-0.064385\n51.490614\n-1.731036\n27\n54.408769\n\n\n3\n-0.094986\n51.519522\n-1.818510\n14\n53.925985\n\n\n4\n-0.094986\n51.519522\n-1.081245\n14\n53.958359\n\n\n\n\n\n\n\n\ntry:\n    from kepler_config import config_map_3\nexcept ImportError:\n    config_map_3 = config\n\n\nif KEPLER_OUTPUT:\n    # Use the config_3 in kepler_config.py in the repo to reproduce the same image\n    map_3 = KeplerGl(data={'london':gdf_epsg.copy(),  \"commuters\": df_commuter_london_office.copy()}, config=config_map_3, height=800)\n    display(map_3)\nelse:\n    display(Image(\"images/map3.png\"))\n\n\n\n\n\n\n\n\nAs noted before, many of the workplaces and residences have the same coordinates and are overlapping on the map. Also in the dataset reduced to the boundaries of London residences and offices are overlapping, but outside the boundary, as expected, there are only residences."
  },
  {
    "objectID": "posts/gds-2022-07-29-office-pos/index.html#select-the-office-location",
    "href": "posts/gds-2022-07-29-office-pos/index.html#select-the-office-location",
    "title": "How well positioned is your office?",
    "section": "Select the office location",
    "text": "Select the office location\nNow the goal is to select a single office of interest (or a group of nearby offices), and to examine its location in respect to the location of the employees. The functionality of drawing polygons provided by Kepler Gl comes in handy for this purpose.\nOn the top right menu, selecting the “draw on map” button we can narrow down a region and copy its geometry to clipboard right-clicking on the completed polygon. We can then paste the geometry in the cell of our Jupyter notebook.\nThe geometry, encoded in a json, can be then parsed into a shapely polygon.\nAs an example we copy pasted two polygons in the cell below (you are invited to continue the tutorial with your own regions though!). The first one encompasses an office in St Luke’s Close, the closest office to the silicon roundabout, Old Street, and the second one in Albert Road, just between the London city airport and the river Thames.\n\npolygon_st_luke_office = {\"type\":\"Polygon\",\"coordinates\":[[[-0.0930210043528368,51.52553386809767],[-0.09362754938510826,51.5257442611004],[-0.09398505401347826,51.52546150215205],[-0.09363181940230854,51.525218817282784],[-0.09313761642997592,51.52527679524477],[-0.0930210043528368,51.52553386809767]]]}\n\npolygon_albert_road = {\"type\":\"Polygon\",\"coordinates\":[[[0.05074120549614755,51.503014231092195],[0.04882522609357891,51.50189434877025],[0.051410997081145014,51.49996117091324],[0.05337913172491038,51.501678115383754],[0.05074120549614755,51.503014231092195]]]}\n\nNext we narrow the office locations to the selected geometry, embed the geometry in a geopandas DataFrame, and feed the result to a Kepler map to visualise where we are at.\n\n# narrow dataset to the geometry\nmask_st_luke_office = gdf_commuters_workplace.intersects(shape(polygon_st_luke_office))\ndf_commuters_st_luke_office = df_commuter[mask_st_luke_office]\n\n# embed shape into a geopandas to visualise in kepler\ngdf_st_luke_geometry = gpd.GeoDataFrame({'geometry':[shape(polygon_st_luke_office)], \"display_name\": [\"St Luke's Close Office\"]})\n\n\n# Same for Albert Road office\n\nmask_albert_road = gdf_commuters_workplace.intersects(shape(polygon_albert_road))\ndf_commuters_albert_road = df_commuter[mask_albert_road]\n\ngdf_albert_road = gpd.GeoDataFrame({'geometry':[shape(polygon_albert_road)], \"display_name\": [\"St Luke's Close Office\"]})\n\n\ntry:\n    from kepler_config import config_map_4\nexcept ImportError:\n    config_map_4 = config\n\n\nif KEPLER_OUTPUT:\n    map_4 = KeplerGl(\n        data={\n            \"St Luke's Close Office\": gdf_st_luke_geometry.copy(),  \n            \"commuters to St Luke\": df_commuters_st_luke_office.copy(),\n            \"Albert Road Office\": gdf_albert_road.copy(),\n            \"commuters to Albert\": df_commuters_albert_road.copy(),\n        }, \n        config=config_map_4, \n        height=800)  # kepler knows what to do when fed with a geodataframe\n    display(map_4)\nelse:\n    display(Image(\"images/map4.png\"))\n\n\n\n\n\n\n\n\n\nprint(\n    f\"Commuters to St Luke office {len(df_commuters_st_luke_office)} ({round(100 * len(df_commuters_st_luke_office) / len(df_commuter), 4)} %)\" \n)\nprint(\n    f\"Commuters to Albert Road office {len(df_commuters_albert_road)} ({round(100 *  len(df_commuters_albert_road) / len(df_commuter), 4)} %)\"\n)\n\nCommuters to St Luke office 2197 (0.3233 %)\nCommuters to Albert Road office 225 (0.0331 %)"
  },
  {
    "objectID": "posts/gds-2022-07-29-office-pos/index.html#compute-bearing-and-distance-of-all-the-commuters-to-the-selected-office",
    "href": "posts/gds-2022-07-29-office-pos/index.html#compute-bearing-and-distance-of-all-the-commuters-to-the-selected-office",
    "title": "How well positioned is your office?",
    "section": "Compute bearing and distance of all the commuters to the selected office",
    "text": "Compute bearing and distance of all the commuters to the selected office\nNow that we have the dataset of commuters to two custom selected offices in London, we want to compute the bearing and distance from the office to each employees residence.\nThere are multiple ways to achieve this result, such as using one of the many python libraries, as haversine, geopy, geographiclib… As our goal is to learn some tools from geodata science, we will try to implement the formulae from scratch.\nGiven two points (lng1, lat1) and (lng2, lat2), the Haversine formula (geodesic distance on the sphere) is given by:\n\\[\n\\mathcal{H} = 2 R \\arcsin\\left(\\sqrt{d}\\right)\n\\]\nwhere\n\\[\nd = \\sin^2 \\left(\\frac{\\Delta \\text{lat}}{2} \\right) + \\cos(\\text{lat1}) \\cos(\\text{lat2})  \\sin^2\\left(\\frac{\\Delta \\text{lon}}{2}\\right)\n\\]\nand \\(\\Delta \\text{lat} = \\text{lat1} - \\text{lat2}\\), \\(\\Delta \\text{lon} = \\text{lon1} - \\text{lon2}\\), and \\(R\\) is the hearth’s radius1.\nThe formula for the bearing, as the angle formed by the geodesics between the north pole and (lng1, lat1), and the geodesic between (lng1, lat1) and (lng2, lat2) is, in radiants:\n\\[\n\\mathcal{B} = \\arctan\\left(\n    \\frac{\n        \\sin(\\Delta \\text{lon}) \\cos(\\text{lat2})\n    }{\n        \\cos(\\text{lat1}) \\sin(\\text{lat2}) - \\sin(\\text{lat1}) \\cos(\\text{lat2}) \\cos\\left( \\Delta \\text{lon} \\right)\n    }\n\\right)\n\\]\nBoth formulae are based on the spherical model, which is a reasonable approximation, though in geospatial data science the standard model is the ellipsoid model World Geodesics System 1984 (WGS84), where the distance between the centre and the equator is higher than the distance between the centre and the poles by about 31 Km.\nExposing the reason why these formulae above are correct, and generalising them for the WGS84 model (the Vincenty’s formulae), would entail expanding the blog post beyond reason. This topic is therefore left as future work, or to the reader.\n\nfrom typing import Tuple\n\nfrom math import radians\n\ndef haversine(lng1: float, lat1: float, lng2: float, lat2: float) -&gt; Tuple[float, float]:\n    \"\"\" returns (haversine distance in km, bearing in degrees from point 1 to point 2), vectorised \"\"\"\n\n    avg_earth_radius_km = 6371.0072\n   \n    lng1, lat1, lng2, lat2 = map(np.deg2rad, [lng1, lat1, lng2, lat2])\n    d_lat, d_lng = lat2 - lat1, lng2 - lng1\n    d = np.sin((d_lat)/2)**2 + np.cos(lat1)*np.cos(lat2) * np.sin((d_lng)/2)**2\n    hav_dist = 2 * avg_earth_radius_km * np.arcsin(np.sqrt(d))\n   \n    y = np.sin(d_lng) * np.cos(lat2)\n    x = np.cos(lat1) * np.sin(lat2) - np.sin(lat1) * np.cos(lat2) * np.cos(d_lng)\n    bearing = (np.arctan2(y, x) + 2 * np.pi) % (2 * np.pi)\n    \n    return hav_dist, np.rad2deg(bearing)\n\n\ndef add_bearing_deg_and_distance_km(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"bearing between A and B is the angle between the geodesics connecting A and the north pole, and the geodesics connecting A and B.\n    Both the bearing and distance are computed on the Spherical model.\n    \"\"\"\n    df = df.copy()\n    \n    lng_work, lat_work = df.workplace_lng.to_numpy(), df.workplace_lat.to_numpy()\n    lng_home, lat_home = df.residence_lng.to_numpy(), df.residence_lat.to_numpy()\n    \n    df[\"distance_km\"], df[\"bearing_deg\"] = haversine(lng_work, lat_work, lng_home, lat_home)\n    \n    return df\n\n\ndf_commuters_st_luke_office = add_bearing_deg_and_distance_km(df_commuters_st_luke_office)\ndf_commuters_albert_road = add_bearing_deg_and_distance_km(df_commuters_albert_road)\n\ndisplay(df_commuters_st_luke_office.head())\ndisplay(df_commuters_albert_road.head())\n\n\n\n\n\n\n\n\nworkplace_lng\nworkplace_lat\nresidence_lng\nall_flows\nresidence_lat\ndistance_km\nbearing_deg\n\n\n\n\n122746\n-0.093427\n51.525514\n0.100309\n14\n51.669006\n20.824387\n39.910497\n\n\n122840\n-0.093427\n51.525514\n0.081235\n16\n51.654781\n18.767046\n39.943704\n\n\n122960\n-0.093427\n51.525514\n0.013113\n15\n51.683825\n19.079225\n22.642044\n\n\n123150\n-0.093427\n51.525514\n0.112969\n21\n51.694097\n23.548401\n37.165113\n\n\n123277\n-0.093427\n51.525514\n0.243324\n13\n51.708458\n30.893113\n48.684224\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nworkplace_lng\nworkplace_lat\nresidence_lng\nall_flows\nresidence_lat\ndistance_km\nbearing_deg\n\n\n\n\n123573\n0.05024\n51.502045\n0.069103\n10\n51.642368\n15.657587\n4.768539\n\n\n127534\n0.05024\n51.502045\n0.321880\n12\n51.474405\n19.057378\n99.174466\n\n\n127640\n0.05024\n51.502045\n0.365564\n13\n51.464030\n22.240404\n100.833071\n\n\n127928\n0.05024\n51.502045\n0.252849\n15\n51.482116\n14.201101\n98.898103\n\n\n128083\n0.05024\n51.502045\n0.327369\n12\n51.479152\n19.355094\n97.449008"
  },
  {
    "objectID": "posts/gds-2022-07-29-office-pos/index.html#visualise-the-results-in-a-radar-histogram-plot",
    "href": "posts/gds-2022-07-29-office-pos/index.html#visualise-the-results-in-a-radar-histogram-plot",
    "title": "How well positioned is your office?",
    "section": "Visualise the results in a radar-histogram plot",
    "text": "Visualise the results in a radar-histogram plot\nFor an distance and bearing effective visualisation, a circular histogram would do what we need. The polar visualisation of matplotlib will do this for us.\nWe group the dataset into three categories, according to their radial distance from the office: - Within a radius of 10 Km - Between 10Km and 20 Km - Above 20 Km\nThen we compute the histograms in polar coordinates. As we have to do it for two different dataset, instead of repeating the same code twice, we embed the transformation and visualisation in a single function.\nWithin the same function there is some repeated code for each radius. It is left to you to embed the repeated code in a for cycle for the inner, between and outer histogram, and to generalise it to any number of radii and distances.\nAs a further exercise, separating the data pre-processing and the visualisation part in two different function would be better practice than keeping both steps in the same function.\n\nsns.set_style(\"darkgrid\", {\"grid.color\": \".6\", \"grid.linestyle\": \":\"})\n\ndef radar_histogram(ax, df):\n    \"\"\"\n    Input: \n        df with at least 2 columns distance_km and bearing_deg.\n    Output: radar histogram plot.\n    \"\"\"\n    # Figures parameter\n    directions = 40\n    \n    bottom = 4\n    height_scale = 8\n    \n    # bearing: degrees from nort pole clockwise\n    bearing_bins = np.linspace(0, 360, directions+1, endpoint=False)\n    # angle visualisation: rad from east counterclockwise\n    theta = - 1 * np.linspace(0, 2 * np.pi, directions, endpoint=False) + np.pi/2\n    width = (2*np.pi) / directions\n    \n    # data binning\n    se_bins = pd.cut(df[\"bearing_deg\"].to_numpy(), bearing_bins)\n    np_bins = se_bins.value_counts().to_numpy()\n    bins =  height_scale * np.array(np_bins) / np.max(np_bins)\n    \n    # Uncomment to debug figure:\n    # bins = range(directions)\n    \n    # plotting    \n    ax_bars = ax.bar(theta, bins, width=width, bottom=bottom, color=\"blue\")\n\n    ax.set_yticklabels([])\n    ax.set_xticks(np.linspace(0, 2 * np.pi, 8, endpoint=False))\n    ax.set_xticklabels(['E', '', 'N', '', 'W', '', 'S', ''])\n    ax.grid(False)\n\n    return ax\n\ndef radar_histogram_3_levels(ax, df):\n    \"\"\"\n    Input: \n        df with at least 2 columns distance_km and bearing_deg.\n    Output: radar histogram plot.\n    \"\"\"\n    # Figures parameter\n    directions = 40\n    height_scale = 2\n\n    bottom_inner = 2\n    bottom_betw = 5\n    bottom_outer = 8\n    \n    # bearing: degrees from nort pole clockwise\n    bearing_bins = np.linspace(0, 360, directions+1, endpoint=False)\n    # angle visualisation: rad from east counterclockwise\n    theta = - 1 * np.linspace(0, 2 * np.pi, directions, endpoint=False) + np.pi/2\n    width = (2*np.pi) / directions\n    \n    # data binning\n    \n    df_inner = df[df[\"distance_km\"] &lt;= 10]\n    se_bins_inner = pd.cut(df_inner[\"bearing_deg\"].to_numpy(), bearing_bins)\n    np_bins_inner = se_bins_inner.value_counts().to_numpy()\n    bins_inner =  height_scale * np.array(np_bins_inner) / np.max(np_bins_inner)\n    \n    df_betw = df[(df[\"distance_km\"] &gt; 10) & (df[\"distance_km\"] &lt;= 20)]\n    se_bins_betw = pd.cut(df_betw[\"bearing_deg\"].to_numpy(), bearing_bins)\n    np_bins_betw = se_bins_betw.value_counts().to_numpy()\n    bins_betw =  height_scale * np.array(np_bins_betw) / np.max(np_bins_betw)\n    \n    df_outer = df[df[\"distance_km\"] &gt; 20]\n    se_bins_outer = pd.cut(df_outer[\"bearing_deg\"].to_numpy(), bearing_bins)\n    np_bins_outer = se_bins_outer.value_counts().to_numpy()\n    bins_outer =  height_scale * np.array(np_bins_outer) / np.max(np_bins_outer)\n    \n    # plotting\n    \n    ax_bars_inner = ax.bar(theta, bins_inner, width=width, bottom=bottom_inner, color=\"blue\")\n    ax_bars_betw = ax.bar(theta, bins_betw, width=width, bottom=bottom_betw, color=\"blue\")\n    ax_bars_outer = ax.bar(theta, bins_outer, width=width, bottom=bottom_outer, color=\"blue\")\n\n    \n    ax.set_yticklabels([])\n    # uncomment to add values on radius axis\n    # ax.set_yticks(np.arange(0,10,1.0))\n    # ax.set_yticklabels(['', '', '', '&lt;=10Km ', '', '', '&gt;10Km\\n &lt;=20Km', '', '', '&gt;20Km'])\n\n    ax.set_xticks(np.linspace(0, 2 * np.pi, 8, endpoint=False))\n    ax.set_xticklabels(['E', '', 'N', '', 'W', '', 'S', ''])\n    ax.grid(False)\n\n    return ax\n\n\nfig = plt.figure(figsize=(12, 12))\n\nax11 = fig.add_subplot(221, polar=True)\nax11 = radar_histogram(ax11, df_commuters_st_luke_office)\nax11.set_title(\"Saint Luke Office\", y=1.08, x=1.1)\n\nax12 = fig.add_subplot(222, polar=True)\nax12 = radar_histogram_3_levels(ax12, df_commuters_st_luke_office)\n\n\nax21 = fig.add_subplot(223, polar=True)\nax21 = radar_histogram(ax21, df_commuters_albert_road)\nax21.set_title(\"Albert Road Office\", y=1.08, x=1.1)\n\nax22 = fig.add_subplot(224, polar=True)\nax22 = radar_histogram_3_levels(ax22, df_commuters_albert_road)\n\nplt.plot()\n\n\n\n\n\n\n\n\nRadar histogram plot with the distribution of the location of the employees respect to the office, at the centre of the diagram. In the first column all the residences are shown regardless of the distance from the centre. In the second column the residences are split by distance from the centre: inner circle &lt; 10Km, mid circle between 10 and 20 Km, outer circle &gt; 20 Km.\nFrom the graphs above we can see that the office in Saint Luke is reasonably well balance across the location of the employees, overall, and splitting the commuters into three categories based on radial distance.\nThe same can not be said for the office located in Albert road office, whose employees should consider to relocate to an office further North-East."
  },
  {
    "objectID": "posts/gds-2022-07-29-office-pos/index.html#can-we-do-better",
    "href": "posts/gds-2022-07-29-office-pos/index.html#can-we-do-better",
    "title": "How well positioned is your office?",
    "section": "Can we do better?",
    "text": "Can we do better?\nFrom the question “Is your company office optimally located in respect to the position of its employees?”, we developed a small example of the geospatial data science capabilities to visualise the employees distribution around in respect to the position of their office, via the computation of bearing and distance, to see how off-center it can be, and in which direction it should be relocated to reduce the commuting distance for each employee. In doing so we showed how to download city boundaries from the OSM python API, how to intersect points in polygons, and how visualise geospatial data with Keplerl GL.\nThere are several limitations that are worth mentioning. The first and most obvious one is that the bearing and distance between office and residence is not a the single metric to justify an office relocation. From the point of view of the employee, there are other factors that have not been considered, such as commuting time, cost, frequency of commute, as well as the employee position in the company hierarchy and its “can’t bother” factor. This last metric, is an empirical one of the will (or lack thereof) to make a change, it is entirely based upon the individual opinion, taking into account for example possible facilities in the new office, traffic, proximity to children’s schools and so on. All these parameters has to be considered for the current office location and the potential new office location.\nFrom the point of view of the employer, there is of course the cost of the new office, as well as the cost of the move, as well as prestige of location in respect to possible investors.\nThe last limitation is obviously the dataset. The whole post was written around the toy dataset downloaded from the of Kepler GL examples page. Is it realistic enough or reliable? Can we for example make further analysis on the dataset and obtain some statistics and insights about commuters’ habits? We already noticed that some, if not all commuting locations coincided with the location of the offices. Can we do some further analysis to know how little we should trust the data? To answer this question, we can end up with some minimal data analysis to the dataset to see if the ratio of number of offices in respect to the number of employees is convincing.\n\nnumber_of_offices = len(df_commuter.groupby([\"workplace_lng\", \"workplace_lat\"]).count())\nnumber_of_residences = len(df_commuter.groupby([\"residence_lng\", \"residence_lat\"]).count())\n\nnumber_of_offices_in_london_and_city = len(df_commuter_london_office.groupby([\"workplace_lng\", \"workplace_lat\"]).count())\nnumber_of_residences_commuting_to_london_and_city = len(df_commuter_london_office.groupby([\"residence_lng\", \"residence_lat\"]).count())\n\ncommuters_office_ratio = number_of_residences / number_of_offices\ncommuters_office_ratio_in_london_and_city = number_of_residences_commuting_to_london_and_city / number_of_offices_in_london_and_city\n\nprint(\n    f\"Number of offices in london and the city {number_of_offices_in_london_and_city} ({round(100 * number_of_offices_in_london_and_city / number_of_offices, 4)} %)\"\n)\nprint(\n    f\"Number of residences commuting to london and the city {number_of_residences_commuting_to_london_and_city} ({round(100 * number_of_residences_commuting_to_london_and_city / number_of_residences, 4)} %)\"\n)\nprint(\n    f\"Number of commuters residences per office {commuters_office_ratio}\"\n)\nprint(\n    f\"Number of commuters residences per office in london and the city {round(commuters_office_ratio_in_london_and_city, 4)}\"\n)\n\nNumber of offices in london and the city 983 (13.6509 %)\nNumber of residences commuting to london and the city 2678 (37.1893 %)\nNumber of commuters residences per office 1.0\nNumber of commuters residences per office in london and the city 2.7243\n\n\nCertainly the ratio of commuter’s residences per office can tell us that there is something wrong with the dataset. We could speculated about the fact that the dataset is synthetically generated, or that the arrows direction was swapped when generating the dataset, or both. With no further information, we can only say that no analysis on this dataset can provide us with any reasonable answers or statistics to questions related to commuters in the UK. Nonetheless it is a useful dataset for visualistaion and toy exercises such as the one just presented."
  },
  {
    "objectID": "posts/gds-2022-07-29-office-pos/index.html#resources",
    "href": "posts/gds-2022-07-29-office-pos/index.html#resources",
    "title": "How well positioned is your office?",
    "section": "Resources:",
    "text": "Resources:\n\nhttps://geopandas.org/en/stable/index.html\nhttps://stackoverflow.com/questions/65064351/python-osmnx-how-to-download-a-city-district-map-from-openstreetmap-based-on-t\nhttps://gis.stackexchange.com/questions/343725/convert-geojson-to-geopandas-geodataframe\nhttps://stackoverflow.com/questions/4913349/haversine-formula-in-python-bearing-and-distance-between-two-gps-points\nhttps://anitagraser.github.io/movingpandas/\nhttps://stackoverflow.com/questions/17624310/geopy-calculating-gps-heading-bearing\nhttps://geodesy.noaa.gov/CORS/\nhttps://stackoverflow.com/questions/22562364/circular-polar-histogram-in-python\nhttps://stackoverflow.com/questions/12750355/python-matplotlib-figure-title-overlaps-axes-label-when-using-twiny\nhttps://www.dexplo.org/jupyter_to_medium/"
  },
  {
    "objectID": "posts/gds-2022-07-29-office-pos/index.html#also-source-of-inspiration-for-writing-this-blog-post",
    "href": "posts/gds-2022-07-29-office-pos/index.html#also-source-of-inspiration-for-writing-this-blog-post",
    "title": "How well positioned is your office?",
    "section": "Also source of inspiration for writing this blog post:",
    "text": "Also source of inspiration for writing this blog post:\n\nMaxime Labonne\nKhuyen Tran\nAbdishakur\nHerbert Lui\nMaciej Tarsa"
  },
  {
    "objectID": "posts/gds-2022-10-03-polygons-part-1/index.html",
    "href": "posts/gds-2022-10-03-polygons-part-1/index.html",
    "title": "Using polygons",
    "section": "",
    "text": "This article was first published on the 10 October 2022 on geods.hashnode.dev. The version presented here is the maintained one."
  },
  {
    "objectID": "posts/gds-2022-10-03-polygons-part-1/index.html#setting-up-the-environment",
    "href": "posts/gds-2022-10-03-polygons-part-1/index.html#setting-up-the-environment",
    "title": "Using polygons",
    "section": "Setting up the environment",
    "text": "Setting up the environment\nCreate a python 3.9 environment called venv and activate it. There are several options, the code below is to create an environment via virtualenv, as quicker than conda, and more than enough for these small experiments:\nvirtualenv venv -p python3.9\nsource venv/bin/activate\nWith the environment activated, install the requirements below. You can install each library individually with pip pip install &lt;copy paste each line here&gt;, or you can copy paste all the requirements in a file in the root of your project requirements.txt and install them all in one go via pip install -r requirements.txt.\n# requirements.txt\ngeopandas==0.11.1\njupyter==1.0.0\nkeplergl==0.3.2\nmatplotlib==3.6.0\nosmnx==1.2.2\nshapely==1.8.4"
  },
  {
    "objectID": "posts/gds-2022-10-03-polygons-part-1/index.html#polygons-intersecting-water-and-land.",
    "href": "posts/gds-2022-10-03-polygons-part-1/index.html#polygons-intersecting-water-and-land.",
    "title": "Using polygons - part 1",
    "section": "1.1 Polygons intersecting water and land.",
    "text": "1.1 Polygons intersecting water and land.\nLand and water on a map are typically modelled by polygons through the segmentation of the underlying geographical features. Given a polygon or a bounding box drawn on a map as input, the goal is to know want to know what are the polygons of land and water this polygon intersects, and, after trimming them to the given geometry, we want to measure the percentage of land respect to the percentage of water. For the example below we will use a polygon drawn around the city of Genova, Italy, (44.405551, 8.904023).\nYou are invited to reproduce the results using a different city.\n\n\nfrom copy import deepcopy\n\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport osmnx\nimport pandas as pd\nimport shapely\n\nfrom geopy import distance\nfrom keplergl import KeplerGl\nfrom shapely.geometry import Polygon, Point, shape\n\n\n# creates an empty map where to draw the polygon\nmap_0 = KeplerGl(height=800)\ndisplay(map_0)\n\n\n# you can create a different polygon with kepler, using the drawing pencil (top right), then right click on it to copy the values and paste it in a dictionary.\ndict_roi = {\"type\":\"Polygon\",\"coordinates\":[[[8.987382009236233,44.54680601507832],[8.841126105836173,44.550231465826634],[8.723022747221052,44.53065474626796],[8.621398927017932,44.48511343018655],[8.591873087363503,44.3865638718846],[8.64817817693631,44.307506289856086],[8.75666847147733,44.33796344890003],[8.83906616353428,44.35613196160054],[8.950989695244745,44.3531860988552],[9.023087675794352,44.30554076888227],[9.079392765366244,44.274083482671955],[9.120591611394707,44.288339652995674],[9.143937624144632,44.33108740800724],[9.141877681842741,44.39784910785984],[9.090379124307875,44.49344084451141],[8.987382009236233,44.54680601507832]]]}\n\nsh_roi = shape(dict_roi)\n\nconfig_map1 = {\n    'version': 'v1',\n    'config': {\n        'mapState': {\n            \"bearing\": 0,\n            \"dragRotate\": False,\n            \"latitude\": 44.41404004333898,\n            \"longitude\": 8.871549089955757,\n            \"pitch\": 0,\n            \"zoom\": 10.347869665266995,\n            \"isSplit\": False,\n        }\n    }\n}\n\ngdf_roi =  gpd.GeoDataFrame({\"name\":[\"ROI\"], \"geometry\": [sh_roi]})\ngdf_roi = gdf_roi.set_crs('4326')  # reproject to WGS84 (the only one supported by kepler)\n\n# To reproduce exactly the same images shown in the article you will have to copy the config\n# file from the linked repo and save it in the save it in a configs.py file.\n# You can still follow the tutorial and run all the lines of code without the config.\ntry:\n    from configs import config_map1\nexcept ImportError:\n    config_map1 = dict()\n\nmap_1 = KeplerGl(data=deepcopy({\"roi\": gdf_roi}), config=config_map1, height=800)\ndisplay(map_1)\n\nThe copy pasted geometry from the kepler app is a dictionary with a type and a list of coordinates following a conventional GeoJSON object. GeoJSON is a format for encoding data about geographic features using JavaScript Object Notation (json), established in 2015.\nA geojson to model a single point appears as:\n{\n  \"type\": \"Feature\",\n  \"geometry\": {\n    \"type\": \"Point\",\n    \"coordinates\": [125.6, 10.1]\n  },\n  \"properties\": {\n    \"name\": \"Dinagat Islands\"\n  }\n}\nWhere \"type\" can be Feature for single objects, or FeatureCollections for multiple objects, and where the geometry.type can be a Point, LineString, Polygon, MultiPoint, MultiLineString, and MultiPolygon.\nTo get the “water” within the selected geometry, we will use the osmnx library.\nThere are a few options to obtain land and water intersecting a region. It is possible to get the rivers, the sea (or bay), and the administrative regions and then consider to subtract the rivers to the administrative regions, and their intersection with the selected ROI. This option may leave some empty spaces, as in OSM there may be multiple annotators and the boundaries may not collimate. A different approach is to assume that all what is not water is land. So we query the rivers and the sea, we consider their union and their intersection with the ROI, and we call this new region “water”. The land will be the set difference between water and the ROI.\n\\[\n\\text{water}_{\\text{roi}} = (\\text{sea} \\cup \\text{river}) \\cap \\text{roi}\\\\\n\\text{land}_{\\text{roi}} = \\text{roi} \\setminus \\text{water}_{\\text{roi}}\n\\]\nSo we need to perform the binary polygon operations of union, intersection and subtraction of polygons, as well as to get the polygons of sea and rivers involved in the selected roi.\n\n# how to get tags:\n# https://wiki.openstreetmap.org/wiki/Map_features#Water_related\n\n# First run would take ~5 mins (2 seconds after the first run, as results are stored in the local folder `cache`).\ngdf_rivers = osmnx.geometries_from_polygon(sh_roi, tags={'natural': 'water'})\ngdf_sea = osmnx.geometries_from_polygon(sh_roi, tags={'natural': 'bay'})\n\n# check crs is already set to WGS84\nassert gdf_rivers.crs == 4326\nassert gdf_sea.crs == 4326\n\n# Dissolve the \"geodataframe\" so that all of its rows are conflated into a single observation\ngdf_rivers = gdf_rivers.dissolve()\ngdf_sea = gdf_sea.dissolve()\n\n# Compute water_roi and land_roi\ngse_sea_union_river = gdf_rivers.union(gdf_sea)  # The union of two geodataframes is a geoseries (the union is only on the geometries)\n\ngse_water_roi = gse_sea_union_river.intersection(gdf_roi)\ngse_land_roi = gdf_roi.difference(gse_water_roi)\n\nyou can also explore the administrative boundaries for an alternative route to find the polygons segmenting land.\ngdf_region = ox.geometries_from_polygon(sh_poly, tags={'boundary': 'administrative'})\nBut this route is not explored here, and it is left for the reader to investigate and plot.\n\n# Call deepcopy when passing an object prevents issue \"AttributeError: 'str' object has no attribute '_geom'\".\n# https://github.com/keplergl/kepler.gl/issues/1240\n\ngdf_genova = gpd.GeoDataFrame(\n    {\n        \"name\": [\"roi\", \"water\", \"land\"], \n        \"geometry\":[deepcopy(gdf_roi.geometry.iloc[0]), deepcopy(gse_water_roi.iloc[0]), deepcopy(gse_land_roi.iloc[0])]\n    }\n)\n\ngdf_genova = gdf_genova.set_crs('4326')\n\n\nkepler_data = {}\nfor _, row in gdf_genova.iterrows():\n    kepler_data.update({row[\"name\"]: gdf_genova[gdf_genova[\"name\"] == row[\"name\"]].copy()})\n\n\ntry:\n    from configs import config_map2\nexcept ImportError:\n    config_map2 = dict()\n\n\nmap_2 = KeplerGl(\n    data=deepcopy(kepler_data), \n    height=800,\n    config=config_map2,\n)\nmap_2\n\n\n# Reproject to cylindrical equal area projectcion\ngdf_genova[\"areas (Km2)\"] = gdf_genova.to_crs({'proj':'cea'}).area/ 10**6\ngdf_genova[[\"name\", \"areas (Km2)\"]]  # Table 1"
  },
  {
    "objectID": "posts/gds-2022-10-03-polygons-part-1/index.html#polygons-intersecting-water-and-land-nearby",
    "href": "posts/gds-2022-10-03-polygons-part-1/index.html#polygons-intersecting-water-and-land-nearby",
    "title": "Using polygons",
    "section": "Polygons intersecting water and land… nearby!",
    "text": "Polygons intersecting water and land… nearby!\nThere are situations when the manually delineated region may not be as accurate as it is required. It may be required to get some information of the areas “nearby” as well. “Nearby” can have different definitions, and here we get three of them: 1. Convex hull 2. Bounding Box 3. Buffer\nThe convex hull of the polygon \\(\\Omega\\) is the smallest convex polygon containing \\(\\Omega\\). The bounding box is the smallest rectangle containing \\(\\Omega\\) with edges parallel to the axis of the reference system. The buffer consists of enlarging the boundaries of \\(\\Omega\\) in a direction perpendicular to its sides. The analogous operation for raster objects is called dilation and it is one of the two most common morphological operations along with erosion.\nBut first we transform the operations we wrote so far into a function, taking dict_roi as input and returning the equivalent of gdf_genova for the selected area, which we can try for a different geometry. Then we test that the functions work for a different region. We chose the Viverone Lake, you are invited to select another one.\n\ndef water_and_land(roi: dict) -&gt; gpd.GeoDataFrame:\n    \"\"\"from a polygon embedded in a geojson dict to a geodataframe with water, land and respective areas\"\"\"\n    sh_roi = shape(roi)\n    gdf_roi =  gpd.GeoDataFrame({\"name\":[\"ROI\"], \"geometry\": [sh_roi]}).set_crs('4326')\n    gdf_rivers = osmnx.geometries_from_polygon(sh_roi, tags={'natural': 'water'}).dissolve()\n    gdf_sea = osmnx.geometries_from_polygon(sh_roi, tags={'natural': 'bay'}).dissolve()\n    gse_sea_union_river = gdf_rivers.union(gdf_sea) if len(gdf_sea) else gdf_rivers\n    gse_water_roi = gse_sea_union_river.intersection(gdf_roi)\n    gse_land_roi = gdf_roi.difference(gse_water_roi)\n    \n    gdf = gpd.GeoDataFrame(\n        {\n            \"name\": [\"roi\", \"water\", \"land\"], \n            \"geometry\":[deepcopy(gdf_roi.geometry.iloc[0]), deepcopy(gse_water_roi.iloc[0]), deepcopy(gse_land_roi.iloc[0])]\n        }\n    ).set_crs('4326')\n    \n    gdf[\"areas (Km2)\"] = gdf.to_crs({'proj':'cea'}).area/ 10**6\n    \n    return gdf\n\n\ndef split_gdf_to_layers(gdf: gpd.GeoDataFrame, column_name: str = \"name\") -&gt; dict:\n    kepler_data = {}\n    for _, row in gdf.iterrows():\n        kepler_data.update({row[column_name]: gdf[gdf[column_name] == row[column_name]].copy()})\n        \n    return kepler_data\n\n\ntry:\n    from configs import config_map3\nexcept ImportError:\n    config_map3 = dict()\n\nif True:\n    dict_viverone = {\"type\":\"Polygon\",\"coordinates\":[[[8.098304589093475,45.44917237554611],[8.061826507846979,45.42337850571526],[8.086238762219843,45.3926467614535],[8.015246804101336,45.37550064157892],[7.985783738478874,45.42121207166915],[8.02282302097566,45.44838495097526],[8.098304589093475,45.44917237554611]]]}\n\n\nif KEPLER_OUTPUT:\n    # ~ 5 minutes first run - data are downloaded and cached\n    map_3 = KeplerGl(\n        data=deepcopy(split_gdf_to_layers(water_and_land(dict_viverone))), \n        height=800,\n        config =config_map3,\n    )\n    display(map_3)\nelse:\n    display(Image(\"images/map3.png\"))\n\n\n\n\n\n\n\n\nNow we can consider the three “nearby” operations. For simplicity all the three operations we are interested in are embedded in a class.\n\n\nclass GeoOperations:\n\n    @staticmethod\n    def _data_to_gdf(roi: dict) -&gt; gpd.GeoDataFrame:\n        \"\"\" input value `data` is a dumped geojson to a string format \"\"\"\n        sh_roi = shape(roi)\n        return gpd.GeoDataFrame({\"name\":[\"ROI\"], \"geometry\": [sh_roi]}).set_crs(4326)\n    \n    @staticmethod\n    def _add_area_column(gdf: gpd.GeoDataFrame) -&gt; None:\n        gdf[\"areas (Km2)\"] = gdf.to_crs({'proj':'cea'}).area/ 10**6\n        \n    @staticmethod\n    def _get_polygon_box(row):\n        c_hull = row.convex_hull  # make sure the input is a polygon \n        x, y, X, Y = c_hull.bounds\n        return shapely.geometry.Polygon([(x, y), (X, y), (X, Y), (x, Y), (x, y)])\n    \n    def buffer(self, data: str, buffer_meters:float = 10) -&gt; gpd.GeoDataFrame:\n        gdf = self._data_to_gdf(data)\n        gdf.geometry = gdf.to_crs(crs=3857)['geometry'].buffer(buffer_meters).to_crs(4326)\n        self._add_area_column(gdf)\n        return gdf\n\n    def convex_hull(self, data: str) -&gt; gpd.GeoDataFrame:\n        gdf = self._data_to_gdf(data)\n        gdf.geometry = gdf['geometry'].convex_hull\n        self._add_area_column(gdf)\n        return gdf\n        \n    def bbox(self, data: str) -&gt; gpd.GeoDataFrame:\n        gdf = self._data_to_gdf(data)\n        gdf['geometry'] = gdf['geometry'].apply(lambda x: self._get_polygon_box(x))\n        self._add_area_column(gdf)\n        return gdf\n\n\nkepler_data = split_gdf_to_layers(water_and_land(dict_roi))\n\nge_ops = GeoOperations()\nkepler_data.update({\"buffer\": ge_ops.buffer(dict_roi, 5_000), \"convex hull\": ge_ops.convex_hull(dict_roi), \"bbox\": ge_ops.bbox(dict_roi)})\n\n\ntry:\n    from configs import config_map4\nexcept ImportError:\n    config_map4 = dict()\n\n\nif KEPLER_OUTPUT:\n    map_4 = KeplerGl(\n        data=deepcopy(kepler_data),\n        height=800,\n        config=deepcopy(config_map4),\n    )\n    map_4\nelse:\n    display(Image(\"images/map4.png\"))\n\n\n\n\n\n\n\n\nIn the picture above the bounding box, buffer and convex hull were added to the selected geometry, but the land and sea was not computed for these new polygons. This is left for the reader for practice. For example the water and land within the bounding box should look like the figure below:\n\ndict_bbox = ge_ops.bbox(dict_roi)[\"geometry\"].iloc[0].__geo_interface__\n\nif KEPLER_OUTPUT:\n    map_5 = KeplerGl(\n        data=split_gdf_to_layers(water_and_land(dict_bbox)), \n        height=800,\n        config=deepcopy(config_map4), # we can re-use the same config as the previous map\n    )\n    map_5\nelse:\n    display(Image(\"images/map5.png\"))\n\n\n\n\n\n\n\n\n1.3 Which country intersects this polygon?\nImagine now to draw a polygon on the map with the goal of solving the following problem:\n\nGet the list with the names of all the countries intersecting the given polygon.\n\nThere are two options we can see think of to solve this problem with osmnx: 1. Create a GeoDataFrame in memory with all the countries in the world, and then compute the intersection between the dataframe and the given polygon. 2. Feed the given polygon to osmnx directly, and use the right tag to get all the countries intersecting it.\nThe first one is very slow to initialise, though fast each time a new polygon is passed to the intersect method. The second one is relatively slow each time a new polygon is given.\nHere, out of laziness we will not use any of the methods above. The toy dataset provided geopandas contains already all the information that we need, so we will use this one instead of the one provided by osmnx.\nAs usual the first step is to draw a polygon on the first empty map of the notebook. Then copy paste the polygon geojson below, to be saved in the variable dict_roi. We selected a rectangular region around Myanmar, the reader is free to act according to preference.\n\ndict_roi = {\"type\":\"Polygon\",\"coordinates\":[[[95.27405518168942,28.464895473156595],[88.19328456832459,26.69900877882346],[87.15109322263424,21.154320639876016],[91.6876908450501,14.831845554981104],[98.52323996531177,13.52423467533735],[103.97941818686449,15.157545455928199],[104.43920848643478,20.868173454079436],[104.0713762467781,26.45228108415989],[95.27405518168942,28.464895473156595]]]}\n\nsh_roi = shape(dict_roi)\n\ngdf_roi =  gpd.GeoDataFrame({\"name\":[\"ROI\"], \"geometry\": [sh_roi]})\ngdf_roi = gdf_roi.set_crs('4326')\n\ntry:\n    from configs import config_map6\nexcept ImportError:\n    config_map6 = dict()\n\nif KEPLER_OUTPUT:\n    map_6 = KeplerGl(data=deepcopy({\"roi\": gdf_roi}), height=800, config=config_map6)\n    display(map_6)\nelse:\n    display(Image(\"images/map6.png\"))\n\n\n\n\n\n\n\n\n\nimport geopandas as gpd\n\ngdf_countries = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\ngdf_countries[\"areas (Km2)\"] = gdf_countries.to_crs({'proj':'cea'}).area/ 10**6\n\nax = gdf_countries.plot(figsize=(15, 15), column='areas (Km2)', cmap='Greens')\nax.axis('off')  # images/map_naturalearth.png\n\n\n\n\n\n\n\n\n\ndict_roi = {\"type\":\"Polygon\",\"coordinates\":[[[95.27405518168942,28.464895473156595],[88.19328456832459,26.69900877882346],[87.15109322263424,21.154320639876016],[91.6876908450501,14.831845554981104],[98.52323996531177,13.52423467533735],[103.97941818686449,15.157545455928199],[104.43920848643478,20.868173454079436],[104.0713762467781,26.45228108415989],[95.27405518168942,28.464895473156595]]]}\n\nsh_roi = shape(dict_roi)\n\n# note gdf_countries.intersects(sh_roi) returns a boolean series.\n\ngdf_intersection_countries = gdf_countries[gdf_countries.intersects(sh_roi)].reset_index(drop=True)\ngdf_intersection_countries[[\"name\", 'areas (Km2)']].head(10)\n\ndisplay(gdf_intersection_countries.head())\n\n\n\n\n\n\n\n\npop_est\ncontinent\nname\niso_a3\ngdp_md_est\ngeometry\nareas (Km2)\n\n\n\n\n0\n68414135\nAsia\nThailand\nTHA\n1161000.0\nPOLYGON ((105.21878 14.27321, 104.28142 14.416...\n5.101229e+05\n\n\n1\n7126706\nAsia\nLaos\nLAO\n40960.0\nPOLYGON ((107.38273 14.20244, 106.49637 14.570...\n2.290354e+05\n\n\n2\n55123814\nAsia\nMyanmar\nMMR\n311100.0\nPOLYGON ((100.11599 20.41785, 99.54331 20.1866...\n6.795988e+05\n\n\n3\n96160163\nAsia\nVietnam\nVNM\n594900.0\nPOLYGON ((104.33433 10.48654, 105.19991 10.889...\n3.360194e+05\n\n\n4\n1281935911\nAsia\nIndia\nIND\n8721000.0\nPOLYGON ((97.32711 28.26158, 97.40256 27.88254...\n3.142772e+06\n\n\n\n\n\n\n\nLeft to the reader is to add a column with the information of the the percentage of the area covered by the selected roi over each country (e.g. Myanmar will be 100% covered by the ROI. What about China?)."
  },
  {
    "objectID": "posts/gds-2022-10-03-polygons-part-1/index.html#which-country-intersects-this-polygon",
    "href": "posts/gds-2022-10-03-polygons-part-1/index.html#which-country-intersects-this-polygon",
    "title": "Using polygons - part 1",
    "section": "1.3 Which country intersects this polygon?",
    "text": "1.3 Which country intersects this polygon?\nImagine now to draw a polygon on the map with the goal of wanting to know a list with the names of all the countries intersecting it.\nThere are two options we can see think of to solve this problem with osmnx: 1. Create a GeoDataFrame in memory with all the countries in the world, and then compute the intersection between the dataframe and the given polygon. 2. Feed the given polygon to osmnx directly, and use the right tag to get all the countries intersecting it.\nThe first one is very slow to initialise, though fast each time a new polygon is passed to the intersect method. The second one is relatively slow each time a new polygon is given.\nHere, out of laziness we will not use any of the methods above. The toy dataset provided geopandas contains already all the information that we need.\nAs usual the first step is to draw a polygon on the first empty map of the notebook. Then copy paste the polygon geojson below, to be saved in the variable dict_roi. We selected a rectangular region around Myanmar, the reader is free to act according to preference.\n\ndict_roi = {\"type\":\"Polygon\",\"coordinates\":[[[95.27405518168942,28.464895473156595],[88.19328456832459,26.69900877882346],[87.15109322263424,21.154320639876016],[91.6876908450501,14.831845554981104],[98.52323996531177,13.52423467533735],[103.97941818686449,15.157545455928199],[104.43920848643478,20.868173454079436],[104.0713762467781,26.45228108415989],[95.27405518168942,28.464895473156595]]]}\n\nsh_roi = shape(dict_roi)\n\ngdf_roi =  gpd.GeoDataFrame({\"name\":[\"ROI\"], \"geometry\": [sh_roi]})\ngdf_roi = gdf_roi.set_crs('4326')\n\ntry:\n    from configs import config_map6\nexcept ImportError:\n    config_map6 = dict()\n\nmap_6 = KeplerGl(data=deepcopy({\"roi\": gdf_roi}), height=800, config=config_map6)\ndisplay(map_6)\n\n\nimport geopandas as gpd\n\ngdf_countries = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n\ngdf_countries[\"areas (Km2)\"] = gdf_countries.to_crs({'proj':'cea'}).area/ 10**6\n\nax = gdf_countries.plot(figsize=(15, 15), column='areas (Km2)', cmap='Greens')\nax.axis('off')  # map_naturalearth\n\n\ndict_roi = {\"type\":\"Polygon\",\"coordinates\":[[[95.27405518168942,28.464895473156595],[88.19328456832459,26.69900877882346],[87.15109322263424,21.154320639876016],[91.6876908450501,14.831845554981104],[98.52323996531177,13.52423467533735],[103.97941818686449,15.157545455928199],[104.43920848643478,20.868173454079436],[104.0713762467781,26.45228108415989],[95.27405518168942,28.464895473156595]]]}\n\nsh_roi = shape(dict_roi)\n\n# gdf_countries.intersects(sh_roi) returns a boolean series.\n\ngdf_intersection_countries = gdf_countries[gdf_countries.intersects(sh_roi)].reset_index(drop=True)\n\ngdf_intersection_countries[[\"name\", 'areas (Km2)']].head(10)  # Table 2\n\nLeft to the reader is to add a column with the information of the the percentage of the area covered by the selected roi over each country (e.g. Myanmar will be 100% covered by the ROI. What about China?)."
  },
  {
    "objectID": "posts/gds-2022-10-03-polygons-part-1/index.html#distances-between-countries",
    "href": "posts/gds-2022-10-03-polygons-part-1/index.html#distances-between-countries",
    "title": "Using polygons",
    "section": "Distances between countries",
    "text": "Distances between countries\nNow let’s say you have two polygons representing two countries. How to compute their distance in Km on the surface of the earth?\n\nlist_countries = sorted(list(gdf_countries.name))\nlist_countries[:10]\n\n['Afghanistan',\n 'Albania',\n 'Algeria',\n 'Angola',\n 'Antarctica',\n 'Argentina',\n 'Armenia',\n 'Australia',\n 'Austria',\n 'Azerbaijan']\n\n\n\ngdf_countries.to_crs(epsg=27700,inplace=True) # more about 27700 on the next article! The impatient reader can look under https://epsg.io/27700\n\nsh_france = gdf_countries[gdf_countries.name == \"France\"].geometry.iloc[0]\nsh_italy = gdf_countries[gdf_countries.name == 'Italy'].geometry.iloc[0]\nsh_uk = gdf_countries[gdf_countries.name == 'United Kingdom'].geometry.iloc[0]\n\nprint(f\"Distance between France and UK polygons: {round(sh_france.distance(sh_uk) / 1_000, 3)} Km\")\n\nDistance between France and UK polygons: 37.14 Km\n\n\nA quick investigation using google seems to not confirm this number. Google says the distance is approx 32Km. Did we do something wrong? Let’s use google maps and pick the two point that are visually the closest to each others on the coasts, copy paste them to the notebook and see what is their distance:\n\na = (51.108648, 1.286524)\nb = (50.869575, 1.583608)\n\nprint(f\"Distance between France and UK by naked eye: {round(distance.distance(a, b).km, 3)} Km\")\n\nDistance between France and UK by naked eye: 33.801 Km\n\n\nThis number looks way closer to the one found with the shapely distance method. So Why the discrepancy? Is it a problem with the coordinates? Maybe the 27700 is not the optimal one in this case?\nLooking at the data we can find that the accuracy of the countries segmentation is way less that the actual underlying coast. The accuracy is so low that the distance between the polygon is actually wider!\n\ntry:\n    from configs import config_map7\nexcept ImportError:\n    config_map7 = dict()\n\nif KEPLER_OUTPUT:\n    map_7 = KeplerGl(data=deepcopy({\"rois\": gdf_countries.to_crs('4326')}), height=800, config=config_map7)\n    display(map_7)\nelse:\n    display(Image(\"images/map7.png\"))\n\n\n\n\n\n\n\n\nNOTE: the artefacts that you can see in the map above are caused by polygons crossing the antimeridian. This topic will be discussed in the next blog post of the polygon series “reference systems for polygons”.\nThis section also confirmed numerically the importance of the quality of the segmentation in taking measurements, which lead us straight to the next (and last!) section of this first tutorial about polygons."
  },
  {
    "objectID": "posts/gds-2022-10-03-polygons-part-1/index.html#same-country-different-borders-how-different-are-they",
    "href": "posts/gds-2022-10-03-polygons-part-1/index.html#same-country-different-borders-how-different-are-they",
    "title": "Using polygons",
    "section": "Same country, different borders: how different are they?",
    "text": "Same country, different borders: how different are they?\nThe problem we want to address in this section is to quantify the differences between segmentations of the same region. As often happens in geospatial data science (and not only there) a ground truth is not available. Also, as in the case of natural phenomena, the true segmentation varies over time, and an approximation is all what we can get. Quantifying the differences between two shapes consists of reducing the dimensionality of the problem, from 2D to 1D. The methods here presented can be generalised from ND to 1D, which can happen in case altitude and time are taken into account in the segmentation process.\nWe will take three segmentations of a the Laguna Honda reservoir in San Francisco, CA: a most accurate, a less accurate, and an absurdly inaccurate one, then we measure their distances with a range of four metrics.\n\ndict_lh_accurate = {\"type\":\"Polygon\",\"coordinates\":[[[-122.46336870734235,37.75382312075932],[-122.46361657885608,37.75373534452417],[-122.4635831171001,37.75365833165909],[-122.46316396885612,37.75301643279386],[-122.46300520664897,37.752840487963525],[-122.46230161881246,37.7522463019286],[-122.46211503445602,37.75203693263166],[-122.46178081988387,37.75186782984793],[-122.46148855443528,37.75184019740165],[-122.46133288449474,37.75201341062547],[-122.4612061599761,37.752084737992796],[-122.46103218224685,37.75214417741294],[-122.46077014171657,37.752179841041794],[-122.46037947391036,37.752188392148845],[-122.46021718629451,37.75217343769624],[-122.4600201930734,37.75211950941181],[-122.45991172291727,37.75207281219638],[-122.45983991818758,37.752077610017345],[-122.45994274713122,37.75230912513596],[-122.45996851594319,37.75240437677724],[-122.46006708164848,37.7524522572354],[-122.46084756619754,37.752511150412346],[-122.46122218790283,37.75259302382214],[-122.46131143485962,37.75262966326064],[-122.46167022567394,37.7526676568636],[-122.46188775902073,37.75272257149743],[-122.46205025380969,37.752816858793715],[-122.462376261119,37.75308710607375],[-122.46251137863547,37.753166819591506],[-122.46262051201403,37.75320708721178],[-122.46268905680037,37.75326534110075],[-122.46270650192626,37.75330849493771],[-122.46273109980764,37.753422092415555],[-122.46277517530947,37.75345753879545],[-122.4628296309809,37.7534771096768],[-122.46294915057157,37.75346704465253],[-122.4630682867433,37.753516920096686],[-122.46336870734235,37.75382312075932]]]}\n\ndict_lh_less_accurate = {\"type\":\"Polygon\",\"coordinates\":[[[-122.46336953589098,37.753832862738975],[-122.4636233738959,37.75373251334907],[-122.4631072366194,37.75293640336153],[-122.46177458709447,37.75186264481159],[-122.46148267338904,37.75183922934248],[-122.4612246047509,37.752080073813296],[-122.46047155200345,37.75218711554916],[-122.4599004164929,37.752066693585995],[-122.45984118762517,37.752080073813296],[-122.45996387599406,37.752411233681165],[-122.46131767868594,37.752618625974755],[-122.46194804306447,37.75272232190355],[-122.46268840391157,37.75324748937671],[-122.46272647961233,37.753431464361086],[-122.46314954295359,37.75359536896148],[-122.46336953589098,37.753832862738975]]]}\n\n\ndict_lh_inaccurate = {\"type\":\"Polygon\",\"coordinates\":[[[-122.46176576455443,37.75368924488866],[-122.46242948841775,37.75334575268072],[-122.4631052799878,37.752296183276115],[-122.46372073302484,37.75144697531703],[-122.46259843631039,37.75127522420765],[-122.46204332180642,37.75167597617579],[-122.46192264474033,37.752410682480374],[-122.46095722821178,37.752954551279636],[-122.4605348584804,37.75364153773237],[-122.46176576455443,37.75368924488866]]]}\n\n\nsh_lh_accurate = shape(dict_lh_accurate)\nsh_lh_less_accurate = shape(dict_lh_less_accurate)\nsh_lh_inaccurate = shape(dict_lh_inaccurate)\n\ngdf_lh_accurate = gpd.GeoDataFrame({\"name\":[\"Accurate\"], \"geometry\": [sh_lh_accurate]}).set_crs('4326')\ngdf_lh_less_accurate = gpd.GeoDataFrame({\"name\":[\"Less accurate\"], \"geometry\": [sh_lh_less_accurate]}).set_crs('4326')\ngdf_lh_inaccurate = gpd.GeoDataFrame({\"name\":[\"Inaccurate\"], \"geometry\": [sh_lh_inaccurate]}).set_crs('4326')\n\n\ntry:\n    from configs import config_map8\nexcept ImportError:\n    config_map8 = dict()\n\nif KEPLER_OUTPUT:\n    map_8 = KeplerGl(\n        data={\n            \"Laguna Honda accurate\": deepcopy(gdf_lh_accurate),\n            \"Laguna Honda less accurate\": deepcopy(gdf_lh_less_accurate),\n            \"Laguna Honda inaccurate\": deepcopy(gdf_lh_inaccurate),\n        }, \n        height=800,\n        config=config_map8\n    )\n    display(map_8)\nelse:\n    display(Image(\"images/map8.png\"))\n\n\n\n\n\n\n\n\n\nDice’s Score\nAfter Lee R. Dice, the Dice’s score measures the area of the overlap between two polygons normalised to the sum of the individual areas of the two polygons. Therefore if the polygon are perfectly congruent, then the dice equals 1, if there is no intersection between the two elements, the measure is zero.\nFormally: let \\(P_1\\) and \\(P_2\\) be two simple (i.e. where their area can be computed uniquely) polygons drawn on a map, \\(\\mathcal{A}(P_1)\\) and \\(\\mathcal{A}(P_2)\\) their area, and \\(P_1 \\cap P_2\\) their intersection. The Dice’s score of \\(P_1\\) and \\(P_2\\) is defined as: \\[\n\\text{Dice}(P_1, P_2) = \\frac{2 \\mathcal{A}(P_1 \\cap P_2) }{ \\mathcal{A}(P_1) + \\mathcal{A}(P_2)}~.\n\\]\nThis definition does not correspond to the intuitive definition of a distance that is zero when the measured object as as close to each others as possible.\n\n\nDice’s Distance\nThe Dice’s score can be turned into a distance simply modifying its formula as: \\[\n\\text{DiceDist}(P_1, P_2) = 1 - \\frac{2 \\mathcal{A}(P_1 \\cap P_2) }{ \\mathcal{A}(P_1) + \\mathcal{A}(P_2)}~,\n\\] so that the distance between two identical polygons is zero.\n\n\nCovariance Distance\nWe can consider the polygon’s vertex as a cloud of points, and compare the principal components of their distribution encoded as a Symmetric Positive Definite matrix through the covariance matrix.\nThe covariance distance is given by the sum of the 2 eigenvalues of the product of the covariance matrices, normalised by the sum of the products of squared eigenvalues of each covariance matrix. As done for the dice score, to have zero when the two shapes are identica, we take \\(1\\) minus the value obtained.\nFormally: with the same notation as above the covariance distance is defined as \\[\n\\text{CovDist}(P_1, P_2) = \\alpha \\left( 1 - \\frac{ \\text{Tr}( \\text{c}(P_1) \\text{c}(P_2) )  }{ \\text{Fro}(P_1) + \\text{Fro}(P_2)} \\right) ~,\n\\] where \\(\\alpha\\) is a multiplicative factor to scale the data (it can be one if the points are normalised with standard deviation = 1, or computed as the reciprocal of the average standard deviations of the clouds distributions), and \\(\\text{Tr}\\) and \\(\\text{Fro}\\) are the trace and Frobenius norm respectively.\n\n\nHausdoroff Distance\nThe main feature of the covariance distance is that only the principal components of the two geometries are considered, and the noise is not taken into account. The Hausdoroff distance instead aims at being very sensitive to the misplacement of a single vertex between the two geometries.\nIt’s definition is based on as maximal distance between the vertex of one polygon the distance between the \\[\n\\text{HausDist}(P_1, P_2) = \\text{max} \\left\\{ \\text{max}_{p_i \\in \\partial P_1} d(p_i, \\partial P_2) , \\text{max}_{p_j \\in \\partial P_2} d(p_j, \\partial P_1) \\right\\} ~,\n\\] where \\(\\partial P_1\\) is the contour, or perimeter of the polygon \\(P_i\\) and \\(d\\) is the Euclidean distance (or in the case of the curved surface, the geodesic distance).\n\n\nNormalised Symmetric Contour distance\nThe contour distance between \\(P_1\\) and \\(P_2\\) is given by the the sum of the distances of all the vertex of \\(P_1\\) to the contour of \\(P_2\\).\nAs contour distance so defined is not symmetric (the contour distance between \\(P_1\\) and \\(P_2\\) is not the same of the contour distance between \\(P_2\\) and \\(P_1\\)), we can make it symmetric considering the sum of the contour distance between \\(P_1\\) and \\(P_2\\) with the contour distance between \\(P_2\\) and \\(P_1\\).\nThe distance can be normalised considering a distance factor encompassing both geometries in respect to their distances, which can be the lengths of both perimeters.\nFormally: \\[\n\\text{NSCD}(P_1, P_2) = \\frac{  S(P_1, P_2) + S(P_1, P_2) }{ \\vert \\partial P_1 \\vert + \\vert \\partial P_2 \\vert } ~,\n\\] where the length of the contour is indicated with \\(\\vert \\partial P_i \\vert\\), and \\(S(P_1, P_2)\\) is the contour distance, computed as: \\[\nS(P_2, P_1) = \\sum_{p_i \\in \\partial P_1} d(p_i, \\partial P_2)~.\n\\]\nBelow we implement the four distances in a class and then we use this tool to measure the distances between the polygon we created above: sh_lh_accurate, sh_lh_less_accurate and sh_lh_inaccurate.\nAs usual you are invited to try it out on your own, on different geometries, to check the behaviours of the distances.\n\nclass Dist:\n    def __init__(self, p1: Polygon, p2: Polygon):\n        self.p1 = p1\n        self.p2 = p2\n        self.prec = 6\n        \n    def dice(self) -&gt; float:\n        assert self.p1.area + self.p2.area &gt; 0, \"Polygons must have positive areas.\"\n        return np.round(1 - 2 * self.p1.intersection(self.p2).area / (self.p1.area + self.p2.area), self.prec)\n    \n    def cov(self) -&gt; float:\n        x1, y1 = self.p1.exterior.coords.xy[0], self.p1.exterior.coords.xy[1]\n        x2, y2 = self.p2.exterior.coords.xy[0], self.p2.exterior.coords.xy[1]\n        cov1 = np.cov(np.array([(x1 - np.mean(x1)) /  np.std(x1), (y1 - np.mean(y1)) / np.std(y1) ]))\n        cov2 = np.cov(np.array([(x2 - np.mean(x2)) /  np.std(x2), (y2 - np.mean(y2)) / np.std(y2) ]))\n        return np.round((1 - (np.trace(cov1.dot(cov2)) / (np.linalg.norm(cov1, ord='fro') * np.linalg.norm(cov2, ord='fro')))), self.prec)\n    \n    def hau(self) -&gt; float:\n        \"\"\" measured in degrees !!\"\"\"\n        return np.round(self.p1.hausdorff_distance(self.p2), self.prec)\n    \n    def nsc(self) -&gt; float:\n        \"\"\" measured in degrees !!\"\"\"\n        def bd(p1, p2):\n            return np.round(sum([p1.boundary.distance(Point(x, y)) for x,y in zip(p2.exterior.coords.xy[0], p2.exterior.coords.xy[1])]), self.prec)\n        \n        return (bd(self.p1, self.p2) + bd(self.p2, self.p1)) / (self.p1.length + self.p2.length)\n    \n    def d(self, selected_measure: str) -&gt; float:\n        map_measures = {\n            \"dice\": self.dice,\n            \"cov\": self.cov,\n            \"haus\": self.hau,\n            \"nsc\": self.nsc,\n        }\n        return map_measures[selected_measure]()\n        \n        \n\n\n# simplify the name according to the polygons colour in the image above\ngreen = sh_lh_accurate\nyellow = sh_lh_less_accurate\nred = sh_lh_inaccurate\n\nmeas = [ \"dice\", \"cov\", \"haus\", \"nsc\"]\n\ndf_dist = pd.DataFrame(\n    {\n        \"green - green\": [Dist(green, green).d(m) for m in meas],\n        \"green - yellow\": [Dist(green, yellow).d(m) for m in meas],\n        \"green - red\": [Dist(green, red).d(m) for m in meas],\n        \"yellow - red\": [Dist(yellow, red).d(m) for m in meas],    \n    },\n    index=meas\n)  \n\ndisplay(df_dist)\n\n\n\n\n\n\n\n\ngreen - green\ngreen - yellow\ngreen - red\nyellow - red\n\n\n\n\ndice\n-0.0\n0.039633\n0.760388\n0.783997\n\n\ncov\n0.0\n0.000104\n0.692591\n0.706379\n\n\nhaus\n0.0\n0.000078\n0.001526\n0.001545\n\n\nnsc\n0.0\n0.044710\n1.657110\n1.038613\n\n\n\n\n\n\n\n\nNote 1:\nThe considered Hausdorff and nsc distances are in degrees and not in meters. As we want to have a measure of the differences for geometries (specifically geometries within comparable latitude intervals) this is not invalidating the results. Though there are cases when these metrics should return the results in meters. This is left as an exercise for the reader (you can leave a comment below for a discussion about possible solutions!).\n\n\nNote 2:\nThe same methods can be used for comparing the differences in shape of two objects that are expected to be similar or two measure the variability of two shapes that are changing over time, assuming that their segmentation is accurate and unbiased.\nAlso the same method can be used to measure the accuracy of a segmentation of two different people segmenting the same object (inter-rater variability) or the same person segmenting twice the same object (intra-rater variability). If the variability is assessed without the rater knowing he is segmenting twice the same shape (out of a stack of different shapes to undergo segmentation, one is repeated twice, re-labelled and re-shuffled in the stack), then this experiment is called test re-test."
  },
  {
    "objectID": "posts/gds-2022-10-03-polygons-part-1/index.html#appendix-list-of-topics-and-further-theoretical-ideas",
    "href": "posts/gds-2022-10-03-polygons-part-1/index.html#appendix-list-of-topics-and-further-theoretical-ideas",
    "title": "Using polygons",
    "section": "Appendix: list of topics and further theoretical ideas",
    "text": "Appendix: list of topics and further theoretical ideas\nIn solving three recurring problems we went through a relatively long list of topics. To maintain an hands on approach, the theory was barely touched. In the list below you can find a summary reporting the list of the topics touched and some hints to dig further into the theory:\n\nPolygons and sets\n\npolygon as a list of vectors\npolygon as a set on a surface\noperations between sets: union, intersection, difference, disjoint union.\nCan you express union and difference as combinations of intersection and disjoint union?\nDe Morgan laws\nMathematical concept of Algebra and sigma algebra. Are the polygons on the surface of a sphere with the operation of intersection and disjoint union a sigma algebra?\nGeojson format to represent a polygon\nGeojson to shapely, and shapely operations\nAdding metadata to a polygon with geopandas\nOsmnx library\nVisualisation with KeplerGl and WGS84\n\nMorphological operations in GIS\n\nDilation (or buffer) and erosion\nBounding box\nConvex hull\nClass as a toolbox\n\nDistance between polygons and projections\n\n“Optimal Algorithms for Computing the Minimum Distance Between Two Finite Planar Sets” Toussaint, Bhattacharya\nMinkovsky sum\nHaversine distance\nVincentry distance\n\nShape differences\n\nSegmentation\nDifference quantification as a dimensionality reduction problem\nIntra-rater and inter-rater variability\nDice’s score\nCovariance of a polygon (of a set of 2d points) and covariance distance\nHausdoroff distance\nNormalized symmetric contour distance"
  },
  {
    "objectID": "posts/gds-2022-10-03-polygons-part-1/index.html#resources",
    "href": "posts/gds-2022-10-03-polygons-part-1/index.html#resources",
    "title": "Using polygons",
    "section": "Resources",
    "text": "Resources\n\nGeopandas and shapely basics https://www.learndatasci.com/tutorials/geospatial-data-python-geopandas-shapely/\nShapely manual https://shapely.readthedocs.io/en/stable/manual.html\nGeopandas crs usage https://geopandas.org/en/stable/docs/user_guide/projections.html\nintersection polygon algorithm https://www.swtestacademy.com/intersection-convex-polygons-algorithm/\nBuffer (dilation) to remove holes https://gis.stackexchange.com/questions/409340/removing-small-holes-from-the-polygon/409398\nmaps projections https://automating-gis-processes.github.io/2017/lessons/L2/projections.html\nTrace and covariance https://online.stat.psu.edu/stat505/lesson/1/1.5"
  },
  {
    "objectID": "posts/gds-2022-10-03-polygons-part-1/index.html#inspired-by",
    "href": "posts/gds-2022-10-03-polygons-part-1/index.html#inspired-by",
    "title": "Using polygons",
    "section": "Inspired by",
    "text": "Inspired by\n\nQiusheng Wu\nMaxime Labonne"
  },
  {
    "objectID": "about/about.html",
    "href": "about/about.html",
    "title": "Sebastiano Ferraris",
    "section": "",
    "text": "Welcome to my blog,\nI’m Sebastiano Ferraris, data scientist at General System, and this is my blog about Geospatial Data Science.\nHere I am sharing some tutorials and small projects about geospatial data science with particular focus on algorithms, mathematics and coding practices.\nAll my posts are aimed at be as practical and hands-on as possible, so as to give you the possibility of reproducing all that is shown.\nThe blog you are reading started on Medium, then moved to hashnode, to finally land to Quarto. The source code can be found here.\nIf you want to get in touch, please drop me an email.\n\nUnless otherwise specified, all the images are created by the author. In this page you can see The Apparition, painted by Chagall in 1925."
  },
  {
    "objectID": "z_draft/bl-2022-07-30-to-hashnode/index.html",
    "href": "z_draft/bl-2022-07-30-to-hashnode/index.html",
    "title": "Medium to Hashnode",
    "section": "",
    "text": "This post documents the transition of my newly born blog dedicated to geospatal data science from Medium to Hashnode. Here you will find the pros and cons of both platforms I have found so far and how my workflow have changed after this choice. In between there is the description of two issues with hashnode when articles are sources from github, and at the end you will find a list of possible alternatives for creating a blog."
  },
  {
    "objectID": "z_draft/bl-2022-07-30-to-hashnode/index.html#content",
    "href": "z_draft/bl-2022-07-30-to-hashnode/index.html#content",
    "title": "Medium to Hashnode",
    "section": "Content",
    "text": "Content\n\nPros and Cons\n\nMedium\nHashnode\n\nCurrent issues with Hashnode\nWorkflow\n\nMedium\nHashnode\n\nOther alternatives\nConclusions\nLinks\nCredits"
  },
  {
    "objectID": "z_draft/bl-2022-07-30-to-hashnode/index.html#pros-and-cons",
    "href": "z_draft/bl-2022-07-30-to-hashnode/index.html#pros-and-cons",
    "title": "Medium to Hashnode",
    "section": " 1. Pros and cons",
    "text": "1. Pros and cons\nThe two reasons why hashnode is so attractive for tech writing are the possibility to source the articles directly from github with the github integration (like the article that you are reading right now!), and the possibility of collaborating with other users. But these are only the main reasons. Let’s see a list of 4 pros and cons for each platform, starting with Medium.\n\n Medium\n\nPros:\n\nPopularity: Medium had been around since 2012, it has 200000 writers adn 54 million users and thanks to the quality of its content, whatever you are looking for on google, a medium article is likely to appear in the top 20 results.\nSimplicity: it is very simple to use, and does not require coding experience. For plain text Medium has the shortest possible pathway to connect your content with potential readers.\nMinimalism: the UI for the Medium’s readers is easily recognisable, iconic, clutter free, and simple to interact with.\nWide community: Being widely used and popular, you are very likely to find an expert to ask questions to about something you want to know more about on Medium than anywhere else. As a writer, you can be sure your articles will receive immediate attention from other users with similar interests.\n\nCons:\nI already wrote about the limitations of Medium on a post with the emphatic title The 7 reasons why I am not writing on Medium. Here you can find a condensed version, in only 4 points.\n\nWYSIWYG: the intention of being usable by the widest possible audience has some limitations. The text editor follows the WYSIWYG (what you see is what you get) paradigm: what you write is exactly what you will see in the page, as it happens in other tools, such as in Microsoft Word, and FrontPage. This feature reduces the flexibility in formatting and customisation, and a technical writer typically used to compiled markup languages and text editors with shortcuts, multiple cursors, custom colorschemes, integrated terminals, and other goodies, will have the sensation of writing with handcuffs.\nNot flexible or programmable: The interface of Medium has minimal editability. You can import third party contents, such as images, import code from github gists, and cusomise the graphic. Here as well, all is done via UI and not via programming language, which reduces the flexibility developers are used to have.\nNo version control: also providing the shortest path between writers and readers has a costs. As it would probably complicate the interface beyond the scopes of Medium and it would require some knowledge with version control systems, Medium lacks the possibility of having multiple branches for the same draft and the idea of checkpoint the work to be able to go back in the past at a tagged point.\nNo collaboration: probably for a similar reason why there is no version control, it is also not possible to collaborate with another author while writing an article. Git and its hosts such as github and gitlab have solved the problem of writing collaboration (as well as version control), though the learning curve may discourage the writer who is only looking for a simple platform.\n\n\n\n\n Hashnode Pros\n\nPros:\n\nProgrammable: articles on hashnode are written in markdown. It is not a WYSIWYG language, but it is simple enough to allow the author to focus on the content, without loosing the benefits of a markup language. Moreover hashnode can be customised directly in CSS, can have integration with github.\nCollaborative: collaboration on hashnode can happen either on github integration or adding directly multiple team members to a single blog. The paradigm of the solitary developer in a dark room it’s a thing of the past, and more and more devs are investing a percentage of their time in looking at other people work and sharing theirs, for learning, contaminations, and for finding direct collaborators. Imagine a novel writer reads only his own work, and does not even publish it!\nTech community and support: probably hashnode is not explicitly targeting only tech experts, though these are the writers and readers who will find themselves more at home here than in many other blog platforms I know of. Not only tech articles are encouraged and you will easily find personal blogs of developer talking about various topics such as Kotlin, XML and how to talk confidently in public.\nGithub integration: that’s the main reason why hashnode is so attractive to me (and possibly to many others who have joined). The idea of writing a post, version control and collaborate on it on github, and having it automatically up to date on the community facing blog. There are currently a couple of issues that I have delineated below, though they will hopefully be soon solved to make the integration seamless.\n\nCons:\n\nNot as popular: being more user friendly (for the writer and reader not in for technical content) medium certainly wins in popularity, and therefore in visibility. I expect hashnode will never be as popular, or will have as many users as medium, though it is certainly destined to improve, and to become the go-to choice for developer looking for a quick way of posting content and for a community to interact with.\nNot as minimalistic: compared to the now iconic Medium UI, the dashboard of hashnode looks a crammed and still has to find its personality. Zooming out and going dark mode helps, though there is sometimes too much information in one go for my taste.\nToo many emoji: emoji are like swearing 🤬, they are cool 🆒 and funny 🤣 only if not abused 🔫 or misplaced 🚽. An excessive 📈 amount would infantilise 👶 the reader 👓, visually saturate the page 📃, and divert the attention ㊟ from the content 🔃 (yes, you got me 🥇, I am showing you an example here 🤣🤣🤣🤣🤣, how funny 🤪). Also there is something odd in having 10 different ways of showing appreciation to an article (unlike the single black-and-white claps of Medium). A single option encourages the reader to give kudos, 10 options to chose from so far steered me away from upvoting."
  },
  {
    "objectID": "z_draft/bl-2022-07-30-to-hashnode/index.html#current-issues-with-hashnode",
    "href": "z_draft/bl-2022-07-30-to-hashnode/index.html#current-issues-with-hashnode",
    "title": "Medium to Hashnode",
    "section": " 2. Current issues with Hashnode",
    "text": "2. Current issues with Hashnode\nThis post is also the opportunity to document a couple of issues found (29 July 2022) sourcing articles from github.\n\nAll the underscores appearing in the markdown in the source code are escaped with a backslash. For example the following url that contains underscores appears on github as: txt     https://github.com/SebastianoF/GeoBlog/blob/master/office_positioning/office_positioning.md though when it is formatted as a link, the underscores are escaped, and the link results to be broken.\n\nhttps://github.com/SebastianoF/GeoBlog/blob/master/office_positioning/office_positioning.md\n\nThis happens also when linking images. I raised this issue on stackoverflow as well and communicated it to the hashnode development team. Hopefully it will be only a matter of time before the bug will be fixed.\nLaTeX code is also not formatted, remaining raw across the article. For example, I’m adding below the formula for the bearing:\n\\[\n\\mathcal{B} = \\arctan\\left(\n     \\frac{\n         \\sin(\\Delta \\text{lon}) \\cos(\\text{lat2})\n     }{\n         \\cos(\\text{lat1}) \\sin(\\text{lat2}) - \\sin(\\text{lat1}) \\cos(\\text{lat2}) \\cos\\left( \\Delta \\text{lon} \\right)\n     }\n\\right)\n\\]\nAnd here the integral of the Gaussian:\n\\[\n\\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^{\\infty} e^{-\\frac{1}{2}\\xi^2} \\, d\\xi = 1\n\\]\n\nTo see how the correct formatting should look like (admitting that the bug had not been fixed in the meantime), the source of this article can be found here (NOTE: you will have to remove the slash before each underscores manually from the url after clicking on the link)."
  },
  {
    "objectID": "z_draft/bl-2022-07-30-to-hashnode/index.html#workflows",
    "href": "z_draft/bl-2022-07-30-to-hashnode/index.html#workflows",
    "title": "Medium to Hashnode",
    "section": " 3. Workflows",
    "text": "3. Workflows\nThe platform you are using changes your workflow, and so has an influence on the content, exactly as the mean used to write influences the writing quality. If the workflow is too convoluted there will be detrimental effects on the results, and it would diminish the enthusiasm in starting a new article.\n\n 3. Medium\nSince my technical writing involves producing content with a text editor (or a jupyter notebook) versioned controlled and stored on github, to publish on Medium this is what I would be doing:\n\nCreate, revise and improve content on a jupyter notebook in a versioned controlled repository on github, with images stored in a folder in the same repo, and latex formulae saved in markdown in the same place.\nManually copy paste each cell over to Medium, and re-format in place, transforming link, bold, italic, itemisation etc…\nManually copy each code snippet on github (with some naming that would allow to insert snippets in between snippets already created, and to find the snippets after a few months of not having worked on a post).\nManually create the images from latex formulae, and save them in an external folder somewhere (as it makes no sense to save them on the repository).\nManually upload the images, via drag and drop on the article, then copy paste the caption.\nRe-read, find issues and correct in both places.\n\n\n\n 3. Hashnode\nWith hashnode I found two different workflows, depending on if I want to source the code from github or not.\n\nCreate, revise and improve content on a jupyter notebook in a versioned controlled repository on github, with images stored in a folder in the same repo, and latex formulae saved in markdown in the same place.\nAutomatically convert the Jupyter notebook to markdown with nbconvert.\n\nWith github integration:\n\nAdd the hashnode header to the markdown file, then commit, and merge to master, as you would do anyway with github.\nRe-read, find issues on hashnode and correct directly on github (look at the log if something went wrong in the uploading).\n\nWithout github integration:\n\nCreate a new article on hashnode and copy paste the created markdown.\nRe-read, find issues on hashnode, correct directly on github (look at the log if something went wrong in the uploading), and re-do copy paste of the full file from the repository to hashnode."
  },
  {
    "objectID": "z_draft/bl-2022-07-30-to-hashnode/index.html#alternatives",
    "href": "z_draft/bl-2022-07-30-to-hashnode/index.html#alternatives",
    "title": "Medium to Hashnode",
    "section": " 4. Alternatives",
    "text": "4. Alternatives\nHere we are in the not tried yet zone, so the following list of possible alternatives is based on google searches only (please leave a comment if you have hints and recommendations):\n\nCreate it from scratch with HTML, CSS and JavaScript. if not already knowledgeable about it, it may be worth digging into HTML, CSS and JavaScript at least once in life. A blog can be the right opportunity for it. If you want to pursue this path, the blog by Alex Nim telling you about his journey and secrets may be a good starting point.\nWordpress is certainly a valid option. It has a very simple interface, has a wide range of customisation, and most importantly, it supports LaTeX. There are several technical blogs out there worth mentioning written in wordpress, one for all the blog by Terence Tao\nGhost is another valid alternative, allowing to inject content directly in HTML or render LaTeX equations\nFastpages a “wrapper” of Jekyll hosted on github pages and developed by volunteers, it allows the developers to create a blog directly from the jupyter notebooks. A cool example is the ML blog by Maxime Labonne."
  },
  {
    "objectID": "z_draft/bl-2022-07-30-to-hashnode/index.html#conclusions",
    "href": "z_draft/bl-2022-07-30-to-hashnode/index.html#conclusions",
    "title": "Medium to Hashnode",
    "section": " 5. Conclusions",
    "text": "5. Conclusions\nDespite the markdown and LaTeX formatting issues documented above for when sourcing an article from github, hashnode has plenty to offer for the technical writer. Using markdown instead of a WYSIWYG interface greatly reduces the editing time, it gives the possiblity of having collaborators, and set your blog into an ecosystem of bloggers producing content for developers."
  },
  {
    "objectID": "z_draft/bl-2022-07-30-to-hashnode/index.html#links",
    "href": "z_draft/bl-2022-07-30-to-hashnode/index.html#links",
    "title": "Medium to Hashnode",
    "section": " 6. Links",
    "text": "6. Links\n(All the links below are underscore-free, so they should work despite the documented issue, even if this article is sourced from github)\n\nConnect a repository to hashnode\nHow to publish articles on hashnode\nList of tags\nFiles uploader\nTemplate repository\nGive feedback to hashnode"
  },
  {
    "objectID": "z_draft/bl-2022-07-30-to-hashnode/index.html#credits",
    "href": "z_draft/bl-2022-07-30-to-hashnode/index.html#credits",
    "title": "Medium to Hashnode",
    "section": " 7. Credits",
    "text": "7. Credits\n\nQiusheng Wu: his blog also had moved from Medium to Hashnode, and he introduced me to this platform.\nAlex Nim for the tutorial about how to build your own blog from scratch.\nMaxime Labonne for his very inspiring blog, and the insights about how to transition from a blog in fastpages to Medium.\nHashnode support team, in particular to uncle-big-bay, of the hashnode support team, for promptly answering my messages and looking into the issue."
  },
  {
    "objectID": "posts/bp-2022-07-23-gpg/index.html#appendix-0-list-created-keys-and-delete-some",
    "href": "posts/bp-2022-07-23-gpg/index.html#appendix-0-list-created-keys-and-delete-some",
    "title": "How to sign your commits",
    "section": "Appendix 0: List created keys and delete some",
    "text": "Appendix 0: List created keys and delete some\nTo undo the key creation of step 1 you can retrieve the list of existing keys with gpg -k, hen copy the key public id to clipboard, that is a string like this dummy 43525435HJJH5K2H3KJHK3452KJH65NBMBV in the output\npub   ed25519 2022-02-18 [SC] [expires: 2024-02-18]\n      43525435HJJH5K2H3KJHK3452KJH65NBMBV\nuid           [ultimate] Sebastiano Ferraris &lt;seb@email.com&gt;\nsub   cv25519 2022-02-18 [E] [expires: 2024-02-18]\nFinally delete public and private key with:\ngpg --delete-secret-key 43525435HJJH5K2H3KJHK3452KJH65NBMBV\ngpg --delete-key 43525435HJJH5K2H3KJHK3452KJH65NBMBV"
  },
  {
    "objectID": "posts/gds-2022-10-03-polygons-part-1/index.html#topics-covered",
    "href": "posts/gds-2022-10-03-polygons-part-1/index.html#topics-covered",
    "title": "Using polygons - part 1",
    "section": "Topics covered",
    "text": "Topics covered\nThis is the first article of a series of three. The topics covered in the three parts are:\n\nPart 1: Definitions: intersections, operations and measurements of polygons\n\nPolygons intersecting water and land.\n…and with water and land nearby.\nWhich country intersect this polygon?\nDistances between countries.\nSame country, different borders: how different are they?\n\nPart 2: Reference systems for polygons\n\nChanging reference system\nPolygons crossing the antimeridian\nWhat at the poles.\n\nPart 3: Random polygons and wandering polygons\n\nCreating random polygons\nFloating polygons\nPolygons deformed by a vector field"
  },
  {
    "objectID": "posts/gds-2022-10-03-polygons-part-1/index.html#polygons-intersecting-water-and-land",
    "href": "posts/gds-2022-10-03-polygons-part-1/index.html#polygons-intersecting-water-and-land",
    "title": "Using polygons",
    "section": "Polygons intersecting water and land",
    "text": "Polygons intersecting water and land\nLand and water on a map are typically modelled by polygons through the segmentation of the underlying geographical features.\nGiven a polygon or a bounding box drawn on a map as input, the goal is to answer the question\n\nWhat is the percentage of land and water that the polygon drawn on the map intersects?\n\nFor the example below we will use a polygon drawn around the city of Genova, Italy, (44.405551, 8.904023). As usual you are invited to reproduce the results using a different city.\n\nfrom copy import deepcopy\n\nfrom IPython.display import Image\n\nimport geopandas as gpd\nfrom geopy import distance\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport osmnx\nimport shapely\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action='ignore', category=DeprecationWarning)\n\nimport pandas as pd\n\nfrom geopy import distance\nfrom keplergl import KeplerGl\nfrom shapely.geometry import Polygon, Point, shape\n\n\nKEPLER_OUTPUT = False  # Render kepler output if True. Produces a screenshot for the blog article otherwise\n\n\nif KEPLER_OUTPUT:\n    map_0 = KeplerGl(height=800)\n    display(map_0)\nelse:\n    display(Image(\"images/map0.png\"))\n\n\n\n\n\n\n\n\nIn an interactive session the cell above produces creates an empty map where to draw a polygon and copy paste it in the cell below. In the static blog version this is not reproduced, as the variable KEPLER_OUTPUT is set to False.\nYou can get at the source notebook used to create this blog post and create a different polygon, using the drawing pencil (top right), then right click on it to copy the values and paste it in a dictionary.\n\ndict_roi = {\"type\":\"Polygon\",\"coordinates\":[[[8.987382009236233,44.54680601507832],[8.841126105836173,44.550231465826634],[8.723022747221052,44.53065474626796],[8.621398927017932,44.48511343018655],[8.591873087363503,44.3865638718846],[8.64817817693631,44.307506289856086],[8.75666847147733,44.33796344890003],[8.83906616353428,44.35613196160054],[8.950989695244745,44.3531860988552],[9.023087675794352,44.30554076888227],[9.079392765366244,44.274083482671955],[9.120591611394707,44.288339652995674],[9.143937624144632,44.33108740800724],[9.141877681842741,44.39784910785984],[9.090379124307875,44.49344084451141],[8.987382009236233,44.54680601507832]]]}\n\nsh_roi = shape(dict_roi)\n\nconfig_map1 = {\n    'version': 'v1',\n    'config': {\n        'mapState': {\n            \"bearing\": 0,\n            \"dragRotate\": False,\n            \"latitude\": 44.41404004333898,\n            \"longitude\": 8.871549089955757,\n            \"pitch\": 0,\n            \"zoom\": 10.347869665266995,\n            \"isSplit\": False,\n        }\n    }\n}\n\ngdf_roi =  gpd.GeoDataFrame({\"name\":[\"ROI\"], \"geometry\": [sh_roi]})\ngdf_roi = gdf_roi.set_crs('4326')  # reproject to WGS84 (the only one supported by kepler)\n\n# To reproduce exactly the same images shown in the article you will have to copy the config\n# file from the linked repo and save it in the save it in a configs.py file.\n# You can still follow the tutorial and run all the lines of code without the config.\ntry:\n    from configs import config_map1\nexcept ImportError:\n    config_map1 = dict()\n    \nif KEPLER_OUTPUT:\n    map_1 = KeplerGl(data=deepcopy({\"roi\": gdf_roi}), config=config_map1, height=800)\n    display(map_1)\nelse:\n    display(Image(\"images/map1.png\"))\n\n\n\n\n\n\n\n\nThe copy pasted geometry from the kepler app is a dictionary with a type and a list of coordinates following a conventional GeoJSON object. GeoJSON is a format for encoding data about geographic features using JavaScript Object Notation (json), established in 2015.\nA geojson to model a single point appears as:\n{\n  \"type\": \"Feature\",\n  \"geometry\": {\n    \"type\": \"Point\",\n    \"coordinates\": [125.6, 10.1]\n  },\n  \"properties\": {\n    \"name\": \"Dinagat Islands\"\n  }\n}\nWhere \"type\" can be Feature for single objects, or FeatureCollections for multiple objects, and where the geometry.type can be a Point, LineString, Polygon, MultiPoint, MultiLineString, and MultiPolygon.\nTo get the “water” within the selected geometry, we will use the osmnx library.\nThere are a few options to obtain land and water intersecting a region. It is possible to get the rivers, the sea (or bay), and the administrative regions and then consider to subtract the rivers to the administrative regions, and their intersection with the selected ROI. This option may leave some empty spaces, as in OSM there may be multiple annotators and the boundaries may not collimate. A different approach is to assume that all what is not water is land. So we query the rivers and the sea, we consider their union and their intersection with the ROI, and we call this new region “water”. The land will be the set difference between water and the ROI.\n\\[\n\\text{water}_{\\text{roi}} = (\\text{sea} \\cup \\text{river}) \\cap \\text{roi}\\\\\n\\text{land}_{\\text{roi}} = \\text{roi} \\setminus \\text{water}_{\\text{roi}}\n\\]\nSo we need to perform the binary polygon operations of union, intersection and subtraction of polygons, as well as to get the polygons of sea and rivers involved in the selected roi.\n\n# how to get tags:\n# https://wiki.openstreetmap.org/wiki/Map_features#Water_related\n\n# ~5 mins the first run (2 seconds after the first run, as results are stored in the local folder `cache`).\ngdf_rivers = osmnx.geometries_from_polygon(sh_roi, tags={'natural': 'water'})\ngdf_sea = osmnx.geometries_from_polygon(sh_roi, tags={'natural': 'bay'})\n\n# check crs is already set to WGS84\nassert gdf_rivers.crs == 4326\nassert gdf_sea.crs == 4326\n\n# Dissolve the \"geodataframe\" so that all of its rows are conflated into a single observation\ngdf_rivers = gdf_rivers.dissolve()\ngdf_sea = gdf_sea.dissolve()\n\n# Compute water_roi and land_roi\ngse_sea_union_river = gdf_rivers.union(gdf_sea)  # The union of two geodataframes is a geoseries (the union is only on the geometries)\n\ngse_water_roi = gse_sea_union_river.intersection(gdf_roi)\ngse_land_roi = gdf_roi.difference(gse_water_roi)\n\nyou can also explore the administrative boundaries for an alternative route to find the polygons segmenting land.\ngdf_region = ox.geometries_from_polygon(sh_poly, tags={'boundary': 'administrative'})\nBut this route is not explored here, you are invited to investigate and plot it.\n\n# Call deepcopy when passing an object prevents issue \"AttributeError: 'str' object has no attribute '_geom'\".\n# https://github.com/keplergl/kepler.gl/issues/1240\n\ngdf_genova = gpd.GeoDataFrame(\n    {\n        \"name\": [\"roi\", \"water\", \"land\"], \n        \"geometry\":[deepcopy(gdf_roi.geometry.iloc[0]), deepcopy(gse_water_roi.iloc[0]), deepcopy(gse_land_roi.iloc[0])]\n    }\n)\n\ngdf_genova = gdf_genova.set_crs('4326')\n\ndisplay(gdf_genova.head())\n\n\n\n\n\n\n\n\nname\ngeometry\n\n\n\n\n0\nroi\nPOLYGON ((8.98738 44.54681, 8.84113 44.55023, ...\n\n\n1\nwater\nMULTIPOLYGON (((8.62514 44.37508, 8.62505 44.3...\n\n\n2\nland\nMULTIPOLYGON (((8.98738 44.54681, 9.04759 44.5...\n\n\n\n\n\n\n\n\nkepler_data = {}\nfor _, row in gdf_genova.iterrows():\n    kepler_data.update({row[\"name\"]: gdf_genova[gdf_genova[\"name\"] == row[\"name\"]].copy()})\n\n\ntry:\n    from configs import config_map2\nexcept ImportError:\n    config_map2 = dict()\n\n\nif KEPLER_OUTPUT:\n    map_2 = KeplerGl(\n        data=deepcopy(kepler_data), \n        height=800,\n        config=config_map2,\n    )\n    display(map_2)\nelse:\n    display(Image(\"images/map2.png\"))\n\n\n\n\n\n\n\n\n\n# Reproject to cylindrical equal area projectcion\ngdf_genova[\"areas (Km2)\"] = gdf_genova.to_crs({'proj':'cea'}).area/ 10**6\ndisplay(gdf_genova[[\"name\", \"areas (Km2)\"]])\n\n\n\n\n\n\n\n\nname\nareas (Km2)\n\n\n\n\n0\nroi\n920.966148\n\n\n1\nwater\n330.635071\n\n\n2\nland\n590.333976"
  },
  {
    "objectID": "posts/gds-2022-10-03-polygons-part-1/index.html#topics-of-this-part-and-the-next-two",
    "href": "posts/gds-2022-10-03-polygons-part-1/index.html#topics-of-this-part-and-the-next-two",
    "title": "Using polygons",
    "section": "Topics of this part and the next two",
    "text": "Topics of this part and the next two\nThis is the first article of a series of three. The topics covered in the three parts are:\n\nPart 1: Definitions: intersections, operations and measurements of polygons\n\nPolygons intersecting water and land.\n…and with water and land nearby.\nWhich country intersect this polygon?\nDistances between countries.\nSame country, different borders: how different are they?\n\nPart 2: Reference systems for polygons\n\nChanging reference system\nPolygons crossing the antimeridian\nWhat at the poles.\n\nPart 3: Random polygons and wandering polygons\n\nCreating random polygons\nFloating polygons\nPolygons deformed by a vector field"
  },
  {
    "objectID": "posts/gds-2024-01-10-haversine-dist/index.html",
    "href": "posts/gds-2024-01-10-haversine-dist/index.html",
    "title": "Haversine’s Distance Mathematics",
    "section": "",
    "text": "Earth’s Models"
  },
  {
    "objectID": "posts/gds-2024-01-10-haversine-dist/index.html#earths-spherical-model",
    "href": "posts/gds-2024-01-10-haversine-dist/index.html#earths-spherical-model",
    "title": "Haversine’s Distance Mathematics",
    "section": "Earth’s Spherical Model",
    "text": "Earth’s Spherical Model\nIn any dimension, the most symmetric geometrical object is the sphere. Unlike planes, spheres are also symmetric when considered within their embedding in an higher dimensional space.\nEven though the earth is not a perfect sphere, also for its symmetric properties, the 2D sphere embedded in the 3D space is a very reasonable earth’s approximation. Amongst the models that are more accurate we can find the ellipsoid (which is the model used by the GPS systems, more precisely the WGS84 geodetic system), the geoid model, or even the local topographic elevations.\nWhich model is the best one?\nIf the local topography is not relevant and the computations has to be kept bare simple and fast, the sphere is the go to model of the earth. The conventional coordinates system, consisting of the pair longitude (East-West direction) and latitude (North-South direction) measured in degrees from the center of the modelling surface in the 3D space, can be projected in any of the mentioned model, and even more, such as the cylindrical, conical and plane model.\nAdding the radius as third coordinate, we can model the altitude for each point. Unlike latitude and longitude, the location of the zero for the altitude point is model-dependent, and will be different if on the sphere, the ellipsoid or others. The functions mapping the latitude, longitude (and altitude) coordinates across models are called maps projections1.\nFor a fixed radius \\(R\\), and for \\(\\text{rad}: \\text{Deg} \\rightarrow \\text{Rad}\\) the function mapping degrees to radians, we can rename the two angles with the radians with the conventional greek letters \\(\\theta\\) (theta) and \\(\\varphi\\) (phi):\n\\[\n\\begin{align*}\n\\theta &:= \\text{rad}(\\text{Lon}) \\\\\n\\varphi &:= \\text{rad}(\\text{Lat}) \\\\\n\\end{align*}\n\\]\nAnother basic map for the spherical model is \\(\\Pi: \\mathbb{R}^2 \\rightarrow \\mathbb{S}^2 \\subset \\mathbb{R}^3\\) that projects each pair \\((\\theta, \\varphi)\\) for \\(\\theta \\in [-\\pi, \\pi]\\) and \\(\\varphi \\in [-\\pi/2, \\pi/2]\\) on the sphere of radius \\(R\\):\n\\[\n\\Pi(\\theta, \\varphi) = \\begin{cases}\n       x = R \\cos(\\varphi) \\cos(\\theta)\\\\\n       y = R \\cos(\\varphi) \\sin(\\theta)\\\\\n       z = R \\sin(\\varphi)\\\\\n     \\end{cases}\n\\]\nThe function \\(\\Pi\\) is one of the many map projections, and the only one we will consider in this blog post; the reasoning behind its formulation can be derived from the definition of sine and cosine and from the drawing below.\n\n\n\n\n\n\nFigure 1\n\n\n\nPlease note that most mathematical textbooks have the angle \\(\\varphi\\) at zero when the point in on the z-axis, so \\(\\sin\\) and \\(\\cos\\) are swapped, and the angle’s domain would be \\([0,\\pi]\\). In this blot post and to remain faithful to the definition of latitude, we consider \\(\\varphi\\) at zero when the point is on the xy-plane with domain \\([-\\pi,\\pi]\\)."
  },
  {
    "objectID": "posts/gds-2024-01-10-haversine-dist/index.html#map-projection-on-the-sphere",
    "href": "posts/gds-2024-01-10-haversine-dist/index.html#map-projection-on-the-sphere",
    "title": "Haversine’s Distance Mathematics",
    "section": "Map Projection on the Sphere",
    "text": "Map Projection on the Sphere\nTo check all is working correctly we will plot on the 3D sphere a sequence of equidistant points around the equator (in red), around the tropic of Cancer (in green), and around the Arctic Polar circle (of course in blue), and then the cities of Rome, Paris and London (in black).\nWe also plot the meridian zero (in gray) to make sense of the position of the three cities above.\n\nimport numpy as np\n\n\nR_Km = 6371 \n\ndef lon_lat_to_3d(lon, lat):\n    theta, phi = map(np.radians, [lon, lat])\n    x = R_Km * np.cos(phi) * np.cos(theta)\n    y = R_Km * np.cos(phi) * np.sin(theta)\n    z = R_Km * np.sin(phi)\n    return (x, y, z)\n    \n\n\nN = 40\n\n# Create N equidistant points around the equator\nlon_at_equator_deg = np.linspace(0, 360, N)[:-1]\nlat_at_equator_deg = 0 * np.ones_like(lon_at_equator_deg)\n\n# Create N equidistant points around the tropic of cancer\nlon_at_tropic_deg = np.linspace(0, 360, N)[:-1]\nlat_at_tropic_deg = 23.43617 * np.ones_like(lon_at_tropic_deg)\n\n# Create N equidistant points around the north pole\nlon_at_arctic_deg = np.linspace(0, 360, N)[:-1]\nlat_at_arctic_deg = 76.25 * np.ones_like(lon_at_arctic_deg)\n\n# Create N equidistant points at meridian 0\nlat_at_meridian_deg = np.linspace(-90, 90, N)[:-1]\nlon_at_meridian_deg = -90 * np.ones_like(lat_at_meridian_deg) \n\n# a selection of 3 cities\nlon_lat_london = ( 0.1276, 51.5072)\nlon_lat_paris  = ( 2.3522, 48.8566)\nlon_lat_rome   = (12.4964, 41.9028)\n\n\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm, colors\nimport numpy as np\n\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\n\n# Sphere surface\nlon, lat = np.mgrid[-90:90:100j, 0.0:360:100j]\nx, y , z = lon_lat_to_3d(lon, lat)\nax.plot_surface(x, y, z,  rstride=1, cstride=1, color=\"#303E4F\", alpha=0.2, linewidth=0)\n\n# Parallels\nxx_equator, yy_equator, zz_equator = lon_lat_to_3d(lon_at_equator_deg, lat_at_equator_deg)\nax.scatter(xx_equator, yy_equator, zz_equator, color=\"r\",s=20)\n\nxx_tropic, yy_tropic, zz_tropic = lon_lat_to_3d(lon_at_tropic_deg, lat_at_tropic_deg)\nax.scatter(xx_tropic , yy_tropic, zz_tropic, color=\"g\",s=20)\n\nxx_arctic, yy_arctic, zz_arctic = lon_lat_to_3d(lon_at_arctic_deg, lat_at_arctic_deg)\nax.scatter(xx_arctic, yy_arctic, zz_arctic, color=\"b\",s=20)\n\n# Meridian\nxx_meridian, yy_meridian, zz_meridian = lon_lat_to_3d(lon_at_meridian_deg, lat_at_meridian_deg)\nax.scatter(xx_meridian, yy_meridian, zz_meridian, color=\"gray\",s=20)\n\n# Cities\ncities = [lon_lat_london, lon_lat_paris, lon_lat_rome]\nlon_cities = [a[0] for a in cities]\nlat_cities = [a[1] for a in cities]\nxx_cities, yy_cities, zz_cities = lon_lat_to_3d(lon_cities, lat_cities)\nax.scatter(xx_cities, yy_cities, zz_cities, color=\"k\",s=20)\n\n# Plot\nax.set_aspect(\"equal\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFrom the plot above we can see that all works as expected, including the fact that the points at the tropic of cancer, even if they have the same longitude spacing as the point on the equator, have a significantly smaller mutual distance. They obviously come close to each other when approaching the north pole, becoming a single point at the pole itself.\nIf we want to keep the same “horizontal distance” or the distance on the tropic of Cancer as we have it at the equator, then we have to consider a smaller radius (that is the radius that varies in function of the latitude and that is given by \\(R\\cos(\\varphi)\\)) instead of the radius at the equator (that is \\(R\\)).\nThe drawing below may convince you about the need to multiply by the cosine of the latitude to have the sought result.\n\n\n\n\n\n\nFigure 2\n\n\n\nSo now we know how to adjust for the number of points to maintain their distance equal while modulating the latitude.\n\ndef num_points_at_latitude(num_points, latitude_deg):\n    return int(num_points * np.cos(np.radians(latitude_deg)))\n\n# Create N equidistant points around the equator\nlon_at_equator_deg = np.linspace(0, 360, N)[:-1]\nlat_at_equator_deg = 0 * np.ones_like(lon_at_equator_deg)\n\n# Create equidistant points around the tropic of cancer, as at the equator\nN_tropic = num_points_at_latitude(N, 23.43617)\nlon_at_tropic_deg = np.linspace(0, 360, N_tropic)[:-1]\nlat_at_tropic_deg = 23.43617 * np.ones_like(lon_at_tropic_deg)\n\n# Create equidistant points around the north pole, as at the equator\nN_arctic = num_points_at_latitude(N, 76.25)\nlon_at_arctic_deg = np.linspace(0, 360, N_arctic)[:-1]\nlat_at_arctic_deg = 76.25 * np.ones_like(lon_at_arctic_deg)\n\nprint(\"number of points\")\nprint(f\"at the arctic: {N_arctic}\")\nprint(f\"at the tropic: {N_tropic}\")\nprint(f\"at the equator: {N}\")\n\nnumber of points\nat the arctic: 9\nat the tropic: 36\nat the equator: 40\n\n\n\n# Repeated code from cells above\n\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\n\n# Sphere surface\nlon, lat = np.mgrid[-90:90:100j, 0.0:360:100j]\nx, y , z = lon_lat_to_3d(lon, lat)\nax.plot_surface(x, y, z,  rstride=1, cstride=1, color=\"#303E4F\", alpha=0.2, linewidth=0)\n\n# Parallels\nxx_equator, yy_equator, zz_equator = lon_lat_to_3d(lon_at_equator_deg, lat_at_equator_deg)\nax.scatter(xx_equator, yy_equator, zz_equator, color=\"r\",s=20)\n\nxx_tropic, yy_tropic, zz_tropic = lon_lat_to_3d(lon_at_tropic_deg, lat_at_tropic_deg)\nax.scatter(xx_tropic , yy_tropic, zz_tropic, color=\"g\",s=20)\n\nxx_arctic, yy_arctic, zz_arctic = lon_lat_to_3d(lon_at_arctic_deg, lat_at_arctic_deg)\nax.scatter(xx_arctic, yy_arctic, zz_arctic, color=\"b\",s=20)\n\n# Plot\nax.set_aspect(\"equal\")\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/gds-2024-01-10-haversine-dist/index.html#two-points-distance",
    "href": "posts/gds-2024-01-10-haversine-dist/index.html#two-points-distance",
    "title": "Haversine’s Distance Mathematics",
    "section": "Two Points’ Distance",
    "text": "Two Points’ Distance\nAs said above, latitude and longitude coordinates are independent from the chosen model. The model we use to project the coordinates becomes relevant for computing the distance between two points.\nThe Haversine distance is the shortest distance between two points in longitude and latitude coordinates on a spherical model. Its unit of measurement is the same as the one passed as input for the radius.\nBefore introducing the Haversine distance between \\(A = (\\text{Lon}_{A}, \\text{Lat}_{A})\\) and \\(B = (\\text{Lon}_{B}, \\text{Lat}_{B})\\), we start with two related concepts that can be seen as its approximation:\n\nEuclidean distance between the corresponding points on the sphere: \\[\nd_{\\text{Eucl}}(A, B) = \\lVert \\Pi(\\text{rad}(B)) - \\Pi(\\text{rad}(A)) \\rVert = \\sqrt{(x_B - x_A)^2 + (y_B - y_A)^2 + (z_B - z_A)^2}\n\\] where \\(\\text{rad}(A) = (\\varphi_{A}, \\theta_{A})\\) are the angle expressed in radiants. The formual above simply leverages on the Pythagorean theorem in respect to the 3D coordinate axis, and represents the distance we would travel if we could dig a straight tunnel connecting \\(A\\) and \\(B\\). It always underestimate the distance on the sphere, in the worst case by a factor of \\(\\pi \\mathcal{R}\\) for \\(\\mathcal{R}\\) earth radius.\n\nThe Euclidean distance is not very useful if we want to know the distance on the sphere!\nCan we adjust for the curvature of the sphere getting inspired by Figure 3? For example we use the Pythagorean theorem on the surface of the sphere respect to the meridians and the parallels, and adjust for the fact that the latitude distance shrinks when moving away from the equator.\n\nDistance of two points along the same meridian: introducing the point \\(P = (\\text{Lon}_{B}, \\text{Lat}_{A})\\) that makes \\(\\bigtriangleup APB\\) a right triangle on the surface of the sphere. The arch \\(\\hat{BP}\\) runs along a meridian, and its lenght, converted to \\(\\text{Km}\\) for the radius \\(\\mathcal{R} = 6371~\\text{Km}\\) is: \\[\n\\hat{BP} = \\frac{2\\pi \\mathcal{R}}{360^{o}}\\left\\vert \\text{Lat}_{B} - \\text{Lat}_{P}\\right\\vert\n= \\frac{2\\pi \\mathcal{R}}{360^{o}}\\vert \\text{Lat}_{B} - \\text{Lat}_{A}\\vert\n\\]\nDistance of two points along the same parallel: as we saw, the distance along the arch \\(\\hat{AP}\\) is reduced by the cosine of the latitude, hence: \\[\n\\hat{AP} = \\frac{2\\pi \\mathcal{R} \\cos(\\text{rad}(\\text{Lat}_{A}))}{360^{o}}\\left\\vert \\text{Lon}_{A} - \\text{Lon}_{P}\\right\\vert\n= \\frac{2\\pi \\mathcal{R} \\cos(\\text{rad}(\\text{Lat}_{A}))}{360^{o}}\\vert \\text{Lon}_{A} - \\text{Lon}_{B}\\vert\n\\]\n\nAnd so we can use again the Pythagorean theorem to compute \\(\\hat{AB}\\) as an Euclidean-spherical distance:\n\\[\nd_{\\text{Sph}}(A, B) = \\sqrt{\\hat{AP}^ 2 + \\hat{BP}^2}\n= \\frac{2\\pi \\mathcal{R}}{360^{o}} \\sqrt{ \\cos^2(\\text{rad}(\\text{Lat}_{A}))(\\text{Lon}_{A} - \\text{Lon}_{B})^2 + (\\text{Lat}_{A} - \\text{Lat}_{B})^2  }\n\\]\nThis is too an approximation. Even if the lenghts of the sides of the triangle are exact, the curvature of the trajectory \\(\\hat{AB}\\) itself is not taken into account."
  },
  {
    "objectID": "posts/gds-2024-01-10-haversine-dist/index.html#haversine-distance",
    "href": "posts/gds-2024-01-10-haversine-dist/index.html#haversine-distance",
    "title": "Haversine’s Distance Mathematics",
    "section": "Haversine Distance",
    "text": "Haversine Distance\nThe examples proposed above are useful to develop some geometrical intuition about the distances on the sphere. The Haversine distance, after the provides us with the a closed form solution to compute the length of the geodesic curve on the sphere, which is the shortest path on the surface of the sphere between the point \\(A\\) and \\(B\\).\nThanks to the already mentioned symmetry, the shortest path between two points is the arch defined by the intersection between the sphere and the plane passing through \\(A\\), \\(B\\) and the centre of the sphere. This plane always cut the sphere in half, and again by symmetry, and rotating the sphere so that \\(A\\) and \\(B\\) are always on the equator, it is easy to see why a curve made in this way is the shortest one.\n\n\n\n\n\n\nFigure 3\n\n\n\nLet \\(d\\) be the sought geodesic distance between \\(A\\) and \\(B\\) and \\(\\lambda\\) the angle at the center of the sphere in radians. By definition of radians we have:\n\\[\n\\frac{d}{\\mathcal{R}} = \\lambda\n\\] \\[\n\\frac{d}{\\mathcal{R}} = 2 \\frac{\\lambda}{2}\n\\] \\[\n\\frac{d}{\\mathcal{R}} = 2 \\arcsin \\left(\\sqrt{\\sin^2(\\lambda / 2 )}\\right)\n\\] \\[\nd = 2 \\mathcal{R} \\arcsin \\left(\\sqrt{\\sin^2(\\lambda / 2 )}\\right)\n\\] \\[\nd = 2 \\mathcal{R} \\arcsin \\left(\\sqrt{ \\text{hav}(\\lambda) }\\right)\n\\tag{1}\\] Where \\(h\\) is the Haversine function given by: \\[\n\\text{hav}(\\lambda) := \\sin^2\\left(\\frac{\\lambda}{2}\\right) = \\frac{1 - \\cos(\\lambda)}{2}~.\n\\tag{2}\\]\nThe haversine function is particularly handy when we want to avoid computing the \\(\\sin\\), and to transform it into a \\(\\cos\\) function. Since the \\(\\sin\\) for small angle is numerically unstable (it collapses to zero for small enough values in the Taylor expansion), given a \\(\\lambda\\) we can chose if we want to compute \\(\\text{hav}{\\lambda}\\) via the \\(\\sin\\) or the \\(\\cos\\).\nFrom Equation 1 with the definition of the haversine function and some derivations detailed afterwards, we can get: \\[\nd = 2 \\mathcal{R} \\arcsin\n\\left(\n    \\left[  \n        \\text{hav}(\\varphi_B - \\varphi_A) + \\cos\\varphi_A \\cos\\varphi_B ~\\text{hav}(\\theta_B - \\theta_A)\n    \\right]^{1/2}\n\\right) \\\\\n\\tag{3}\\] And equivalently \\[\nd = 2 \\mathcal{R} \\arcsin\n\\left(\n    \\left[  \n        \\text{hav}(\\varphi_B - \\varphi_A) + (1 - \\text{hav}(\\varphi_B - \\varphi_A) - \\text{hav}(\\varphi_B + \\varphi_A) ) \\text{hav}(\\theta_B - \\theta_A)\n    \\right]^{1/2}\n\\right)\n\\tag{4}\\]\nTo prove that the above formulae are correct, we still have to prove that \\[\n\\text{hav}(\\lambda) =\n        \\text{hav}(\\varphi_B - \\varphi_A) + \\cos\\varphi_A \\cos\\varphi_B ~\\text{hav}(\\theta_B - \\theta_A)\n\\tag{5}\\] and \\[\n\\text{hav}(\\lambda) =\n        \\text{hav}(\\varphi_B - \\varphi_A) + (1 - \\text{hav}(\\varphi_B - \\varphi_A) - \\text{hav}(\\varphi_B + \\varphi_A) ) \\text{hav}(\\theta_B - \\theta_A)\n\\tag{6}\\]\n\nProof of Equation 5\nHypothesis: \\(A\\) and \\(B\\) points on the surface of a sphere, with coordinates \\(A=(\\varphi_A, \\theta_A) = (\\cos\\theta_A~\\cos\\varphi_A, \\sin\\theta_A~\\cos\\varphi_A, \\sin\\varphi_A )\\) and \\(B=(\\varphi_B, \\theta_B) = (\\cos\\theta_B~\\cos\\varphi_B, \\sin\\theta_B~\\cos\\varphi_B, \\sin\\varphi_B )\\) and \\(\\lambda\\) angle at the centre of the sphere.\nThesis: \\(\\text{hav}(\\lambda) = \\text{hav}(\\varphi_B - \\varphi_A) + \\cos\\varphi_A \\cos\\varphi_B ~\\text{hav}(\\theta_B - \\theta_A)\\).\nIt is always possible to scale the space to have the unitary radius and to rotate the axis so to have \\(\\theta_A = 0\\), and \\(\\tilde{\\theta}_B = \\theta_B - \\theta_A\\). In the new coordinate system we have: \\[\nA= (\\cos\\varphi_A, 0, \\sin\\varphi_A )\n\\] \\[\nB=(\\cos\\tilde{\\theta}_B~\\cos\\varphi_B, \\sin\\tilde{\\theta}_B~\\cos\\varphi_B, \\sin\\varphi_B )\n\\] How do we connect the able \\(\\lambda\\) with \\(A\\) and \\(B\\)? With the scalar product, of course! Then we can add and subtract a ghost term and apply the addition subtraction formulae: \\[\n\\begin{align*}\n\\cos\\lambda &= A \\cdot B \\\\\n&= \\cos\\varphi_A~\\cos\\tilde{\\theta}_B~\\cos\\varphi_B + \\sin\\varphi_A~\\sin\\varphi_B \\\\\n&= \\cos\\varphi_A~\\cos\\tilde{\\theta}_B~\\cos\\varphi_B + \\sin\\varphi_A~\\sin\\varphi_B + \\cos\\varphi_A~\\cos\\varphi_B -  \\cos\\varphi_A~\\cos\\varphi_B \\\\\n&= \\cos\\varphi_A~\\cos\\varphi_B(\\cos\\tilde{\\theta}_B - 1) + \\cos(\\varphi_A - \\varphi_B)\n\\end{align*}\n\\] With simple algebraic manipulations the previous equation becomes: \\[\n\\begin{align*}\n\\frac{1 - \\cos\\lambda}{2}\n= \\frac{1 - \\cos(\\varphi_A - \\varphi_B)}{2} - \\frac{1}{2}  \\cos\\varphi_A~\\cos\\varphi_B(\\cos\\tilde{\\theta}_B - 1)\n\\end{align*}\n\\] and considering that for any angle \\(\\alpha\\) we have \\(\\cos(\\alpha) = \\cos(-\\alpha)\\), the previous equation is equivalent to the sought thesis: \\[\n\\text{hav}(\\lambda) = \\text{hav}(\\varphi_B - \\varphi_A) + \\cos\\varphi_A \\cos\\varphi_B ~\\text{hav}(\\theta_B - \\theta_A)\n\\]\n\n\nProof of Equation 6\nUnder the hypothesis of the previous proof, the thesis reduces to \\(\\cos\\varphi_A \\cos\\varphi_B = 1 - \\text{hav}(\\varphi_B - \\varphi_A) - \\text{hav}(\\varphi_B + \\varphi_A)\\).\nThe proof starts and end as a direct consequence of the addition subtraction formulae: \\[\n\\begin{align*}\n\\cos\\varphi_A \\cos\\varphi_B\n&= \\frac{1}{2}\\left(  \\cos(\\varphi_B - \\varphi_A) + \\cos(\\varphi_B + \\varphi_A) \\right) \\\\\n&= 1 - \\frac{1}{2} - \\frac{1}{2} + \\frac{\\cos(\\varphi_B - \\varphi_A)}{2} + \\frac{\\cos(\\varphi_B + \\varphi_A)}{2} \\\\\n&= 1  - \\frac{ 1 - \\cos(\\varphi_B - \\varphi_A)}{2} - \\frac{1 - \\cos(\\varphi_B + \\varphi_A)}{2} \\\\\n&= 1  - \\text{hav}(\\varphi_B - \\varphi_A) - \\text{hav}(\\varphi_B + \\varphi_A) \\\\\n\\end{align*}\n\\] which concludes the proof of the Haversine formula."
  },
  {
    "objectID": "posts/gds-2024-01-10-haversine-dist/index.html#numerical-stability-consideration",
    "href": "posts/gds-2024-01-10-haversine-dist/index.html#numerical-stability-consideration",
    "title": "Haversine’s Distance Mathematics",
    "section": "Numerical Stability Consideration",
    "text": "Numerical Stability Consideration"
  },
  {
    "objectID": "posts/gds-2024-01-10-haversine-dist/index.html#implementation-and-examples",
    "href": "posts/gds-2024-01-10-haversine-dist/index.html#implementation-and-examples",
    "title": "Haversine’s Distance Mathematics",
    "section": "Implementation and examples",
    "text": "Implementation and examples\nFrom a computational point of view, as sadi before, we want to avoid computing \\(\\sin\\) when the angles are very small. So a stable implementation would use the last part of Equation 2.\n\ndef hav_function(theta_A, phi_A, theta_B, phi_B):\n    # haversine computed with the sin\n    hav_rad = lambda  x: np.sin(x/2) ** 2\n    return hav_rad(phi_B - phi_A) + (1 - hav_rad(phi_B - phi_A) - hav_rad(phi_B + phi_A) ) * hav_rad(theta_B - theta_A)\n\ndef haversine_distance(lon_1, lon_2, lat_1, lat_2):\n    theta_1, phi_1, theta_2, phi_2 = map(np.radians, [lon_1, lon_2, lat_1, lat_2])\n    return 2 * R_Km * np.arcsin(np.sqrt(hav_function(theta_1, phi_1, theta_2, phi_2)))\n\n\n# Verify that it looks correct:\nPREC = 6\nprint(f\"Distance London to London = {round(haversine_distance(*lon_lat_london, *lon_lat_london), PREC)} Km\")\nprint(f\"Distance London to Paris  = {round(haversine_distance(*lon_lat_london, *lon_lat_paris), PREC)} Km\")\nprint(f\"Distance Paris to London  = {round(haversine_distance(*lon_lat_paris, *lon_lat_london), PREC)} Km\")\n\nDistance London to London = 0.0 Km\nDistance London to Paris  = 334.563443 Km\nDistance Paris to London  = 334.563443 Km\n\n\nIt seems to be working correctly. Let’s compare it with an out of the box function from the library sklearn (see also this stack overflow discussion and this one)\n\nfrom sklearn.metrics.pairwise import haversine_distances\n\nlon_A, lat_A = lon_lat_london\nlon_B, lat_B = lon_lat_paris\nvals = [[np.radians(lat_A), np.radians(lon_A)], [np.radians(lat_B), np.radians(lon_B)]]\ndistance_sklearn = R_Km * haversine_distances(vals)\n\nprint(f\"Distance London to Paris for sklearn  = {round(distance_sklearn[1,0], PREC)} Km\")\n\nDistance London to Paris for sklearn  = 334.563443 Km"
  },
  {
    "objectID": "posts/gds-2024-01-10-haversine-dist/index.html#see-also",
    "href": "posts/gds-2024-01-10-haversine-dist/index.html#see-also",
    "title": "Haversine’s Distance Mathematics",
    "section": "See also…",
    "text": "See also…\nVincentry formula\n\n#https://stackoverflow.com/questions/4913349/haversine-formula-in-python-bearing-and-distance-between-two-gps-points\n# use also geopy.distance for a comparison\n\n\n\nfrom geopy import distance\n\nprint(distance.distance(wedistance.distance(lon_lat_london[::-1], lon_lat_paris[::-1]).km\n\nSyntaxError: incomplete input (694376945.py, line 3)"
  },
  {
    "objectID": "posts/gds-2024-01-10-haversine-dist/index.html#see-also-vincetys-formula",
    "href": "posts/gds-2024-01-10-haversine-dist/index.html#see-also-vincetys-formula",
    "title": "Haversine’s Distance Mathematics",
    "section": "See also: Vincety’s formula",
    "text": "See also: Vincety’s formula\nTo compute the distance between two points on an ellipsoid model (which is more accurate than the spherical model on which the Haversine is based on) there is the Vincety formula.\nIts mathematical derivation may be the topic for another post, though if you want to have it computed with a tool out of the box, we recommend the library geopy.\n\nfrom geopy import distance\n\n\ndist_haversine = haversine_distance(*lon_lat_london, *lon_lat_paris)\ndist_vincenty = distance.distance(lon_lat_london[::-1], lon_lat_paris[::-1]).km\n\nPREC = 4\n\nprint(f\"Distance with haversine =  {dist_haversine} Km\")\nprint(f\"Distance with Vincenty =  {dist_vincenty} Km\")\nprint(f\"Relative error = |ground - calculated|/|ground| = {round(100 * abs(dist_vincenty - dist_haversine) / dist_vincenty, PREC)} %\")\n\nDistance with haversine =  334.5634431209046 Km\nDistance with Vincenty =  334.88388938435094 Km\nRelative error = |ground - calculated|/|ground| = 0.0957 %"
  },
  {
    "objectID": "posts/gds-2024-01-10-haversine-dist/index.html#numerical-stability",
    "href": "posts/gds-2024-01-10-haversine-dist/index.html#numerical-stability",
    "title": "Haversine’s Distance Mathematics",
    "section": "Numerical stability",
    "text": "Numerical stability\nLet’s see what happens in term of numerical stability. We take now two points in london very close to each others:\n\nlon_lat_london_1 = ( 0.1276, 51.5072)\nlon_lat_london_2 = ( 0.1276, 51.50720001)\nPREC = 20\n\nprint(f\"Very small distance (hav with cosine) = {round(haversine_distance(*lon_lat_london_1, *lon_lat_london_2), PREC)} Km\")\n\nVery small distance (hav with cosine) = 1.11194938401393e-06 Km\n\n\nWe can see that our implementation with the haversine function, computed with the `sin`` version, does not collapse to zero.\nWhat happens if we re-implement with function with the cosine?\n\n# repeated code with a small change!\n\ndef hav_function(theta_A, phi_A, theta_B, phi_B):\n    # Haversine computed with the cosine - Numerically unstable!\n    hav_rad = lambda  x: (1-np.cos(x))/2\n    return hav_rad(phi_B - phi_A) + (1 - hav_rad(phi_B - phi_A) - hav_rad(phi_B + phi_A) ) * hav_rad(theta_B - theta_A)\n\ndef haversine_distance(lon_1, lon_2, lat_1, lat_2):\n    theta_1, phi_1, theta_2, phi_2 = map(np.radians, [lon_1, lon_2, lat_1, lat_2])\n    return 2 * R_Km * np.arcsin(np.sqrt(hav_function(theta_1, phi_1, theta_2, phi_2)))\n\n\nprint(f\"Distance London to Paris (hav with sine) = {round(haversine_distance(*lon_lat_london, *lon_lat_paris), PREC)} Km\")\nprint(f\"Very small distance (hav with sine)      = {round(haversine_distance(*lon_lat_london_1, *lon_lat_london_2), PREC)} Km\")\n\nDistance London to Paris (hav with sine) = 334.5634431209046 Km\nVery small distance (hav with sine)      = 0.0 Km\n\n\nThe function works well for large angles, though for small angle the distance collapses.\nHad the developers of sklearn use the sin or the cos?\n\nlon_A, lat_A = lon_lat_london_1\nlon_B, lat_B = lon_lat_london_2\nvals = [[np.radians(lat_A), np.radians(lon_A)], [np.radians(lat_B), np.radians(lon_B)]]\ndistance_sklearn = R_Km * haversine_distances(vals)\n\nprint(f\"Distance London to Paris for sklearn  = {round(distance_sklearn[1,0], PREC)} Km\")\n\nDistance London to Paris for sklearn  = 1.11194938401393e-06 Km\n\n\nWe can see that (at least for this case) our results and the sklearn implementation is the same!"
  },
  {
    "objectID": "posts/gds-2024-01-10-haversine-dist/index.html#footnotes",
    "href": "posts/gds-2024-01-10-haversine-dist/index.html#footnotes",
    "title": "Haversine’s distance mathematics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee for example by JP Snyder · 1987 “Map projections: A working manual”, U.S. Government Printing Office, or H.S. Roblin “Map projections”, Edward Arnold Publisher Ltd.↩︎"
  },
  {
    "objectID": "posts/gds-2024-01-10-haversine-dist/index.html#map-projection-on-the-sphere-experiments",
    "href": "posts/gds-2024-01-10-haversine-dist/index.html#map-projection-on-the-sphere-experiments",
    "title": "Haversine’s Distance Mathematics",
    "section": "Map Projection on the Sphere Experiments",
    "text": "Map Projection on the Sphere Experiments\nTo check all is working correctly we will plot on the 3D sphere a sequence of equidistant points around the equator (in red), around the tropic of Cancer (in green), around the Arctic Polar circle (of course in blue), and then the cities of Rome, Paris and London (in black). To show a visual reference to make sense of the position of these three cities, we also plot the meridian zero (in gray).\n\nimport numpy as np\n\n\nR_Km = 6371 \n\ndef lon_lat_to_3d(lon, lat):\n    theta, phi = map(np.radians, [lon, lat])\n    x = R_Km * np.cos(phi) * np.cos(theta)\n    y = R_Km * np.cos(phi) * np.sin(theta)\n    z = R_Km * np.sin(phi)\n    return (x, y, z)\n    \n\n\nN = 40\n\n# Create N equidistant points around the equator\nlon_at_equator_deg = np.linspace(0, 360, N)[:-1]\nlat_at_equator_deg = 0 * np.ones_like(lon_at_equator_deg)\n\n# Create N equidistant points around the tropic of cancer\nlon_at_tropic_deg = np.linspace(0, 360, N)[:-1]\nlat_at_tropic_deg = 23.43617 * np.ones_like(lon_at_tropic_deg)\n\n# Create N equidistant points around the north pole\nlon_at_arctic_deg = np.linspace(0, 360, N)[:-1]\nlat_at_arctic_deg = 76.25 * np.ones_like(lon_at_arctic_deg)\n\n# Create N equidistant points at meridian 0\nlat_at_meridian_deg = np.linspace(-90, 90, N)[:-1]\nlon_at_meridian_deg = -90 * np.ones_like(lat_at_meridian_deg) \n\n# a selection of 3 cities\nlon_lat_london = ( 0.1276, 51.5072)\nlon_lat_paris  = ( 2.3522, 48.8566)\nlon_lat_rome   = (12.4964, 41.9028)\n\n\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm, colors\nimport numpy as np\n\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\n\n# Sphere surface\nlon, lat = np.mgrid[-90:90:100j, 0.0:360:100j]\nx, y , z = lon_lat_to_3d(lon, lat)\nax.plot_surface(x, y, z,  rstride=1, cstride=1, color=\"#303E4F\", alpha=0.2, linewidth=0)\n\n# Parallels\nxx_equator, yy_equator, zz_equator = lon_lat_to_3d(lon_at_equator_deg, lat_at_equator_deg)\nax.scatter(xx_equator, yy_equator, zz_equator, color=\"r\",s=20)\n\nxx_tropic, yy_tropic, zz_tropic = lon_lat_to_3d(lon_at_tropic_deg, lat_at_tropic_deg)\nax.scatter(xx_tropic , yy_tropic, zz_tropic, color=\"g\",s=20)\n\nxx_arctic, yy_arctic, zz_arctic = lon_lat_to_3d(lon_at_arctic_deg, lat_at_arctic_deg)\nax.scatter(xx_arctic, yy_arctic, zz_arctic, color=\"b\",s=20)\n\n# Meridian\nxx_meridian, yy_meridian, zz_meridian = lon_lat_to_3d(lon_at_meridian_deg, lat_at_meridian_deg)\nax.scatter(xx_meridian, yy_meridian, zz_meridian, color=\"gray\",s=20)\n\n# Cities\ncities = [lon_lat_london, lon_lat_paris, lon_lat_rome]\nlon_cities = [a[0] for a in cities]\nlat_cities = [a[1] for a in cities]\nxx_cities, yy_cities, zz_cities = lon_lat_to_3d(lon_cities, lat_cities)\nax.scatter(xx_cities, yy_cities, zz_cities, color=\"k\",s=20)\n\n# Plot\nax.set_aspect(\"equal\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFrom the plot above we can see that all works as expected, including the fact that the points at the tropic of cancer, even if they have the same longitude spacing as the point on the equator, have a significantly smaller mutual distance. They obviously come close to each other when approaching the north pole, becoming a single point at the pole itself.\nIf we want to keep the same “horizontal distance” or the distance on the tropic of Cancer as we have it at the equator, then we have to consider a smaller radius (that is the radius that varies in function of the latitude and that is given by \\(R\\cos(\\varphi)\\)) instead of the radius at the equator (that is \\(R\\)).\nThe drawing below may convince you about the need to multiply by the cosine of the latitude to have the sought result.\n\n\n\n\n\n\nFigure 2\n\n\n\nSo now we know how to adjust for the number of points to maintain their distance equal while modulating the latitude.\n\ndef num_points_at_latitude(num_points, latitude_deg):\n    return int(num_points * np.cos(np.radians(latitude_deg)))\n\n# Create N equidistant points around the equator\nlon_at_equator_deg = np.linspace(0, 360, N)[:-1]\nlat_at_equator_deg = 0 * np.ones_like(lon_at_equator_deg)\n\n# Create equidistant points around the tropic of cancer, as at the equator\nN_tropic = num_points_at_latitude(N, 23.43617)\nlon_at_tropic_deg = np.linspace(0, 360, N_tropic)[:-1]\nlat_at_tropic_deg = 23.43617 * np.ones_like(lon_at_tropic_deg)\n\n# Create equidistant points around the north pole, as at the equator\nN_arctic = num_points_at_latitude(N, 76.25)\nlon_at_arctic_deg = np.linspace(0, 360, N_arctic)[:-1]\nlat_at_arctic_deg = 76.25 * np.ones_like(lon_at_arctic_deg)\n\nprint(\"number of points\")\nprint(f\"at the arctic: {N_arctic}\")\nprint(f\"at the tropic: {N_tropic}\")\nprint(f\"at the equator: {N}\")\n\nnumber of points\nat the arctic: 9\nat the tropic: 36\nat the equator: 40\n\n\n\n# Repeated code from cells above\n\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\n\n# Sphere surface\nlon, lat = np.mgrid[-90:90:100j, 0.0:360:100j]\nx, y , z = lon_lat_to_3d(lon, lat)\nax.plot_surface(x, y, z,  rstride=1, cstride=1, color=\"#303E4F\", alpha=0.2, linewidth=0)\n\n# Parallels\nxx_equator, yy_equator, zz_equator = lon_lat_to_3d(lon_at_equator_deg, lat_at_equator_deg)\nax.scatter(xx_equator, yy_equator, zz_equator, color=\"r\",s=20)\n\nxx_tropic, yy_tropic, zz_tropic = lon_lat_to_3d(lon_at_tropic_deg, lat_at_tropic_deg)\nax.scatter(xx_tropic , yy_tropic, zz_tropic, color=\"g\",s=20)\n\nxx_arctic, yy_arctic, zz_arctic = lon_lat_to_3d(lon_at_arctic_deg, lat_at_arctic_deg)\nax.scatter(xx_arctic, yy_arctic, zz_arctic, color=\"b\",s=20)\n\n# Plot\nax.set_aspect(\"equal\")\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/gds-2024-01-10-haversine-dist/index.html#introduction",
    "href": "posts/gds-2024-01-10-haversine-dist/index.html#introduction",
    "title": "Haversine’s distance mathematics",
    "section": "Introduction",
    "text": "Introduction\nThis blog post is for the reader interested in building an intuition for how distances on the sphere are computed (Section 3, Section 4), to understand the details of the maths behind the Haversine distance (Section 5), to have an implementation in python with some examples and details about the numerical stability (Section 6, Section 7), and a conclusion about what to use in the python practice (Section 8). This last one is the only section you should look at if you are here to compute the Haversine distance quickly and accurately.\nThe article starts with the somehow pedantic though possibly important section about the motivations and formulas behind the spherical model."
  },
  {
    "objectID": "posts/gds-2024-01-10-haversine-dist/index.html#sec-spherical-model",
    "href": "posts/gds-2024-01-10-haversine-dist/index.html#sec-spherical-model",
    "title": "Haversine’s distance mathematics",
    "section": "Earth’s Spherical Model",
    "text": "Earth’s Spherical Model\nIn any dimension, the most symmetric geometrical object is the sphere. Unlike planes, spheres are also symmetric when considered within their embedding in an higher dimensional space.\nEven though the earth is not a perfect sphere, also for its symmetric properties, the 2D sphere embedded in the 3D space is a very reasonable earth’s approximation. Amongst the models that are more accurate we can find the ellipsoid (which is the model used by the GPS systems, more precisely the WGS84 geodetic system), the geoid model, or even the local topographic elevations.\nWhich model is the best one?\nIf the local topography is not relevant and the computations has to be kept bare simple and fast, the sphere is the go to model of the earth. The conventional coordinates system, consisting of the pair longitude (East-West direction) and latitude (North-South direction) measured in degrees from the center of the modelling surface in the 3D space, can be projected in any of the mentioned model, and even more, such as the cylindrical, conical and plane model.\nAdding the radius as third coordinate, we can model the altitude for each point. Unlike latitude and longitude, the location of the zero for the altitude point is model-dependent, and will be different if on the sphere, the ellipsoid or others. The functions mapping the latitude, longitude (and altitude) coordinates across models are called maps projections1.\nFor a fixed radius \\(R\\), and for \\(\\text{rad}: \\text{Deg} \\rightarrow \\text{Rad}\\) the function mapping degrees to radians, we can rename the two angles with the radians with the conventional greek letters \\(\\theta\\) (theta) and \\(\\varphi\\) (phi):\n\\[\n\\begin{align*}\n\\theta &:= \\text{rad}(\\text{Lon}) \\\\\n\\varphi &:= \\text{rad}(\\text{Lat}) \\\\\n\\end{align*}\n\\]\nAnother basic map for the spherical model is \\(\\Pi: \\mathbb{R}^2 \\rightarrow \\mathbb{S}^2 \\subset \\mathbb{R}^3\\) that projects each pair \\((\\theta, \\varphi)\\) for \\(\\theta \\in [-\\pi, \\pi]\\) and \\(\\varphi \\in [-\\pi/2, \\pi/2]\\) on the sphere of radius \\(R\\):\n\\[\n\\Pi(\\theta, \\varphi) = \\begin{cases}\n       x = R \\cos(\\varphi) \\cos(\\theta)\\\\\n       y = R \\cos(\\varphi) \\sin(\\theta)\\\\\n       z = R \\sin(\\varphi)\\\\\n     \\end{cases}\n\\]\nThe function \\(\\Pi\\) is one of the many map projections, and the only one we will consider in this blog post; the reasoning behind its formulation can be derived from the definition of sine and cosine and from the drawing below.\n\n\n\n\n\n\nFigure 1\n\n\n\nPlease note that most mathematical textbooks have the angle \\(\\varphi\\) at zero when the point in on the z-axis, so \\(\\sin\\) and \\(\\cos\\) are swapped, and the angle’s domain would be \\([0,\\pi]\\). In this blot post and to remain faithful to the definition of latitude, we consider \\(\\varphi\\) at zero when the point is on the xy-plane with domain \\([-\\pi,\\pi]\\)."
  },
  {
    "objectID": "posts/gds-2024-01-10-haversine-dist/index.html#sec-map-projection",
    "href": "posts/gds-2024-01-10-haversine-dist/index.html#sec-map-projection",
    "title": "Haversine’s distance mathematics",
    "section": "Map Projection on the Sphere Experiments",
    "text": "Map Projection on the Sphere Experiments\nTo check all is working correctly we will plot on the 3D sphere a sequence of equidistant points around the equator (in red), around the tropic of Cancer (in green), around the Arctic Polar circle (of course in blue), and then the cities of Rome, Paris and London (in black). To show a visual reference to make sense of the position of these three cities, we also plot the meridian zero (in gray).\n\nimport numpy as np\n\n\nR_Km = 6371 \n\ndef lon_lat_to_3d(lon, lat):\n    theta, phi = map(np.radians, [lon, lat])\n    x = R_Km * np.cos(phi) * np.cos(theta)\n    y = R_Km * np.cos(phi) * np.sin(theta)\n    z = R_Km * np.sin(phi)\n    return (x, y, z)\n    \n\n\nN = 40\n\n# Create N equidistant points around the equator\nlon_at_equator_deg = np.linspace(0, 360, N)[:-1]\nlat_at_equator_deg = 0 * np.ones_like(lon_at_equator_deg)\n\n# Create N equidistant points around the tropic of cancer\nlon_at_tropic_deg = np.linspace(0, 360, N)[:-1]\nlat_at_tropic_deg = 23.43617 * np.ones_like(lon_at_tropic_deg)\n\n# Create N equidistant points around the north pole\nlon_at_arctic_deg = np.linspace(0, 360, N)[:-1]\nlat_at_arctic_deg = 76.25 * np.ones_like(lon_at_arctic_deg)\n\n# Create N equidistant points at meridian 0\nlat_at_meridian_deg = np.linspace(-90, 90, N)[:-1]\nlon_at_meridian_deg = -90 * np.ones_like(lat_at_meridian_deg) \n\n# a selection of 3 cities\nlon_lat_london = ( 0.1276, 51.5072)\nlon_lat_paris  = ( 2.3522, 48.8566)\nlon_lat_rome   = (12.4964, 41.9028)\n\n\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm, colors\nimport numpy as np\n\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\n\n# Sphere surface\nlon, lat = np.mgrid[-90:90:100j, 0.0:360:100j]\nx, y , z = lon_lat_to_3d(lon, lat)\nax.plot_surface(x, y, z,  rstride=1, cstride=1, color=\"#303E4F\", alpha=0.2, linewidth=0)\n\n# Parallels\nxx_equator, yy_equator, zz_equator = lon_lat_to_3d(lon_at_equator_deg, lat_at_equator_deg)\nax.scatter(xx_equator, yy_equator, zz_equator, color=\"r\",s=20)\n\nxx_tropic, yy_tropic, zz_tropic = lon_lat_to_3d(lon_at_tropic_deg, lat_at_tropic_deg)\nax.scatter(xx_tropic , yy_tropic, zz_tropic, color=\"g\",s=20)\n\nxx_arctic, yy_arctic, zz_arctic = lon_lat_to_3d(lon_at_arctic_deg, lat_at_arctic_deg)\nax.scatter(xx_arctic, yy_arctic, zz_arctic, color=\"b\",s=20)\n\n# Meridian\nxx_meridian, yy_meridian, zz_meridian = lon_lat_to_3d(lon_at_meridian_deg, lat_at_meridian_deg)\nax.scatter(xx_meridian, yy_meridian, zz_meridian, color=\"gray\",s=20)\n\n# Cities\ncities = [lon_lat_london, lon_lat_paris, lon_lat_rome]\nlon_cities = [a[0] for a in cities]\nlat_cities = [a[1] for a in cities]\nxx_cities, yy_cities, zz_cities = lon_lat_to_3d(lon_cities, lat_cities)\nax.scatter(xx_cities, yy_cities, zz_cities, color=\"k\",s=20)\n\n# Plot\nax.set_aspect(\"equal\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFrom the plot above we can see that all works as expected, including the fact that the points at the tropic of cancer, even if they have the same longitude spacing as the point on the equator, have a significantly smaller mutual distance. They obviously come close to each other when approaching the north pole, becoming a single point at the pole itself.\nIf we want to keep the same “horizontal distance” or the distance on the tropic of Cancer as we have it at the equator, then we have to consider a smaller radius (that is the radius that varies in function of the latitude and that is given by \\(R\\cos(\\varphi)\\)) instead of the radius at the equator (that is \\(R\\)).\nThe drawing below may convince you about the need to multiply by the cosine of the latitude to have the sought result.\n\n\n\n\n\n\nFigure 2\n\n\n\nSo now we know how to adjust for the number of points to maintain their distance equal while modulating the latitude.\n\ndef num_points_at_latitude(num_points, latitude_deg):\n    return int(num_points * np.cos(np.radians(latitude_deg)))\n\n# Create N equidistant points around the equator\nlon_at_equator_deg = np.linspace(0, 360, N)[:-1]\nlat_at_equator_deg = 0 * np.ones_like(lon_at_equator_deg)\n\n# Create equidistant points around the tropic of cancer, as at the equator\nN_tropic = num_points_at_latitude(N, 23.43617)\nlon_at_tropic_deg = np.linspace(0, 360, N_tropic)[:-1]\nlat_at_tropic_deg = 23.43617 * np.ones_like(lon_at_tropic_deg)\n\n# Create equidistant points around the north pole, as at the equator\nN_arctic = num_points_at_latitude(N, 76.25)\nlon_at_arctic_deg = np.linspace(0, 360, N_arctic)[:-1]\nlat_at_arctic_deg = 76.25 * np.ones_like(lon_at_arctic_deg)\n\nprint(\"number of points\")\nprint(f\"at the arctic: {N_arctic}\")\nprint(f\"at the tropic: {N_tropic}\")\nprint(f\"at the equator: {N}\")\n\nnumber of points\nat the arctic: 9\nat the tropic: 36\nat the equator: 40\n\n\n\n# Repeated code from cells above\n\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\n\n# Sphere surface\nlon, lat = np.mgrid[-90:90:100j, 0.0:360:100j]\nx, y , z = lon_lat_to_3d(lon, lat)\nax.plot_surface(x, y, z,  rstride=1, cstride=1, color=\"#303E4F\", alpha=0.2, linewidth=0)\n\n# Parallels\nxx_equator, yy_equator, zz_equator = lon_lat_to_3d(lon_at_equator_deg, lat_at_equator_deg)\nax.scatter(xx_equator, yy_equator, zz_equator, color=\"r\",s=20)\n\nxx_tropic, yy_tropic, zz_tropic = lon_lat_to_3d(lon_at_tropic_deg, lat_at_tropic_deg)\nax.scatter(xx_tropic , yy_tropic, zz_tropic, color=\"g\",s=20)\n\nxx_arctic, yy_arctic, zz_arctic = lon_lat_to_3d(lon_at_arctic_deg, lat_at_arctic_deg)\nax.scatter(xx_arctic, yy_arctic, zz_arctic, color=\"b\",s=20)\n\n# Plot\nax.set_aspect(\"equal\")\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/gds-2024-01-10-haversine-dist/index.html#sec-two-points-distance",
    "href": "posts/gds-2024-01-10-haversine-dist/index.html#sec-two-points-distance",
    "title": "Haversine’s distance mathematics",
    "section": "Two Points’ Distance",
    "text": "Two Points’ Distance\nAs said above, latitude and longitude coordinates are independent from the chosen model. The model we use to project the coordinates becomes relevant for computing the distance between two points.\nThe Haversine distance is the shortest distance between two points in longitude and latitude coordinates on a spherical model. Its unit of measurement is the same as the one passed as input for the radius.\nBefore introducing the Haversine distance between \\(A = (\\text{Lon}_{A}, \\text{Lat}_{A})\\) and \\(B = (\\text{Lon}_{B}, \\text{Lat}_{B})\\), we start with two related concepts that can be seen as its approximation:\n\nEuclidean distance between the corresponding points on the sphere: \\[\nd_{\\text{Eucl}}(A, B) = \\lVert \\Pi(\\text{rad}(B)) - \\Pi(\\text{rad}(A)) \\rVert = \\sqrt{(x_B - x_A)^2 + (y_B - y_A)^2 + (z_B - z_A)^2}\n\\] where \\(\\text{rad}(A) = (\\varphi_{A}, \\theta_{A})\\) are the angle expressed in radiants. The formual above simply leverages on the Pythagorean theorem in respect to the 3D coordinate axis, and represents the distance we would travel if we could dig a straight tunnel connecting \\(A\\) and \\(B\\). It always underestimate the distance on the sphere, in the worst case by a factor of \\(\\pi \\mathcal{R}\\) for \\(\\mathcal{R}\\) earth radius.\n\nThe Euclidean distance is not very useful if we want to know the distance on the sphere!\nCan we adjust for the curvature of the sphere getting inspired by Figure 4? For example we use the Pythagorean theorem on the surface of the sphere respect to the meridians and the parallels, and adjust for the fact that the latitude distance shrinks when moving away from the equator.\n\n\n\n\n\n\nFigure 3\n\n\n\n\nDistance of two points along the same meridian: introducing the point \\(P = (\\text{Lon}_{B}, \\text{Lat}_{A})\\) that makes \\(\\bigtriangleup APB\\) a right triangle on the surface of the sphere, the arch \\(\\hat{BP}\\) runs along a meridian, and its length, converted to \\(\\text{Km}\\) for the radius \\(\\mathcal{R} = 6371~\\text{Km}\\) is given by: \\[\n\\hat{BP} = \\frac{2\\pi \\mathcal{R}}{360^{o}}\\left\\vert \\text{Lat}_{B} - \\text{Lat}_{P}\\right\\vert\n= \\frac{2\\pi \\mathcal{R}}{360^{o}}\\vert \\text{Lat}_{B} - \\text{Lat}_{A}\\vert\n\\]\nDistance of two points along the same parallel: as we saw, the distance along the arch \\(\\hat{AP}\\) is reduced by the cosine of the latitude, hence: \\[\n\\hat{AP} = \\frac{2\\pi \\mathcal{R} \\cos(\\text{rad}(\\text{Lat}_{A}))}{360^{o}}\\left\\vert \\text{Lon}_{A} - \\text{Lon}_{P}\\right\\vert\n= \\frac{2\\pi \\mathcal{R} \\cos(\\text{rad}(\\text{Lat}_{A}))}{360^{o}}\\vert \\text{Lon}_{A} - \\text{Lon}_{B}\\vert\n\\]\n\nAnd so we can use again the Pythagorean theorem to compute \\(\\hat{AB}\\) as an Euclidean-spherical distance:\n\\[\nd_{\\text{Sph}}(A, B) = \\sqrt{\\hat{AP}^ 2 + \\hat{BP}^2}\n= \\frac{2\\pi \\mathcal{R}}{360^{o}} \\sqrt{ \\cos^2(\\text{rad}(\\text{Lat}_{A}))(\\text{Lon}_{A} - \\text{Lon}_{B})^2 + (\\text{Lat}_{A} - \\text{Lat}_{B})^2  }\n\\]\nThis too is an approximation. Even if the lenghts of the sides of the triangle are exact, the curvature of the trajectory \\(\\hat{AB}\\) itself is not taken into account."
  },
  {
    "objectID": "posts/gds-2024-01-10-haversine-dist/index.html#sec-haversine-distance",
    "href": "posts/gds-2024-01-10-haversine-dist/index.html#sec-haversine-distance",
    "title": "Haversine’s distance mathematics",
    "section": "Haversine Distance",
    "text": "Haversine Distance\nThe examples proposed above are useful to develop some geometrical intuition about the distances on the sphere. The Haversine distance, provides us with a closed form solution to compute the length of the geodesic curve on the sphere, which is the shortest path on the surface of the sphere between the point \\(A\\) and \\(B\\).\nThanks to the already mentioned symmetry, the geodesics is the arch defined by the intersection between the sphere and the plane passing through \\(A\\), \\(B\\) and the centre of the sphere. This plane always cuts the sphere in half, and again by symmetry, and rotating the sphere so that \\(A\\) and \\(B\\) are on the equator, it is easy to see why a curve made in this way is the shortest one (or why any other curve would be longer).\n\n\n\n\n\n\nFigure 4\n\n\n\nLet \\(d\\) be the sought geodesic distance between \\(A\\) and \\(B\\), and \\(\\lambda\\) the angle at the center of the sphere in radians. By definition of radians we have:\n\\[\n\\frac{d}{\\mathcal{R}} = \\lambda\n\\] \\[\n\\frac{d}{\\mathcal{R}} = 2 \\frac{\\lambda}{2}\n\\] \\[\n\\frac{d}{\\mathcal{R}} = 2 \\arcsin \\left(\\sqrt{\\sin^2(\\lambda / 2 )}\\right)\n\\] \\[\nd = 2 \\mathcal{R} \\arcsin \\left(\\sqrt{\\sin^2(\\lambda / 2 )}\\right)\n\\] \\[\nd = 2 \\mathcal{R} \\arcsin \\left(\\sqrt{ \\text{hav}(\\lambda) }\\right)\n\\tag{1}\\] Where \\(h\\) is the Haversine function given by: \\[\n\\text{hav}(\\lambda) := \\sin^2\\left(\\frac{\\lambda}{2}\\right) = \\frac{1 - \\cos(\\lambda)}{2}~.\n\\tag{2}\\]\nThe haversine function is particularly handy when we want to avoid computing a \\(\\cos\\), and to transform it into a \\(\\sin\\) function. Since the \\(\\cos\\) for small angle is numerically unstable, given a \\(\\lambda\\) we can chose if we want to compute \\(\\text{hav}{\\lambda}\\) via the \\(\\sin\\) or the \\(\\cos\\) according to the numerical stability we want to obtain.\nFrom Equation 1 with the definition of the haversine function and some derivations detailed afterwards, we can get: \\[\nd = 2 \\mathcal{R} \\arcsin\n\\left(\n    \\left[  \n        \\text{hav}(\\varphi_B - \\varphi_A) + \\cos\\varphi_A \\cos\\varphi_B ~\\text{hav}(\\theta_B - \\theta_A)\n    \\right]^{1/2}\n\\right) \\\\\n\\tag{3}\\] And equivalently \\[\nd = 2 \\mathcal{R} \\arcsin\n\\left(\n    \\left[  \n        \\text{hav}(\\varphi_B - \\varphi_A) + (1 - \\text{hav}(\\varphi_B - \\varphi_A) - \\text{hav}(\\varphi_B + \\varphi_A) ) \\text{hav}(\\theta_B - \\theta_A)\n    \\right]^{1/2}\n\\right)\n\\tag{4}\\]\nWhere the last two equations are a consequence of the following ones, that are still to be proven: \\[\n\\text{hav}(\\lambda) =\n        \\text{hav}(\\varphi_B - \\varphi_A) + \\cos\\varphi_A \\cos\\varphi_B ~\\text{hav}(\\theta_B - \\theta_A)\n\\tag{5}\\] and \\[\n\\text{hav}(\\lambda) =\n        \\text{hav}(\\varphi_B - \\varphi_A) + (1 - \\text{hav}(\\varphi_B - \\varphi_A) - \\text{hav}(\\varphi_B + \\varphi_A) ) \\text{hav}(\\theta_B - \\theta_A)\n\\tag{6}\\]\n\nProof of Equation 5\nHypothesis: \\(A\\) and \\(B\\) points on the surface of a sphere, with coordinates\n\\(A=(\\varphi_A, \\theta_A) = (\\cos\\theta_A~\\cos\\varphi_A, \\sin\\theta_A~\\cos\\varphi_A, \\sin\\varphi_A )\\) and\n\\(B=(\\varphi_B, \\theta_B) = (\\cos\\theta_B~\\cos\\varphi_B, \\sin\\theta_B~\\cos\\varphi_B, \\sin\\varphi_B )\\)\nand \\(\\lambda\\) angle at the centre of the sphere.\nThesis: \\(\\text{hav}(\\lambda) = \\text{hav}(\\varphi_B - \\varphi_A) + \\cos\\varphi_A \\cos\\varphi_B ~\\text{hav}(\\theta_B - \\theta_A)\\).\nIt is always possible to rotate the axis so to have \\(\\theta_A = 0\\), and \\(\\tilde{\\theta}_B = \\theta_B - \\theta_A\\). In the new coordinate system we have: \\[\n\\begin{align*}\nA &= (\\cos\\varphi_A, 0, \\sin\\varphi_A ) \\\\\nB &=(\\cos\\tilde{\\theta}_B~\\cos\\varphi_B, \\sin\\tilde{\\theta}_B~\\cos\\varphi_B, \\sin\\varphi_B )\n\\end{align*}\n\\]\nNow, how do we connect the angle \\(\\lambda\\) with \\(A\\) and \\(B\\)?\nWith the scalar product, of course!\nThen we can add and subtract a ghost term and apply the addition subtraction formulae: \\[\n\\begin{align*}\n\\cos\\lambda &= A \\cdot B \\\\\n&= \\cos\\varphi_A~\\cos\\tilde{\\theta}_B~\\cos\\varphi_B + \\sin\\varphi_A~\\sin\\varphi_B \\\\\n&= \\cos\\varphi_A~\\cos\\tilde{\\theta}_B~\\cos\\varphi_B + \\sin\\varphi_A~\\sin\\varphi_B + \\cos\\varphi_A~\\cos\\varphi_B -  \\cos\\varphi_A~\\cos\\varphi_B \\\\\n&= \\cos\\varphi_A~\\cos\\varphi_B(\\cos\\tilde{\\theta}_B - 1) + \\cos(\\varphi_A - \\varphi_B)\n\\end{align*}\n\\] With simple algebraic manipulations the previous equation becomes: \\[\n\\begin{align*}\n\\frac{1 - \\cos\\lambda}{2}\n= \\frac{1 - \\cos(\\varphi_A - \\varphi_B)}{2} - \\frac{1}{2}  \\cos\\varphi_A~\\cos\\varphi_B(\\cos\\tilde{\\theta}_B - 1)\n\\end{align*}\n\\] and considering that for any angle \\(\\alpha\\) we have \\(\\cos(\\alpha) = \\cos(-\\alpha)\\), the previous equation is equivalent to the sought thesis: \\[\n\\text{hav}(\\lambda) = \\text{hav}(\\varphi_B - \\varphi_A) + \\cos\\varphi_A \\cos\\varphi_B ~\\text{hav}(\\theta_B - \\theta_A)\n\\]\n\n\nProof of Equation 6\nUnder the hypothesis of the previous proof, the thesis reduces to \\(\\cos\\varphi_A \\cos\\varphi_B = 1 - \\text{hav}(\\varphi_B - \\varphi_A) - \\text{hav}(\\varphi_B + \\varphi_A)\\).\nThe proof starts and end as a direct consequence of the addition subtraction formulae: \\[\n\\begin{align*}\n\\cos\\varphi_A \\cos\\varphi_B\n&= \\frac{1}{2}\\left(  \\cos(\\varphi_B - \\varphi_A) + \\cos(\\varphi_B + \\varphi_A) \\right) \\\\\n&= 1 - \\frac{1}{2} - \\frac{1}{2} + \\frac{\\cos(\\varphi_B - \\varphi_A)}{2} + \\frac{\\cos(\\varphi_B + \\varphi_A)}{2} \\\\\n&= 1  - \\frac{ 1 - \\cos(\\varphi_B - \\varphi_A)}{2} - \\frac{1 - \\cos(\\varphi_B + \\varphi_A)}{2} \\\\\n&= 1  - \\text{hav}(\\varphi_B - \\varphi_A) - \\text{hav}(\\varphi_B + \\varphi_A) \\\\\n\\end{align*}\n\\] which concludes the proof of the Haversine formula."
  },
  {
    "objectID": "posts/gds-2024-01-10-haversine-dist/index.html#sec-implementation",
    "href": "posts/gds-2024-01-10-haversine-dist/index.html#sec-implementation",
    "title": "Haversine’s distance mathematics",
    "section": "Implementation and examples",
    "text": "Implementation and examples\nFrom a computational point of view, as sadi before, we want to avoid computing \\(\\sin\\) when the angles are very small. So a stable implementation would use the last part of Equation 2.\n\n\"\"\" Haversine function in python\"\"\"\n\ndef hav_function(theta_A, phi_A, theta_B, phi_B):\n    # haversine computed with the sin\n    hav_rad = lambda  x: np.sin(x/2) ** 2\n    return hav_rad(phi_B - phi_A) + (1 - hav_rad(phi_B - phi_A) - hav_rad(phi_B + phi_A) ) * hav_rad(theta_B - theta_A)\n\ndef haversine_distance(lon_1, lon_2, lat_1, lat_2):\n    theta_1, phi_1, theta_2, phi_2 = map(np.radians, [lon_1, lon_2, lat_1, lat_2])\n    return 2 * R_Km * np.arcsin(np.sqrt(hav_function(theta_1, phi_1, theta_2, phi_2)))\n\n\n# Verify that it looks correct:\nPREC = 6\nprint(f\"Distance London to London = {round(haversine_distance(*lon_lat_london, *lon_lat_london), PREC)} Km\")\nprint(f\"Distance London to Paris  = {round(haversine_distance(*lon_lat_london, *lon_lat_paris), PREC)} Km\")\nprint(f\"Distance Paris to London  = {round(haversine_distance(*lon_lat_paris, *lon_lat_london), PREC)} Km\")\n\nDistance London to London = 0.0 Km\nDistance London to Paris  = 334.563443 Km\nDistance Paris to London  = 334.563443 Km\n\n\nIt seems to be working correctly. Let’s compare it with an out of the box function from the library sklearn (see also this stack overflow discussion and this one)\n\nfrom sklearn.metrics.pairwise import haversine_distances\n\nlon_A, lat_A = lon_lat_london\nlon_B, lat_B = lon_lat_paris\nvals = [[np.radians(lat_A), np.radians(lon_A)], [np.radians(lat_B), np.radians(lon_B)]]\ndistance_sklearn = R_Km * haversine_distances(vals)\n\nprint(f\"Distance London to Paris for sklearn  = {round(distance_sklearn[1,0], PREC)} Km\")\n\nDistance London to Paris for sklearn  = 334.563443 Km"
  },
  {
    "objectID": "posts/gds-2024-01-10-haversine-dist/index.html#sec-numerical-stability",
    "href": "posts/gds-2024-01-10-haversine-dist/index.html#sec-numerical-stability",
    "title": "Haversine’s distance mathematics",
    "section": "Numerical stability",
    "text": "Numerical stability\nLet’s see what happens in term of numerical stability. We take now two points in london very close to each others:\n\nlon_lat_london_1 = (0.1276, 51.5072)\nlon_lat_london_2 = (0.1276, 51.50720001)\nPREC = 20\n\nprint(f\"Very small distance (hav with cosine) = {round(haversine_distance(*lon_lat_london_1, *lon_lat_london_2), PREC)} Km\")\n\nVery small distance (hav with cosine) = 1.11194938401393e-06 Km\n\n\nWe can see that our implementation with the haversine function, computed with the sin version, does not collapse to zero.\nWhat happens if we re-implement with function with the cosine?\n\n# repeated code with a small change!\n\ndef hav_function(theta_A, phi_A, theta_B, phi_B):\n    # Haversine computed with the cosine - Numerically unstable!\n    hav_rad = lambda  x: (1-np.cos(x))/2\n    return hav_rad(phi_B - phi_A) + (1 - hav_rad(phi_B - phi_A) - hav_rad(phi_B + phi_A) ) * hav_rad(theta_B - theta_A)\n\ndef haversine_distance(lon_1, lon_2, lat_1, lat_2):\n    theta_1, phi_1, theta_2, phi_2 = map(np.radians, [lon_1, lon_2, lat_1, lat_2])\n    return 2 * R_Km * np.arcsin(np.sqrt(hav_function(theta_1, phi_1, theta_2, phi_2)))\n\n\nprint(f\"Distance London to Paris (hav with sine) = {round(haversine_distance(*lon_lat_london, *lon_lat_paris), PREC)} Km\")\nprint(f\"Very small distance (hav with sine)      = {round(haversine_distance(*lon_lat_london_1, *lon_lat_london_2), PREC)} Km\")\n\nDistance London to Paris (hav with sine) = 334.5634431209046 Km\nVery small distance (hav with sine)      = 0.0 Km\n\n\nThe function works well for large angles, though for small angle the distance collapses.\nHad the developers of sklearn use the sin or the cos?\n\nlon_A, lat_A = lon_lat_london_1\nlon_B, lat_B = lon_lat_london_2\nvals = [[np.radians(lat_A), np.radians(lon_A)], [np.radians(lat_B), np.radians(lon_B)]]\ndistance_sklearn = R_Km * haversine_distances(vals)\n\nprint(f\"Distance London to Paris for sklearn  = {round(distance_sklearn[1,0], PREC)} Km\")\n\nDistance London to Paris for sklearn  = 1.11194938401393e-06 Km\n\n\nWe can see that (at least for this case) our results and the sklearn implementation is the same!"
  },
  {
    "objectID": "posts/gds-2024-01-10-haversine-dist/index.html#sec-vincentry-formula",
    "href": "posts/gds-2024-01-10-haversine-dist/index.html#sec-vincentry-formula",
    "title": "Haversine’s distance mathematics",
    "section": "See also: Vincety’s formula",
    "text": "See also: Vincety’s formula\nTo compute the distance between two points on an ellipsoid model (which is more accurate than the spherical model on which the Haversine is based on) there is the Vincety formula.\nIts mathematical derivation may be the topic for another post, though if you want to have it computed with a tool out of the box, we recommend the library geopy.\n\nfrom geopy import distance\n\n\ndist_haversine = haversine_distance(*lon_lat_london, *lon_lat_paris)\ndist_vincenty = distance.distance(lon_lat_london[::-1], lon_lat_paris[::-1]).km\n\nPREC = 4\n\nprint(f\"Distance with haversine =  {dist_haversine} Km\")\nprint(f\"Distance with Vincenty =  {dist_vincenty} Km\")\nprint(f\"Relative error = |ground - calculated|/|ground| = {round(100 * abs(dist_vincenty - dist_haversine) / dist_vincenty, PREC)} %\")\n\nDistance with haversine =  334.5634431209046 Km\nDistance with Vincenty =  334.88388938435094 Km\nRelative error = |ground - calculated|/|ground| = 0.0957 %"
  },
  {
    "objectID": "posts/bp-2022-07-23-gpg/index.html#footnotes",
    "href": "posts/bp-2022-07-23-gpg/index.html#footnotes",
    "title": "How to sign your commits",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote that gpg stands for Gnu Privacy Guard and implements the OpenPGP standard as defined by RFC4880. It is not the PGP protocol.↩︎"
  },
  {
    "objectID": "posts/gds-2022-07-29-office-pos/index.html#footnotes",
    "href": "posts/gds-2022-07-29-office-pos/index.html#footnotes",
    "title": "How well positioned is your office?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDetails about how to prove and compute the Haversine function can be found in the blog post https://geospatial.netlify.app/posts/gds-2024-01-10-haversine-dist/.↩︎"
  },
  {
    "objectID": "posts/bp-2022-07-23-gpg/index.html#appendix-3-trigger-gpg-passphrase-on-linux",
    "href": "posts/bp-2022-07-23-gpg/index.html#appendix-3-trigger-gpg-passphrase-on-linux",
    "title": "How to sign your commits",
    "section": "Appendix 3: Trigger gpg passphrase on linux",
    "text": "Appendix 3: Trigger gpg passphrase on linux\nOn some linux distributions, the prompt to insert the gpg passphrase does not pop up when you create a new commit. Git simply refuses to add a new non-signed commit.\nSo you will be in the situation where you have some code to commit, you know your passphrase, you have the gpg-agent on so you not need to retype the passphrase each time, but nobody is asking you for your passphrase when you are committing a message.\nA workaround is to trigger gpg to ask you for your passphrase for another reason (e.g. signing a file), after which your passphrase is stored in the gpg-agent and you will be free to create signed commits, as the passphrase is automatically retrieved from cache. The list of commands would be:\ngit commit -am \"new stuff\"  # this commit is not added and does not trigger the passphrase prompt\ncd \ntouch z_tmp.txt  # creating a dummy file to authenticate\ngpg -s z_tmp.txt  # this trigger the passphrase (and a y/n question to confirm your choice)\ncd &lt;repo you were working&gt;\ngit commit -am \"new stuff\"\nTo turn this workaround into a oneliner, you can create the dummy file z_tmp.txt in the root directory (as above), and then add the following line to your bash profile:\nalias gpg_trigger='gpg -s ~/z_tmp.txt'\nSo the next time, instead of running all the commands of the previous block, you can simply call the newly created alias gpg_trigger."
  },
  {
    "objectID": "posts/gds-2024-01-24-timestamp/index.html#introduction",
    "href": "posts/gds-2024-01-24-timestamp/index.html#introduction",
    "title": "Python datetimes manipulations",
    "section": "Introduction",
    "text": "Introduction\nWhen first started working with time series, a recurrent task was for me to google up ways to convert datetimes across python and pandas data types. It took me some times to get to have a grasp about the possibilities and conversion methods, as well as how to set and change timezones without making mistakes.\nThis blog post is for you to get the same information I got browsing for a while around various stackoverflows and documentation pages, all in one place.\nAs it is for manipulations of polygons, handling datetimes is another recurring task in geospatial data science.\nIn python there are multiple ways to encode a point in time according to a range of criteria:\n\nReadability\nStoring space\nOut of the box methods\n\nBefore delving into some examples, let’s warm up with a code snippet showing what can go wrong if dates and times are manipulated carelessly.\n\nimport datetime as dt\n\nFORMAT = \"%H:%M:%S%z %Y-%m-%d\"\n\nevent_1 = dt.datetime.strptime(\"22:15:00+00:00 2022-05-01\", FORMAT)\nevent_2 = dt.datetime.strptime(\"18:30:00+00:00 2022-06-01\", FORMAT)\n\nprint(f\"Events as type {type(event_1)}\")\nprint(f\"Event 1: {event_1}\")\nprint(f\"Event 2: {event_2}\")\n\nif event_1 &lt; event_2: \n    print(f\"\\nDatetime {event_1} comes before {event_2}\")\n\n\nevent_1 = event_1.strftime(FORMAT)\nevent_2 = event_2.strftime(FORMAT)\n\nprint(f\"\\nEvents as type {type(event_1)}\")\nprint(f\"Event 1: {event_1}\")\nprint(f\"Event 2: {event_2}\")\n\n\nif event_1 &gt; event_2: \n    print(f\"\\nDatetime {event_1} comes after {event_2}.\")\n    print(\"Oh, Really?\")\n\nEvents as type &lt;class 'datetime.datetime'&gt;\nEvent 1: 2022-05-01 22:15:00+00:00\nEvent 2: 2022-06-01 18:30:00+00:00\n\nDatetime 2022-05-01 22:15:00+00:00 comes before 2022-06-01 18:30:00+00:00\n\nEvents as type &lt;class 'str'&gt;\nEvent 1: 22:15:00+0000 2022-05-01\nEvent 2: 18:30:00+0000 2022-06-01\n\nDatetime 22:15:00+0000 2022-05-01 comes after 18:30:00+0000 2022-06-01.\nOh, Really?\n\n\nCan you see what went wrong?\nIn this example we started from two datetimes (any representation of a date and a time) in string format. These were converted to instances of datetime.datetime from the standard library datetime.\nThe first odd thing, that becomes evident when calling type(dt.datetime.strptime(\"22:15:00+00:00 2022-05-01\", FORMAT)) is that datetime.datetime is not a class. It is a type, like int and str, so it does not follow the camel case convention1.\nAlso that the type name and the library name are the same. So, in some modules you will find from datetime import datetime and in other simply import datetime, a confusion that I avoid usually aliasing the library datetime to dt, as done in the previous example.\nFrom the diagram shown, we can see another source of possible confusions: in the python world the term timestamp refers to a datetime object converted to an integer — more details in the section later about how this is done. In the pandas world instead, the term timestamp is used as the class that encodes the pandas datetime object. So there is a change in the naming convention if you are using the python standards libraries, or if you are using pandas.\nStarting from the center of the diagram, in each section of this post we will explore a conversion and the specifications of the given types, leaving pandas.Timestamp for last."
  },
  {
    "objectID": "posts/gds-2024-01-24-timestamp/index.html#many-datetimes",
    "href": "posts/gds-2024-01-24-timestamp/index.html#many-datetimes",
    "title": "Datetimes manipulations",
    "section": "Many datetimes",
    "text": "Many datetimes\n\nDraft below:"
  },
  {
    "objectID": "posts/gds-2024-01-24-timestamp/index.html#datetime.datetime",
    "href": "posts/gds-2024-01-24-timestamp/index.html#datetime.datetime",
    "title": "Datetimes manipulations",
    "section": "datetime.datetime",
    "text": "datetime.datetime\nThe fact that both the library and the object have the same name could cause confusion, in particular because different module of the same project can import only the datetime library with the idiomatic import datetime or if only datetime.datetime is called, it may be imported as from datetime import datetime. To avoid the confusion I (and pandas) have the preference for aliasing the library name with dt, as in import datetime as dt. In this post, nonetheless, at the cost of typing more letters, we keep all the imports explicit with import datetime."
  },
  {
    "objectID": "posts/gds-2024-01-24-timestamp/index.html#datetime.datetime-to-and-from-integerfloat-timestamp",
    "href": "posts/gds-2024-01-24-timestamp/index.html#datetime.datetime-to-and-from-integerfloat-timestamp",
    "title": "Datetimes manipulations",
    "section": "datetime.datetime to and from integer/float timestamp",
    "text": "datetime.datetime to and from integer/float timestamp\nA timestamp is the name for an integer representing"
  },
  {
    "objectID": "posts/gds-2024-01-24-timestamp/index.html#datetime.datetime-to-and-from-string",
    "href": "posts/gds-2024-01-24-timestamp/index.html#datetime.datetime-to-and-from-string",
    "title": "Datetimes manipulations",
    "section": "datetime.datetime to and from string",
    "text": "datetime.datetime to and from string"
  },
  {
    "objectID": "posts/gds-2024-01-24-timestamp/index.html#datetime.datetime-to-and-from-pandas.timestamp",
    "href": "posts/gds-2024-01-24-timestamp/index.html#datetime.datetime-to-and-from-pandas.timestamp",
    "title": "Datetimes manipulations",
    "section": "datetime.datetime to and from pandas.timestamp",
    "text": "datetime.datetime to and from pandas.timestamp\nBy default and by the standard ISO8601, if no timezone is specified, then the datetime is in UTC (+00, also called Zulu or Paris timezone). UTC is where all the conversions and storage of data should happen, unless there is a very specific need to do otherwise."
  },
  {
    "objectID": "posts/gds-2024-01-24-timestamp/index.html#datetime.datetime-to-str-and-back",
    "href": "posts/gds-2024-01-24-timestamp/index.html#datetime.datetime-to-str-and-back",
    "title": "Python datetimes manipulations",
    "section": "datetime.datetime to str and back",
    "text": "datetime.datetime to str and back\nThere are many formats to represent a datetime into a string. One of the standard is specified in ISO8601.\n\n# create a datetime - you need at least year, month and day.\ndt_event = dt.datetime(year=2024, month=1, day=22)\ndt_event\n\ndatetime.datetime(2024, 1, 22, 0, 0)\n\n\n\n# create a datetime with hour, minutes and seconds\ndt_event = dt.datetime(year=2024, month=1, day=22, hour=13, minute=55, second=12)\ndt_event\n\ndatetime.datetime(2024, 1, 22, 13, 55, 12)\n\n\n\n# convert to string with a range of formats\nprint(dt_event.strftime(\"%m/%d/%Y, %H:%M:%S\"))\nprint(dt_event.strftime(\"%Y-%m-%d, %H:%M\"))\nprint(dt_event.strftime(\"%H:%M\"))\n\n01/22/2024, 13:55:12\n2024-01-22, 13:55\n13:55\n\n\nOnce we have the string we can move back to the timestamp object, passing the same format code (this operation is not bijective if some of the information is not included in the chosen format code).\nMore info about the format codes and what each letter means is available at the official documentation https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior.\n\n# Create a datetime object\ndt_event = dt.datetime(year=2024, month=1, day=22, hour=13, minute=55, second=12, microsecond=120)\nprint(f\"Datetime: {dt_event}\")\n\n# convert to string\nFORMAT_CODE = \"%m/%d/%Y, %H:%M:%S\"\nstr_event = dt_event.strftime(FORMAT_CODE)\nprint(f\"String:   {dt_event}\")\n\n# string to datetime\nprint(f\"Datetime: {dt.datetime.strptime(str_event, FORMAT_CODE)}\")\n\nDatetime: 2024-01-22 13:55:12.000120\nString:   2024-01-22 13:55:12.000120\nDatetime: 2024-01-22 13:55:12\n\n\nIn addition to timestamp.timestamp you can create only the time, the date and a timedelta.\n\nprint(dt.time(hour=4))\nprint(dt.date(year=2024, month=3, day=11))\nprint(dt.timedelta(hours=3))  \n\n# we can not sum a timedelta to a time\ntry:\n    print(dt.time(hour=4) + dt.time(hour=4))\nexcept TypeError as err:\n    print(f\"Error raised: TypeError {err}\")\n\n# nor to a date\ntry:\n    print(dt.date(year=2024, month=3, day=11) + dt.time(hour=4))\nexcept TypeError as err:\n    print(f\"Error raised: TypeError {err}\")\n\n# but we can sum it to a timestamp\nprint(dt.datetime(year=2024, month=3, day=11, hour=3) + dt.timedelta(hours=4))\n\n04:00:00\n2024-03-11\n3:00:00\nError raised: TypeError unsupported operand type(s) for +: 'datetime.time' and 'datetime.time'\nError raised: TypeError unsupported operand type(s) for +: 'datetime.date' and 'datetime.time'\n2024-03-11 07:00:00\n\n\n\n# if we quickly want to have the timestamp converted to a string in isoformat we can use:\ndt_event.isoformat()\n\n'2024-01-22T13:55:12.000120'"
  },
  {
    "objectID": "posts/gds-2024-01-24-timestamp/index.html#datetime.datetime-to-int-and-back",
    "href": "posts/gds-2024-01-24-timestamp/index.html#datetime.datetime-to-int-and-back",
    "title": "Python datetimes manipulations",
    "section": "datetime.datetime to int and back",
    "text": "datetime.datetime to int and back\nThe integer representation of a datetime is called timestamp in the python world. We note again that it is not the same as the the pandas Timestamp object.\nA timestamp is a numerical representation of a date in the Unix epoch (or Unix time or POSIX time or Unix timestamp). It is the number of seconds that have elapsed since January 1, 1970 (midnight UTC/GMT), not counting leap seconds (using the ISO 8601 convention it is 1970-01-01T00:00:00Z).\n\ndt_event = dt.datetime(year=2024, month=3, day=11, hour=3)\ndt_event.timestamp?\n\nDocstring: Return POSIX timestamp as float.\nType:      builtin_function_or_method\n\n\nOut of the box, the python timestamp is in seconds, and encoded as a float. In some dataset, and to save memory, it can be encoded in milliseconds or even in microseconds, and encoded as an integer.\n\nprint(f\"Datetime:         {dt_event}\")\nprint(f\"Python timestamp: {dt_event.timestamp()}\")\n\nDatetime:         2024-03-11 03:00:00\nPython timestamp: 1710126000.0\n\n\n\nISO_FORMAT_CODE = \"%Y-%m-%dT%H:%M:%S%z\"\ndt_event_zero = dt.datetime.strptime(\"1970-01-01T00:00:00Z\", ISO_FORMAT_CODE)\nprint(f\"Datetime:         {dt_event_zero}\")\nprint(f\"Python timestamp: {dt_event_zero.timestamp()}\")\n\nDatetime:         1970-01-01 00:00:00+00:00\nPython timestamp: 0.0\n\n\n\ndt_event = dt.datetime(year=2024, month=3, day=11, hour=3, minute=13, second=45)\nsec_event = int(dt_event.timestamp())\nmsec_event = int(dt_event.timestamp() * 1000)  # convert to milliseconds before casting\nback_to_dt_event = dt.datetime.fromtimestamp(msec_event / 1000)\n\nprint(f\"Datetime:              {dt_event}\")\nprint(f\"Python timestamp (s):  {sec_event}\")\nprint(f\"Python timestamp (ms): {msec_event}\")\nprint(f\"Back to datetime:      {back_to_dt_event}\")\n\nDatetime:              2024-03-11 03:13:45\nPython timestamp (s):  1710126825\nPython timestamp (ms): 1710126825000\nBack to datetime:      2024-03-11 03:13:45"
  },
  {
    "objectID": "posts/gds-2024-01-24-timestamp/index.html#datetime.datetime-and-other-types-to-pandas.timestamp",
    "href": "posts/gds-2024-01-24-timestamp/index.html#datetime.datetime-and-other-types-to-pandas.timestamp",
    "title": "Datetimes manipulations",
    "section": "datetime.datetime and other types to pandas.Timestamp",
    "text": "datetime.datetime and other types to pandas.Timestamp"
  },
  {
    "objectID": "posts/gds-2024-01-24-timestamp/index.html#timezones",
    "href": "posts/gds-2024-01-24-timestamp/index.html#timezones",
    "title": "Python datetimes manipulations",
    "section": "Timezones",
    "text": "Timezones\nSome basic concepts first:\n\nUTC universal time coordinates: is the standard to assign to each country in the world a timezone, that is an offset in respect to the timezone zero, also called Zulu or GTM.\nGMT Greenwich Mean Time: is the time zone at the meridian zero, passing at Greenwich in England. The UTC standard tells you how many time zone you are away from Greenwich.\nZulu for the NATO alphabet letter Z, was originally indicating the letter z as in “zero meridian”. This is an equivalent of GTM and it is commonly used in the army and in the civilian aviation. The final “zulu” in a radio message refers to the fact that the time is not read in local time. For example 02:00am at GMT, would read “zero two hundred zulu” in a radio communication.\n\nThe names of the timezones are typically the UTC offsets, written as suffix to the timestamps, such as +02:00 or -05:00, though some particularly recurring timezones have their names, such as:\n\nEastern Standard Time (EST): UTC-0500 or 5 hours behind GMT.\nCentral Standard Time (CST): UTC-0600 or 6 hours behind GMT.\nMountain Standard Time (MST): UTC-0700 or 7 hours behind GMT.\nPacific Standard Time (PST): UTC-0800 or 8 hours behind GMT.\n\n\nTimezones in timestamps\nWhen timezones are written as offsets and appended to the timestamps, the are also summed or subtracted to the datetime itself.\nFor example to write 2023-02-21T06:07:21+03:30 in GMT, we have to set the offset to zero and subtract the offset: 2023-02-21T02:37:21+00:00, that in the ISO 8601 becomes 2023-02-21T02:37:21Z, with in the Zulu timezone, and in a convention that is often found in public datasets as 2023-02-21 02:37:21 UTC, conflating the concepts of GMT and UTC.\nAnother way of encoding the timezones is with the name of a representative city where the timezone belongs to. +03:30 does not tell much about where we are, if compared to \"Asia/Tehran\".\n\n\nNo timezone given\nWhen a timezone is not specified, the timestamp is said to be naive, or not timezone-aware.\nIn this case it is important to check with the data provider if the datetime are given in local time or at GMT. If the datetime are given as integer UNIX timestamps, then they are at GMT, unless there was a mistake in the data ingestion.\n\nimport datetime as dt\nfrom dateutil import tz\n\n# naive\nstr_event = \"2023-02-21 02:37:21\"\n\ndt_event = dt.datetime.strptime(str_event, \"%Y-%m-%d %H:%M:%S\")\nprint(f\"naive datetime:  {dt_event.isoformat()}\")\n\n# GTM\nstr_event = \"2023-02-21 02:37:21 GMT\"  # works also with UTC instead of GMT\n\ndt_event = dt.datetime.strptime(str_event, \"%Y-%m-%d %H:%M:%S %Z\")\nprint(f\"GTM datetime:    {dt_event.isoformat()}\")\n\n# zero offset\nstr_event = \"2023-02-21 02:37:21+00:00\"\n\ndt_event = dt.datetime.strptime(str_event, \"%Y-%m-%d %H:%M:%S%z\")\nprint(f\"datetime +00:00: {dt_event.isoformat()}\")\n\n# +1h offset\nstr_event = \"2023-02-21 03:37:21+01:00\"\n\ndt_event = dt.datetime.strptime(str_event, \"%Y-%m-%d %H:%M:%S%z\")\nprint(f\"datetime +01:00: {dt_event.isoformat()}\")\n\nnaive datetime:  2023-02-21T02:37:21\nGTM datetime:    2023-02-21T02:37:21\ndatetime +00:00: 2023-02-21T02:37:21+00:00\ndatetime +01:00: 2023-02-21T03:37:21+01:00\n\n\nThe code above should be enough to convince you that when the offset is not specified, then we can assume the datetime is at GMT, as it was at GMT when we passed as a string.\n\n\nTimezones as capital cities\nBelow, with dateutil we can also pass the offset with the capital city based offset, such as “Asia/Tehran”.\n\nfrom dateutil import tz\n\n# country/city offset\nstr_event = \"2023-02-21 03:37:21\"\nstr_offset = \"Asia/Tehran\"\n\ndt_event = dt.datetime.strptime(str_event, \"%Y-%m-%d %H:%M:%S\")\nprint(f\"datetime: {dt_event.isoformat()}\")\ndt_event = dt_event.astimezone(tz.gettz(str_offset))\nprint(f\"datetime: {dt_event.isoformat()}\")\n\ndatetime: 2023-02-21T03:37:21\ndatetime: 2023-02-21T07:07:21+03:30\n\n\nIf you use this solution be aware that there is no validation, and if the string is misspelled or non existing, the datetime will be assigned the default +00:00 offset.\n\ndt_event = dt.datetime.strptime(str_event, \"%Y-%m-%d %H:%M:%S\")\nprint(f\"datetime: {dt_event.isoformat()}\")\n\ncorrect = \"Asia/Tehran\"\ndt_event = dt_event.astimezone(tz.gettz(correct))\nprint(f\"datetime in {correct}: {dt_event.isoformat()}\")\n\nmisspelled = \"Asia/Theran\"\ndt_event = dt_event.astimezone(tz.gettz(misspelled))\nprint(f\"datetime in {misspelled}: {dt_event.isoformat()}\")\n\nplain_wrong = \"MiddleEarth/County\"\ndt_event = dt_event.astimezone(tz.gettz(plain_wrong))\nprint(f\"datetime in {plain_wrong}: {dt_event.isoformat()}\")\n\ndatetime: 2023-02-21T03:37:21\ndatetime in Asia/Tehran: 2023-02-21T07:07:21+03:30\ndatetime in Asia/Theran: 2023-02-21T03:37:21+00:00\ndatetime in MiddleEarth/County: 2023-02-21T03:37:21+00:00\n\n\nNot raising an error for non existing capitals is not ideal. So rather than assigning the timezone dateutil I would recommend the method timezone from pytz.\n\nfrom pytz import timezone\n\ndt_event = dt.datetime.strptime(str_event, \"%Y-%m-%d %H:%M:%S\")\n\ncorrect = \"Asia/Tehran\"\ndt_event = dt_event.astimezone(tz.gettz(correct))\nprint(f\"datetime in {correct}: {dt_event.isoformat()}\")\n\nplain_wrong = \"MiddleEarth/County\"\ntry:\n    dt_event.astimezone(timezone(plain_wrong))\n    print(f\"datetime in {plain_wrong}: {dt_event.isoformat()}\")\nexcept Exception as err:\n    print(f\"Error raised: {type(err)} {err}\")\n\ndatetime in Asia/Tehran: 2023-02-21T07:07:21+03:30\nError raised: &lt;class 'pytz.exceptions.UnknownTimeZoneError'&gt; 'MiddleEarth/County'\n\n\n\n\nTimezones in pandas\nAlso to the pd.Timestamp object can be assigned a timezone with tz_localize and can be converted from one timezone to another with tz_convert (this last works only with the non-naive timestamps).\n\nindex_events = pd.date_range(start='3/1/2022', end='3/10/2022')\n\n\ntry:\n    index_events.tz_convert('US/Pacific')\nexcept TypeError as err:\n    print(f\"We can not use tz_convert directly. Error raised: {err}\")\n\nWe can not use tz_convert directly. Error raised: Cannot convert tz-naive timestamps, use tz_localize to localize\n\n\n\nindex_events_localised = index_events.tz_localize('US/Pacific')\nprint_events(index_events_localised)  # note that the datetimes are not shifted! They are considered in local time at the specified location\n\n10 days: \n[\n    2022-03-01 00:00:00-08:00\n    2022-03-02 00:00:00-08:00\n    2022-03-03 00:00:00-08:00\n    2022-03-04 00:00:00-08:00\n    2022-03-05 00:00:00-08:00\n    2022-03-06 00:00:00-08:00\n    2022-03-07 00:00:00-08:00\n    2022-03-08 00:00:00-08:00\n    2022-03-09 00:00:00-08:00\n    2022-03-10 00:00:00-08:00\n]\nThis is a &lt;class 'pandas.core.indexes.datetimes.DatetimeIndex'&gt; of &lt;class 'pandas._libs.tslibs.timestamps.Timestamp'&gt; elements.\n\n\n\nprint_events(index_events_localised.tz_convert('GMT'))\n\n10 days: \n[\n    2022-03-01 08:00:00+00:00\n    2022-03-02 08:00:00+00:00\n    2022-03-03 08:00:00+00:00\n    2022-03-04 08:00:00+00:00\n    2022-03-05 08:00:00+00:00\n    2022-03-06 08:00:00+00:00\n    2022-03-07 08:00:00+00:00\n    2022-03-08 08:00:00+00:00\n    2022-03-09 08:00:00+00:00\n    2022-03-10 08:00:00+00:00\n]\nThis is a &lt;class 'pandas.core.indexes.datetimes.DatetimeIndex'&gt; of &lt;class 'pandas._libs.tslibs.timestamps.Timestamp'&gt; elements.\n\n\nFor a given dataset of datetime in GMT, we have first to localise them over Greenwich, and then convert to the local timezone.\n\nindex_events = pd.date_range(start='3/1/2022', end='3/10/2022').tz_localize(\"GMT\")\nprint_events(index_events.tz_convert('US/Pacific'))\n\n10 days: \n[\n    2022-02-28 16:00:00-08:00\n    2022-03-01 16:00:00-08:00\n    2022-03-02 16:00:00-08:00\n    2022-03-03 16:00:00-08:00\n    2022-03-04 16:00:00-08:00\n    2022-03-05 16:00:00-08:00\n    2022-03-06 16:00:00-08:00\n    2022-03-07 16:00:00-08:00\n    2022-03-08 16:00:00-08:00\n    2022-03-09 16:00:00-08:00\n]\nThis is a &lt;class 'pandas.core.indexes.datetimes.DatetimeIndex'&gt; of &lt;class 'pandas._libs.tslibs.timestamps.Timestamp'&gt; elements."
  },
  {
    "objectID": "posts/gds-2024-01-24-timestamp/index.html#datetime.datetime-str-and-int-to-pandas.timestamp-and-back",
    "href": "posts/gds-2024-01-24-timestamp/index.html#datetime.datetime-str-and-int-to-pandas.timestamp-and-back",
    "title": "Python datetimes manipulations",
    "section": "datetime.datetime, str and int to pandas.Timestamp and back",
    "text": "datetime.datetime, str and int to pandas.Timestamp and back\nIn pandas we typically don’t deal with a single datetime, but with a list of those. With timeseries, pandas becomes particularly efficient when timeseries are used as data frames indexes.\nHere we see some examples of conversion of lists of datetime.datetime into pandas pd.Timestamp indexes, and into int and string.\n\ndef print_events(iterable_events):\n    \"\"\"helper to print info about iterable events len and types\"\"\"\n    print(f\"{len(iterable_events)} days: \")\n    print(\"[\")\n    for ev in iterable_events:\n        print(f\"    {ev}\")\n    print(\"]\")\n    print(f\"This is a {type(iterable_events)} of {type(iterable_events[0])} elements.\")\n\n\n# Create a list of datetime.datetime\ndt_start = dt.datetime(2022, 3, 1)\ndt_end = dt.datetime(2022, 3, 10)\n \nlist_events = []\n \nwhile dt_start &lt;= dt_end:\n    list_events.append(dt_start)\n    dt_start += dt.timedelta(days=1)\n\nprint_events(list_events)\n\n10 days: \n[\n    2022-03-01 00:00:00\n    2022-03-02 00:00:00\n    2022-03-03 00:00:00\n    2022-03-04 00:00:00\n    2022-03-05 00:00:00\n    2022-03-06 00:00:00\n    2022-03-07 00:00:00\n    2022-03-08 00:00:00\n    2022-03-09 00:00:00\n    2022-03-10 00:00:00\n]\nThis is a &lt;class 'list'&gt; of &lt;class 'datetime.datetime'&gt; elements.\n\n\n\n# Create the same list as above, with pandas.date_range https://pandas.pydata.org/docs/reference/api/pandas.date_range.html\nimport pandas as pd\nimport numpy as np\n\ndt_start = dt.datetime(2022, 3, 1)\ndt_end = dt.datetime(2022, 3, 10)\n \nindex_events = pd.date_range(start=dt_start, end=dt_end)\n\nprint_events(index_events)\n\n10 days: \n[\n    2022-03-01 00:00:00\n    2022-03-02 00:00:00\n    2022-03-03 00:00:00\n    2022-03-04 00:00:00\n    2022-03-05 00:00:00\n    2022-03-06 00:00:00\n    2022-03-07 00:00:00\n    2022-03-08 00:00:00\n    2022-03-09 00:00:00\n    2022-03-10 00:00:00\n]\nThis is a &lt;class 'pandas.core.indexes.datetimes.DatetimeIndex'&gt; of &lt;class 'pandas._libs.tslibs.timestamps.Timestamp'&gt; elements.\n\n\n\n# Assign the index to a dataframe\nimport numpy as np\ndf = pd.DataFrame(np.random.choice(list(\"random\"), size=len(index_events))).set_index(index_events)\ndf\n\n\n\n\n\n\n\n\n0\n\n\n\n\n2022-03-01\nm\n\n\n2022-03-02\nr\n\n\n2022-03-03\nn\n\n\n2022-03-04\no\n\n\n2022-03-05\nr\n\n\n2022-03-06\nd\n\n\n2022-03-07\nm\n\n\n2022-03-08\nn\n\n\n2022-03-09\na\n\n\n2022-03-10\nd\n\n\n\n\n\n\n\n\n# Same pandas index_events created above can be created directly with strings\nindex_events = pd.date_range(start='3/1/2022', end='3/10/2022')\n\nprint_events(index_events)\n\n10 days: \n[\n    2022-03-01 00:00:00\n    2022-03-02 00:00:00\n    2022-03-03 00:00:00\n    2022-03-04 00:00:00\n    2022-03-05 00:00:00\n    2022-03-06 00:00:00\n    2022-03-07 00:00:00\n    2022-03-08 00:00:00\n    2022-03-09 00:00:00\n    2022-03-10 00:00:00\n]\nThis is a &lt;class 'pandas.core.indexes.datetimes.DatetimeIndex'&gt; of &lt;class 'pandas._libs.tslibs.timestamps.Timestamp'&gt; elements.\n\n\n\n# Convert the list of datetimes to the pandas index\nindex_events_converted = pd.to_datetime(list_events)\n\nprint_events(index_events)\n\nif not pd.testing.assert_index_equal(index_events_converted, index_events):\n    print()\n    print(\"The two index events are not considered equal! Why?\")\n\n10 days: \n[\n    2022-03-01 00:00:00\n    2022-03-02 00:00:00\n    2022-03-03 00:00:00\n    2022-03-04 00:00:00\n    2022-03-05 00:00:00\n    2022-03-06 00:00:00\n    2022-03-07 00:00:00\n    2022-03-08 00:00:00\n    2022-03-09 00:00:00\n    2022-03-10 00:00:00\n]\nThis is a &lt;class 'pandas.core.indexes.datetimes.DatetimeIndex'&gt; of &lt;class 'pandas._libs.tslibs.timestamps.Timestamp'&gt; elements.\n\nThe two index events are not considered equal! Why?\n\n\nHere something interesting have happened. Even if the two series have the same values, they are not equal.\nPrinting them individually will show why:\n\nindex_events\n\nDatetimeIndex(['2022-03-01', '2022-03-02', '2022-03-03', '2022-03-04',\n               '2022-03-05', '2022-03-06', '2022-03-07', '2022-03-08',\n               '2022-03-09', '2022-03-10'],\n              dtype='datetime64[ns]', freq='D')\n\n\n\nindex_events_converted\n\nDatetimeIndex(['2022-03-01', '2022-03-02', '2022-03-03', '2022-03-04',\n               '2022-03-05', '2022-03-06', '2022-03-07', '2022-03-08',\n               '2022-03-09', '2022-03-10'],\n              dtype='datetime64[ns]', freq=None)\n\n\nThe freq= value of the converted frequency is different from the frequency of the original one, that is defined by days. Under the hood they are different objects, as the first one contains first and last element and the frequency, and the second contains every single individual element.\nEven assigning the frequency to index_events_converted afterwards does not make the two DateteimeIndex equal:\n\nindex_events_converted.freq = \"D\"\nindex_events_converted\n\nDatetimeIndex(['2022-03-01', '2022-03-02', '2022-03-03', '2022-03-04',\n               '2022-03-05', '2022-03-06', '2022-03-07', '2022-03-08',\n               '2022-03-09', '2022-03-10'],\n              dtype='datetime64[ns]', freq='D')\n\n\n\nif not pd.testing.assert_index_equal(index_events_converted, index_events):\n    print()\n    print(\"The two index events are still not equal!\")\n\n\nThe two index events are still not equal!\n\n\nWhen working with pd.Timestamp we have to keep in mind that different constructors creating the same numerical results produce different objects.\nWhat about the memory? Also this is the same for both objects:\n\nprint(index_events.memory_usage())\nprint(index_events_converted.memory_usage())\n\n80\n80\n\n\nNow we can convert back from a pandas DatetimeIndex to a list of datetime.datetime objects.\nA pd.Timestamp object can not be converted directly into a datetime.datetime object, but it can be converted into a datetime.date and a datetime.time object.\nSo we can use a list comprehension and combine the two exported formats:\n\n# Create a Pandas Timestamp object\npd_event = pd.Timestamp('2022-06-18 12:34:56')\n\n# Convert the Timestamp to a Python datetime.date object\ndate = pd_event.date()\ntime = pd_event.time()\ndt_event = dt.datetime.combine(date, time)\n\nprint(f\"Event {pd_event} of type {type(pd_event)}\")\nprint(f\"Event {dt_event} of type {type(dt_event)}\")\n\nEvent 2022-06-18 12:34:56 of type &lt;class 'pandas._libs.tslibs.timestamps.Timestamp'&gt;\nEvent 2022-06-18 12:34:56 of type &lt;class 'datetime.datetime'&gt;\n\n\n\nindex_event_converted_back = [dt.datetime.combine(x.date(), x.time()) for x in index_events_converted]\nprint_events(index_event_converted_back)\n\n10 days: \n[\n    2022-03-01 00:00:00\n    2022-03-02 00:00:00\n    2022-03-03 00:00:00\n    2022-03-04 00:00:00\n    2022-03-05 00:00:00\n    2022-03-06 00:00:00\n    2022-03-07 00:00:00\n    2022-03-08 00:00:00\n    2022-03-09 00:00:00\n    2022-03-10 00:00:00\n]\nThis is a &lt;class 'list'&gt; of &lt;class 'datetime.datetime'&gt; elements.\n\n\n\nPandas Timestamp to int\nTo convert a pandas DatetimeIndex from pd.Timestamp to integer, we have to use the .astype method of a pandas series.\n\nprint(index_events)\nindex_events_int = index_events.astype(int)\nprint()\nprint(index_events_int)\n\nDatetimeIndex(['2022-03-01', '2022-03-02', '2022-03-03', '2022-03-04',\n               '2022-03-05', '2022-03-06', '2022-03-07', '2022-03-08',\n               '2022-03-09', '2022-03-10'],\n              dtype='datetime64[ns]', freq='D')\n\nIndex([1646092800000000000, 1646179200000000000, 1646265600000000000,\n       1646352000000000000, 1646438400000000000, 1646524800000000000,\n       1646611200000000000, 1646697600000000000, 1646784000000000000,\n       1646870400000000000],\n      dtype='int64')\n\n\nThe timestamps are now in nanoseconds, as specified in the dtype of the DatetimeIndex index_events. You have to divide it by 1e9 to have them in seconds:\n\nindex_events_int_sec = (index_events_int / 1e9).astype(int)\nindex_events_int_sec\n\nIndex([1646092800, 1646179200, 1646265600, 1646352000, 1646438400, 1646524800,\n       1646611200, 1646697600, 1646784000, 1646870400],\n      dtype='int64')\n\n\n\n# convert back to `DatetimeIndex`\npd.to_datetime(index_events_int)\n\nDatetimeIndex(['2022-03-01', '2022-03-02', '2022-03-03', '2022-03-04',\n               '2022-03-05', '2022-03-06', '2022-03-07', '2022-03-08',\n               '2022-03-09', '2022-03-10'],\n              dtype='datetime64[ns]', freq=None)\n\n\n\n# convert back to `DatetimeIndex`. Pandas expects nanoseconds\npd.to_datetime(index_events_int_sec)\n\nDatetimeIndex(['1970-01-01 00:00:01.646092800',\n               '1970-01-01 00:00:01.646179200',\n               '1970-01-01 00:00:01.646265600',\n                  '1970-01-01 00:00:01.646352',\n               '1970-01-01 00:00:01.646438400',\n               '1970-01-01 00:00:01.646524800',\n               '1970-01-01 00:00:01.646611200',\n               '1970-01-01 00:00:01.646697600',\n                  '1970-01-01 00:00:01.646784',\n               '1970-01-01 00:00:01.646870400'],\n              dtype='datetime64[ns]', freq=None)\n\n\n\n\nPandas Timestamp to str\nConversion to strings works in the same way as with int.\n\n# convert from DatetimeIndex to string.\nprint(index_events)\nindex_events_str = index_events.astype(str)\nprint()\nprint(index_events_str)\n\nDatetimeIndex(['2022-03-01', '2022-03-02', '2022-03-03', '2022-03-04',\n               '2022-03-05', '2022-03-06', '2022-03-07', '2022-03-08',\n               '2022-03-09', '2022-03-10'],\n              dtype='datetime64[ns]', freq='D')\n\nIndex(['2022-03-01', '2022-03-02', '2022-03-03', '2022-03-04', '2022-03-05',\n       '2022-03-06', '2022-03-07', '2022-03-08', '2022-03-09', '2022-03-10'],\n      dtype='object')\n\n\n\n# convert back from string to DatetimeIndex\npd.to_datetime(index_events_str)\n\nDatetimeIndex(['2022-03-01', '2022-03-02', '2022-03-03', '2022-03-04',\n               '2022-03-05', '2022-03-06', '2022-03-07', '2022-03-08',\n               '2022-03-09', '2022-03-10'],\n              dtype='datetime64[ns]', freq=None)"
  },
  {
    "objectID": "posts/gds-2024-01-24-timestamp/index.html#footnotes",
    "href": "posts/gds-2024-01-24-timestamp/index.html#footnotes",
    "title": "Python datetimes manipulations",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn Python &gt;3 the concepts of class and type was unified, and pure type objects are now only the native ones.↩︎"
  },
  {
    "objectID": "posts/gds-2024-01-24-timestamp/index.html#tldr",
    "href": "posts/gds-2024-01-24-timestamp/index.html#tldr",
    "title": "Python datetimes manipulations",
    "section": "TL;DR:",
    "text": "TL;DR:\n\nimport datetime as dt\nimport pandas as pd\n\n# create a datetime\ndt_event = dt.datetime(year=2024, month=1, day=22, hour=13, minute=55, second=12)\n\n# convert to string with specific format code\nFORMAT_CODE = \"%m/%d/%Y, %H:%M:%S\"\ndt_event_str = dt_event.strftime(FORMAT_CODE)\n\n# convert from string to datetime\ndt_event = dt.datetime.strptime(dt_event_str, FORMAT_CODE)\n\n# convert from datetime to integer (milliseconds)\nmsec_event = int(dt_event.timestamp() * 1000)\n\n# convert from milliseconds to integer\ndt_event = dt.datetime.fromtimestamp(msec_event / 1000)\n\n# create a list of timestamps\ndt_start, dt_end = dt.datetime(2022, 3, 1), dt.datetime(2022, 3, 10)\n \nlist_events = []\n\nwhile dt_start &lt;= dt_end:\n    list_events.append(dt_start)\n    dt_start += dt.timedelta(days=1)\n\n# convert list of timestamps to pandas DatetimeIdex of pd.Timestamps\nindex_events = pd.to_datetime(list_events)\n\n# convert pandas DatetimeIdex of pd.Timestamps to list of timestamps\nlist_events = [dt.datetime.combine(x.date(), x.time()) for x in index_events]\n\n\n# convert pandas DatetimeIdex of pd.Timestamps to an Index of integers (nanoseconds)\nindex_events_int = index_events.astype(int)\n\n# convert pandas Index of integers (nanoseconds) to DatetimeIdex of pd.Timestamps\nindex_events = pd.to_datetime(index_events_int)\n\n# convert pandas DatetimeIdex of pd.Timestamps to an Index of strings\nindex_events_str = index_events.astype(str)\n\n# convert pandas Index of strings to DatetimeIdex of pd.Timestamps\nindex_events = pd.to_datetime(index_events_str)"
  },
  {
    "objectID": "posts/gds-2024-01-24-timestamp/index.html#the-tldr",
    "href": "posts/gds-2024-01-24-timestamp/index.html#the-tldr",
    "title": "Python datetimes manipulations",
    "section": "The TL;DR:",
    "text": "The TL;DR:\nThis blog post is about converting datetime objects across types in python and pandas.\nAll what you need for quickly know how to cast data between datetime, int, str, pd.Timestamp is in the code snippet below and the diagram.\nThe sections below provides you the details and the caveats of each method, and concludes with a section about timezones.\n\nimport datetime as dt\nimport pandas as pd\n\n# create a datetime\ndt_event = dt.datetime(year=2024, month=1, day=22, hour=13, minute=55, second=12)\n\n# convert to string with specific format code\nFORMAT_CODE = \"%m/%d/%Y, %H:%M:%S\"\ndt_event_str = dt_event.strftime(FORMAT_CODE)\n\n# convert from string to datetime\ndt_event = dt.datetime.strptime(dt_event_str, FORMAT_CODE)\n\n# convert from datetime to integer (milliseconds)\nmsec_event = int(dt_event.timestamp() * 1000)\n\n# convert from milliseconds to integer\ndt_event = dt.datetime.fromtimestamp(msec_event / 1000)\n\n# create a list of timestamps\ndt_start, dt_end = dt.datetime(2022, 3, 1), dt.datetime(2022, 3, 10)\n \nlist_events = []\n\nwhile dt_start &lt;= dt_end:\n    list_events.append(dt_start)\n    dt_start += dt.timedelta(days=1)\n\n# convert list of timestamps to pandas DatetimeIdex of pd.Timestamps\nindex_events = pd.to_datetime(list_events)\n\n# convert pandas DatetimeIdex of pd.Timestamps to list of timestamps\nlist_events = [dt.datetime.combine(x.date(), x.time()) for x in index_events]\n\n\n# convert pandas DatetimeIdex of pd.Timestamps to an Index of integers (nanoseconds)\nindex_events_int = index_events.astype(int)\n\n# convert pandas Index of integers (nanoseconds) to DatetimeIdex of pd.Timestamps\nindex_events = pd.to_datetime(index_events_int)\n\n# convert pandas DatetimeIdex of pd.Timestamps to an Index of strings\nindex_events_str = index_events.astype(str)\n\n# convert pandas Index of strings to DatetimeIdex of pd.Timestamps\nindex_events = pd.to_datetime(index_events_str)"
  },
  {
    "objectID": "posts/gds-2024-01-24-timestamp/index.html#summary",
    "href": "posts/gds-2024-01-24-timestamp/index.html#summary",
    "title": "Python datetimes manipulations",
    "section": "Summary",
<<<<<<< HEAD
    "text": "Summary\nThis blog post collects in one place a range of methods for converting datetime objects across types in python and pandas.\nThe code snippet and the subsequent diagram summarise the data casting between datetime, int, str, pd.Timestamp.\nThe remaining sections offers more details for each conversion with some additional details and caveats. The post concludes with a section about some snippets to manipulate timezones in all the examined formats.\n\nimport datetime as dt\nimport pandas as pd\n\n# create a datetime\ndt_event = dt.datetime(year=2024, month=1, day=22, hour=13, minute=55, second=12)\n\n# convert to string with specific format code\nFORMAT_CODE = \"%m/%d/%Y, %H:%M:%S\"\ndt_event_str = dt_event.strftime(FORMAT_CODE)\n\n# convert from string to datetime\ndt_event = dt.datetime.strptime(dt_event_str, FORMAT_CODE)\n\n# convert from datetime to integer (milliseconds)\nmsec_event = int(dt_event.timestamp() * 1000)\n\n# convert from milliseconds to integer\ndt_event = dt.datetime.fromtimestamp(msec_event / 1000)\n\n# create a list of timestamps\ndt_start, dt_end = dt.datetime(2022, 3, 1), dt.datetime(2022, 3, 10)\n \nlist_events = []\n\nwhile dt_start &lt;= dt_end:\n    list_events.append(dt_start)\n    dt_start += dt.timedelta(days=1)\n\n# convert list of timestamps to pandas DatetimeIdex of pd.Timestamps\nindex_events = pd.to_datetime(list_events)\n\n# convert pandas DatetimeIdex of pd.Timestamps to list of timestamps\nlist_events = [dt.datetime.combine(x.date(), x.time()) for x in index_events]\n\n\n# convert pandas DatetimeIdex of pd.Timestamps to an Index of integers (nanoseconds)\nindex_events_int = index_events.astype(int)\n\n# convert pandas Index of integers (nanoseconds) to DatetimeIdex of pd.Timestamps\nindex_events = pd.to_datetime(index_events_int)\n\n# convert pandas DatetimeIdex of pd.Timestamps to an Index of strings\nindex_events_str = index_events.astype(str)\n\n# convert pandas Index of strings to DatetimeIdex of pd.Timestamps\nindex_events = pd.to_datetime(index_events_str)"
=======
    "text": "Summary\nThis blog post is collects all the methods for converting datetime objects across types in python and pandas in one place.\nThe code snippet and the subsequent diagram summarise the data casting between datetime, int, str, pd.Timestamp.\nThe remaining sections offer more details and examples about each of those methods. At the end of the post you will find a section about some snippets to manipulate timezones in all the examined formats.\n\nimport datetime as dt\nimport pandas as pd\n\n# create a datetime\ndt_event = dt.datetime(year=2024, month=1, day=22, hour=13, minute=55, second=12)\n\n# convert to string with specific format code\nFORMAT_CODE = \"%m/%d/%Y, %H:%M:%S\"\ndt_event_str = dt_event.strftime(FORMAT_CODE)\n\n# convert from string to datetime\ndt_event = dt.datetime.strptime(dt_event_str, FORMAT_CODE)\n\n# convert from datetime to integer (milliseconds)\nmsec_event = int(dt_event.timestamp() * 1000)\n\n# convert from milliseconds to integer\ndt_event = dt.datetime.fromtimestamp(msec_event / 1000)\n\n# create a list of timestamps\ndt_start, dt_end = dt.datetime(2022, 3, 1), dt.datetime(2022, 3, 10)\n \nlist_events = []\n\nwhile dt_start &lt;= dt_end:\n    list_events.append(dt_start)\n    dt_start += dt.timedelta(days=1)\n\n# convert list of timestamps to pandas DatetimeIdex of pd.Timestamps\nindex_events = pd.to_datetime(list_events)\n\n# convert pandas DatetimeIdex of pd.Timestamps to list of timestamps\nlist_events = [dt.datetime.combine(x.date(), x.time()) for x in index_events]\n\n\n# convert pandas DatetimeIdex of pd.Timestamps to an Index of integers (nanoseconds)\nindex_events_int = index_events.astype(int)\n\n# convert pandas Index of integers (nanoseconds) to DatetimeIdex of pd.Timestamps\nindex_events = pd.to_datetime(index_events_int)\n\n# convert pandas DatetimeIdex of pd.Timestamps to an Index of strings\nindex_events_str = index_events.astype(str)\n\n# convert pandas Index of strings to DatetimeIdex of pd.Timestamps\nindex_events = pd.to_datetime(index_events_str)"
>>>>>>> a2d95fe (wip)
  }
]