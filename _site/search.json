[
  {
    "objectID": "posts/index_gds.html",
    "href": "posts/index_gds.html",
    "title": "Geospatial Data Science",
    "section": "",
    "text": "Using polygons - part 1\n\n\nGeospatial data science basic operations and measurements\n\n\n\n\n\n\nOct 3, 2022\n\n\n\n\n\n\n  \n\n\n\n\nHow well positioned is your office?\n\n\nTime to question your workplace location\n\n\n\n\n\n\nJul 29, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/bp-2022-07-23-gpg/index.html",
    "href": "posts/bp-2022-07-23-gpg/index.html",
    "title": "How to sign your commits",
    "section": "",
    "text": "This short blog post introduces the reader to the PGP key signatures, and suggests a way of creating a signature for a work user, for a private user and how to swap between them painlessly."
  },
  {
    "objectID": "posts/bp-2022-07-23-gpg/index.html#requirements",
    "href": "posts/bp-2022-07-23-gpg/index.html#requirements",
    "title": "How to sign your commits",
    "section": "Requirements",
    "text": "Requirements\n\nyou are familiar with git command line and gitlab / github interface.\nyou installed the gpg command line interface (Mac, Linux or PowerShell - Windows). GPG stands for Gnu Privacy Guard and implements the OpenPGP standard as defined by RFC4880. It is easy to confuse it with PGP!"
  },
  {
    "objectID": "posts/bp-2022-07-23-gpg/index.html#create-and-add-a-pgp-key-signature-to-github-or-gitlab",
    "href": "posts/bp-2022-07-23-gpg/index.html#create-and-add-a-pgp-key-signature-to-github-or-gitlab",
    "title": "How to sign your commits",
    "section": "Create and add a PGP key signature to github or gitlab",
    "text": "Create and add a PGP key signature to github or gitlab\nThe procedure of creating and adding a PGP key signature is similar to the one you may have previously done to add an RSA key to your github or gitlab account (no worries if you have not done it before). The difference is that instead of using the ssh-agent you will be using the gpg CLI, and you will be asked to pair the configurations of your\n\ncreate a new RSA (read only) key-pair with fingerprint (PGP key signature) via gpg the CLI command\n\ngpg --full-generate-key\nThis will bring you to a prompt asking to input the kind of key that you want and its expiry period, followed by your email, username, and a passphrase. Recommended settings for a key to sing commits is RSA (sign only) followed by 4096 for the keysize (see the appendix if you need to undo the key creation).\n\nIf you are using gitlab add the pgp public key to the page https://gitlab.com/-/profile/gpg_keys, if you are using github use the page https://github.com/settings/keys.\n\nYou can see the list of created key pairs with pgp -k, and you can export the public key to a file with gpg --export -a \"email@address.com\" > public.key, then copy the content of public.key to clipboard to export it to github/gitlab.\n\nNow the missing step is to tell git that it has to start using the created key to sign the commits. Change the user setting on the local machine with\n\ngit config --global user.email <same email used when creating the key-pair>\ngit config --global user.name <same username used when creating the key-pair>\ngit config --global user.signingkey <your fingerprint - copy paste from gitlab gpg page>\ngit config --global commit.gpgSign true"
  },
  {
    "objectID": "posts/bp-2022-07-23-gpg/index.html#swap-between-work-and-personal-profile",
    "href": "posts/bp-2022-07-23-gpg/index.html#swap-between-work-and-personal-profile",
    "title": "How to sign your commits",
    "section": "Swap between work and personal profile",
    "text": "Swap between work and personal profile\nAt this point I had the problem of swapping between multiple gitub/gitlab profiles, for work, personal project, etc. With the commands above I can create as many gpg keys with as many username and emails. To quickly tell git to swap between them I created a bash script for each profile in the sub-folder ~/.git-accounts-settings/:\n# ~/.git-accounts-settings/company_A.sh\ngit config --global user.email <same email used when creating the key-pair>\ngit config --global user.name <same username used when creating the key-pair>\ngit config --global user.signingkey <your fingerprint - copy paste from gitlab gpg page>\ngit config --global commit.gpgSign true\n# ~/.git-accounts-settings/company_B.sh\ngit config --global user.email <another email, same used when creating the key-pair>\ngit config --global user.name <another username used when creating the key-pair>\ngit config --global user.signingkey <your fingerprint - copy paste from gitlab gpg page>\ngit config --global commit.gpgSign true\n# ~/.git-accounts-settings/personal.sh\ngit config --global user.email <personal email>\ngit config --global user.name <personal user>\ngit config --global commit.gpgSign false\nEach time I want to swap across profiles, I can call the script with bash ~/.git-accounts-settings/<my profile>.sh.\nAnd to quickly swap between them, the commands can be aliased with shorter commands:\n# in the .bashrc\nalias set_git_config_company_A=\"bash ~/.git-accounts-settings/company_A.sh\"\nalias set_git_config_company_B=\"bash ~/.git-accounts-settings/company_B.sh\"\nalias set_git_config_personal=\"bash ~/.git-accounts-settings/personal.sh\""
  },
  {
    "objectID": "posts/bp-2022-07-23-gpg/index.html#cache-the-passphrases",
    "href": "posts/bp-2022-07-23-gpg/index.html#cache-the-passphrases",
    "title": "How to sign your commits",
    "section": "Cache the passphrases",
    "text": "Cache the passphrases\nTo avoid typing the passphrase each time a commit requires to be signed, it is possible to specify a caching duration the gpp agent config file, under ~/.gnupg/gpg-agent.conf.\nFor caching the passphrase for 400 days, create the config file with these two lines, where 34560000 is 400 times the number of seconds in a day.\ndefault-cache-ttl 34560000\nmaximum-cache-ttl 34560000\n\n\n\nkey-and-nib"
  },
  {
    "objectID": "posts/bp-2022-07-23-gpg/index.html#appendix-0-list-key-creation",
    "href": "posts/bp-2022-07-23-gpg/index.html#appendix-0-list-key-creation",
    "title": "How to sign your commits",
    "section": "Appendix 0: list key creation",
    "text": "Appendix 0: list key creation\nTo undo the key creation of step 1 you can retrieve the list of existing keys with gpg -k, hen copy the key public id to clipboard, that is a string like this dummy 43525435HJJH5K2H3KJHK3452KJH65NBMBV in the output\npub   ed25519 2022-02-18 [SC] [expires: 2024-02-18]\n      43525435HJJH5K2H3KJHK3452KJH65NBMBV\nuid           [ultimate] Sebastiano Ferraris <seb@email.com>\nsub   cv25519 2022-02-18 [E] [expires: 2024-02-18]\nFinally delete public and private key with:\ngpg --delete-secret-key 43525435HJJH5K2H3KJHK3452KJH65NBMBV\ngpg --delete-key 43525435HJJH5K2H3KJHK3452KJH65NBMBV"
  },
  {
    "objectID": "posts/bp-2022-07-23-gpg/index.html#appendix-1-troubleshooting-on-mac",
    "href": "posts/bp-2022-07-23-gpg/index.html#appendix-1-troubleshooting-on-mac",
    "title": "How to sign your commits",
    "section": "Appendix 1: troubleshooting on mac",
    "text": "Appendix 1: troubleshooting on mac\nIf git commit fails to authenticate the git commit, with the following error message\nThen you have to redirect the GPG_TTY key to the local tty with:\nexport GPG_TTY=$(tty)\nIf this solves the problem, you will have to append it to your .bashrc."
  },
  {
    "objectID": "posts/bp-2022-07-23-gpg/index.html#appendix-2-export-public-and-private-keys",
    "href": "posts/bp-2022-07-23-gpg/index.html#appendix-2-export-public-and-private-keys",
    "title": "How to sign your commits",
    "section": "Appendix 2: export public and private keys",
    "text": "Appendix 2: export public and private keys\nHow do I know my public key?\nThis command will export an ascii armored version of the public key:\ngpg --output public.pgp --armor --export username@email\nAlso to export the secret key, there is a similar command:\ngpg --output private.pgp --armor --export-secret-key username@email"
  },
  {
    "objectID": "posts/bp-2022-07-23-gpg/index.html#appendix-3-trigger-gpg-passphrase-linux",
    "href": "posts/bp-2022-07-23-gpg/index.html#appendix-3-trigger-gpg-passphrase-linux",
    "title": "How to sign your commits",
    "section": "Appendix 3: trigger gpg passphrase linux",
    "text": "Appendix 3: trigger gpg passphrase linux\nOn some linux distributions, the prompt to insert the gpg passphrase does not pop up when you create a new commit. Git simply refuses to add a new non-signed commit. So you will be in the situation where you have some code to commit, you know your passphrase, you have the gpg-agent on so you not need to retype the passphrase each time, but nobody is asking you for your passphrase.\nA workaround is to trigger gpg to ask you for your passphrase for another reason (e.g. signing a file), after which your passphrase is stored in the gpg-agent and you will be free to create signed commits, as the passphrase is automatically retrieved. The list of commands would be:\ngit commit -am \"new stuff\"  # this commit is not added and does not trigger the passphrase prompt\ncd \ntouch z_tmp.txt  # creating a dummy file to authenticate\ngpg -s z_tmp.txt  # this trigger the passphrase (and a y/n question to confirm your choice)\ncd <repo you were working>\ngit commit -am \"new stuff\"\nTo turn this workaround into a oneliner, you can create the dummy file z_tmp.txt in the root directory (as above), and then add the following line to your bash profile:\nalias gpg_trigger='gpg -s ~/z_tmp.txt'\nSo the next time, instead of running all the commands of the previous block, you can simply call the newly created alias gpg_trigger."
  },
  {
    "objectID": "posts/index_al.html",
    "href": "posts/index_al.html",
    "title": "Algorithms",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "posts/index_bp.html",
    "href": "posts/index_bp.html",
    "title": "Code development",
    "section": "",
    "text": "How to sign your commits\n\n\nSetting up gpg authentication while keeping separate work and personal projects\n\n\n\n\n\n\nJul 23, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Posts",
    "section": "",
    "text": "What I whish I knew when I started my blog\n\n\n\n\n\n\nOct 30, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\nGeospatial data science basic operations and measurements\n\n\n\n\n\n\nOct 3, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\nTime to question your workplace location\n\n\n\n\n\n\nJul 29, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\nSetting up gpg authentication while keeping separate work and personal projects\n\n\n\n\n\n\nJul 23, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/index_bl.html",
    "href": "posts/index_bl.html",
    "title": "Blogging",
    "section": "",
    "text": "Create your blog with Quarto\n\n\nWhat I whish I knew when I started my blog\n\n\n\n\n\n\nOct 30, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/gds-2022-07-29-office-pos/index.html",
    "href": "posts/gds-2022-07-29-office-pos/index.html",
    "title": "How well positioned is your office?",
    "section": "",
    "text": "Employees location respect to their office"
  },
  {
    "objectID": "posts/gds-2022-07-29-office-pos/index.html#problem-statement",
    "href": "posts/gds-2022-07-29-office-pos/index.html#problem-statement",
    "title": "How well positioned is your office?",
    "section": "Problem statement",
    "text": "Problem statement\n\nIs your company office optimally located in respect to the position of its employees?"
  },
  {
    "objectID": "posts/gds-2022-07-29-office-pos/index.html#setup-and-requirements",
    "href": "posts/gds-2022-07-29-office-pos/index.html#setup-and-requirements",
    "title": "How well positioned is your office?",
    "section": "Setup and Requirements",
    "text": "Setup and Requirements\nConda environment:\nconda create -n geods python=3.9\nconda activate geods\nRequirements:\n# requirements.txt\ngeopandas==0.10.2\njupyter==1.0.0\nkeplergl==0.3.2\nmatplotlib==3.5.1\nosmnx==1.1.2\npandas==1.4.2\nseaborn==0.11.2"
  },
  {
    "objectID": "posts/gds-2022-07-29-office-pos/index.html#download-and-visualise-the-dataset",
    "href": "posts/gds-2022-07-29-office-pos/index.html#download-and-visualise-the-dataset",
    "title": "How well positioned is your office?",
    "section": "Download and visualise the dataset",
    "text": "Download and visualise the dataset\n\nimport contextily as cx\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport osmnx\nimport pandas as pd\n\nfrom keplergl import KeplerGl\nfrom shapely.geometry import shape\n\n\n# About 10 seconds\ndf_commuter = pd.read_csv(\"https://github.com/uber-web/kepler.gl-data/raw/master/ukcommute/data.csv\")\ndf_commuter.head()\n\n\nconfig = {\n    'version': 'v1',\n    'config': {\n        'mapState': {\n            'latitude': 51.536265,\n            'longitude': -0.039740,\n            'zoom': 10\n        }\n    }\n}\nmap_1 = KeplerGl(data={'commuters': df_commuter}, config=config, height=800)\n\ndisplay(map_1)"
  },
  {
    "objectID": "posts/gds-2022-07-29-office-pos/index.html#narrow-the-dataset-to-the-city-of-london",
    "href": "posts/gds-2022-07-29-office-pos/index.html#narrow-the-dataset-to-the-city-of-london",
    "title": "How well positioned is your office?",
    "section": "Narrow the dataset to the city of London",
    "text": "Narrow the dataset to the city of London\n\nosmnx.config(use_cache=True, log_console=True)\n\ndef gdf_concat(list_gdf: list):\n    return gpd.GeoDataFrame( pd.concat(list_gdf, ignore_index=True)) \n\nquery_city = {'city': 'City of London'}\nquery_london = {'city': 'London'}\n\ngdf = gdf_concat([osmnx.geocode_to_gdf(query_city), osmnx.geocode_to_gdf(query_london)])\n\ngdf.head()\n\n\ngdf_epsg = gdf.to_crs(epsg=3857)\nax = gdf_epsg.plot(figsize=(10, 10), alpha=0.5, edgecolor='k')\ncx.add_basemap(ax)\n\n\ntry:\n    from kepler_config import config_map_2\nexcept ImportError:\n    config = config_map_2\n\nmap_2 = KeplerGl(data={'london' :gdf_epsg}, config=config, height=800)  # kepler knows what to do when fed with a geodataframe\ndisplay(map_2)\n\n\n# -- about 17 seconds --\ngdf_commuters_workplace = gpd.GeoDataFrame(df_commuter.copy(), geometry=gpd.points_from_xy(df_commuter.workplace_lng, df_commuter.workplace_lat))\n\n# -- about 120 seconds: points in polygon \nmask_points_in_city = gdf_commuters_workplace.intersects(gdf.geometry.iloc[0])\nmask_points_in_london = gdf_commuters_workplace.intersects(gdf.geometry.iloc[1])\n\n\nnum_total_rows = len(gdf_commuters_workplace)\nnum_rows_in_city = len(mask_points_in_city[mask_points_in_city == True])\nnum_rows_in_london = len(mask_points_in_london[mask_points_in_london == True])\nprint(f\"Number of rows for offices in the city {num_rows_in_city} ({100 * num_rows_in_city / num_total_rows} %)\")\nprint(f\"Number of rows for offices in london {num_rows_in_london} ({100 * num_rows_in_london / num_total_rows} %)\")\n\nmask_union = mask_points_in_city | mask_points_in_london\nnum_rows_in_union = mask_union.sum()\nprint(f\"Number of offices in the union of London and the City {num_rows_in_union} ({100 * num_rows_in_union / num_total_rows} %)\")\n\n# Sanity check\nassert num_rows_in_union == num_rows_in_city + num_rows_in_london\n\ndf_commuter_london_office = df_commuter[mask_union]\ndf_commuter_london_office.reset_index(inplace=True, drop=True)\n\n\ntry:\n    from kepler_config import config_map_3\nexcept ImportError:\n    config_map_3 = config\n\n# Use the config_3 in kepler_config.py in the repo to reproduce the same image\nmap_3 = KeplerGl(data={'london':gdf_epsg.copy(),  \"commuters\": df_commuter_london_office.copy()}, config=config_map_3, height=800)\ndisplay(map_3)"
  },
  {
    "objectID": "posts/gds-2022-07-29-office-pos/index.html#select-the-office-location",
    "href": "posts/gds-2022-07-29-office-pos/index.html#select-the-office-location",
    "title": "How well positioned is your office?",
    "section": "Select the office location",
    "text": "Select the office location\nGeometry drawn by hand on Kepler GL and copy-pasted below\n\npolygon_st_luke_office = {\"type\":\"Polygon\",\"coordinates\":[[[-0.0930210043528368,51.52553386809767],[-0.09362754938510826,51.5257442611004],[-0.09398505401347826,51.52546150215205],[-0.09363181940230854,51.525218817282784],[-0.09313761642997592,51.52527679524477],[-0.0930210043528368,51.52553386809767]]]}\n\npolygon_albert_road = {\"type\":\"Polygon\",\"coordinates\":[[[0.05074120549614755,51.503014231092195],[0.04882522609357891,51.50189434877025],[0.051410997081145014,51.49996117091324],[0.05337913172491038,51.501678115383754],[0.05074120549614755,51.503014231092195]]]}\n\n\n# narrow dataset to the geometry\nmask_st_luke_office = gdf_commuters_workplace.intersects(shape(polygon_st_luke_office))\ndf_commuters_st_luke_office = df_commuter[mask_st_luke_office]\n\n# embed shape into a geopandas to visualise in kepler\ngdf_st_luke_geometry = gpd.GeoDataFrame({'geometry':[shape(polygon_st_luke_office)], \"display_name\": [\"St Luke's Close Office\"]})\n\n\n# Same for Albert Road office\n\nmask_albert_road = gdf_commuters_workplace.intersects(shape(polygon_albert_road))\ndf_commuters_albert_road = df_commuter[mask_albert_road]\n\ngdf_albert_road = gpd.GeoDataFrame({'geometry':[shape(polygon_albert_road)], \"display_name\": [\"St Luke's Close Office\"]})\n\n\ntry:\n    from kepler_config import config_map_4\nexcept ImportError:\n    config_map_4 = config\n\nmap_4 = KeplerGl(\n    data={\n        \"St Luke's Close Office\": gdf_st_luke_geometry.copy(),  \n        \"commuters to St Luke\": df_commuters_st_luke_office.copy(),\n        \"Albert Road Office\": gdf_albert_road.copy(),\n        \"commuters to Albert\": df_commuters_albert_road.copy(),\n    }, \n    config=config_map_4, \n    height=800)  # kepler knows what to do when fed with a geodataframe\ndisplay(map_4)\n\n\nprint(f\"Commuters to St Luke office {len(df_commuters_st_luke_office)} ({100 * len(df_commuters_st_luke_office) / len(df_commuter)} %)\" )\nprint(f\"Commuters to Albert Road office {len(df_commuters_albert_road)} ({100 *  len(df_commuters_albert_road) / len(df_commuter)} %)\")"
  },
  {
    "objectID": "posts/gds-2022-07-29-office-pos/index.html#compute-bearing-and-distance-of-all-the-commuters-to-the-selected-office",
    "href": "posts/gds-2022-07-29-office-pos/index.html#compute-bearing-and-distance-of-all-the-commuters-to-the-selected-office",
    "title": "How well positioned is your office?",
    "section": "Compute bearing and distance of all the commuters to the selected office",
    "text": "Compute bearing and distance of all the commuters to the selected office\nGiven two points \\((\\text{lng1}, \\text{lat1})\\) and \\((\\text{lng2}, \\text{lat2})\\) the Haversine formula (geodesic distance on the sphere) is: \\[\n\\mathcal{H} = 2 * R * \\arcsin\\left(\\sqrt{d}~\\right)~,\n\\] where \\[\nd = \\sin^2 \\left(\\frac{\\Delta \\text{lat}}{2} \\right) + \\cos(\\text{lat1}) \\cos(\\text{lat2})  \\sin^2\\left(\\frac{\\Delta \\text{lon}}{2}\\right)~,\n\\] and \\(\\Delta \\text{lat} = \\text{lat1} - \\text{lat2}\\), \\(\\Delta \\text{lon} = \\text{lon1} - \\text{lon2}\\), and \\(R\\) is the hearth’s radius.\nThe formula for the bearing, as the angle formed by the geodesics between the north pole and \\((\\text{lng1}, \\text{lat1})\\), and the geodesic between \\((\\text{lng1}, \\text{lat1})\\) and \\((\\text{lng2}, \\text{lat2})\\) is (in radiants): \\[\n\\mathcal{B} = \\arctan\\left(\n    \\frac{\n        \\sin(\\Delta \\text{lon}) \\cos(\\text{lat2})\n    }{\n        \\cos(\\text{lat1}) \\sin(\\text{lat2}) - \\sin(\\text{lat1}) \\cos(\\text{lat2}) \\cos\\left( \\Delta \\text{lon} \\right)\n    }\n\\right)\n\\]\nBoth formulae are based on the spherical model, not on the geospatial data science the standard model is the ellipsoid model World Geodesic System 1984 (WGS84). Exposing the reason why these formulae above are correct, and generalising them for the WGS84 model (the Vincenty’s formulae), would entail expanding the blog post beyond reason. This topic is therefore left as future work.\n\nfrom typing import Tuple\n\nfrom math import radians\n\ndef haversine(lng1: float, lat1: float, lng2: float, lat2: float) -> Tuple[float, float]:\n    \"\"\" returns (haversine distance in km, bearing in degrees from point 1 to point 2), vectorised \"\"\"\n\n    avg_earth_radius_km = 6371.0072\n   \n    lng1, lat1, lng2, lat2 = map(np.deg2rad, [lng1, lat1, lng2, lat2])\n    d_lat, d_lng = lat2 - lat1, lng2 - lng1\n    d = np.sin((d_lat)/2)**2 + np.cos(lat1)*np.cos(lat2) * np.sin((d_lng)/2)**2\n    hav_dist = 2 * avg_earth_radius_km * np.arcsin(np.sqrt(d))\n   \n    y = np.sin(d_lng) * np.cos(lat2)\n    x = np.cos(lat1) * np.sin(lat2) - np.sin(lat1) * np.cos(lat2) * np.cos(d_lng)\n    bearing = (np.arctan2(y, x) + 2 * np.pi) % (2 * np.pi)\n    \n    return hav_dist, np.rad2deg(bearing)\n\n\ndef add_bearing_deg_and_distance_km(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"bearing between A and B is the angle between the geodesics connecting A and the north pole, and the geodesics connecting A and B.\n    Both the bearing and distance are computed on the Spherical model.\n    \"\"\"\n    df = df.copy()\n    \n    lng_work, lat_work = df.workplace_lng.to_numpy(), df.workplace_lat.to_numpy()\n    lng_home, lat_home = df.residence_lng.to_numpy(), df.residence_lat.to_numpy()\n    \n    df[\"distance_km\"], df[\"bearing_deg\"] = haversine(lng_work, lat_work, lng_home, lat_home)\n    \n    return df\n\n\ndf_commuters_st_luke_office = add_bearing_deg_and_distance_km(df_commuters_st_luke_office)\ndf_commuters_albert_road = add_bearing_deg_and_distance_km(df_commuters_albert_road)\n\n\ndf_commuters_st_luke_office.head()"
  },
  {
    "objectID": "posts/gds-2022-07-29-office-pos/index.html#visualise-the-results-in-a-radar-histogram-plot",
    "href": "posts/gds-2022-07-29-office-pos/index.html#visualise-the-results-in-a-radar-histogram-plot",
    "title": "How well positioned is your office?",
    "section": "Visualise the results in a radar-histogram plot",
    "text": "Visualise the results in a radar-histogram plot\nFor an distance and bearing effective visualisation, a circular histogram would do what we need. The polar visualisation of matplotlib will do this for us.\nWe group the dataset into three categories, according to their radial distance from the office: - Within a radius of 10 Km - Between 10Km and 20 Km - Above 20 Km\n\nimport seaborn as sns\n\nsns.set_style(\"darkgrid\", {\"grid.color\": \".6\", \"grid.linestyle\": \":\"})\n\n\ndef radar_histogram(ax, df):\n    \"\"\"\n    Input: \n        df with at least 2 columns distance_km and bearing_deg.\n    Output: radar histogram plot.\n    \"\"\"\n    # Figures parameter\n    directions = 40\n    \n    bottom = 4\n    height_scale = 8\n    \n    # bearing: degrees from nort pole clockwise\n    bearing_bins = np.linspace(0, 360, directions+1, endpoint=False)\n    # angle visualisation: rad from east counterclockwise\n    theta = - 1 * np.linspace(0, 2 * np.pi, directions, endpoint=False) + np.pi/2\n    width = (2*np.pi) / directions\n    \n    # data binning\n    se_bins = pd.cut(df[\"bearing_deg\"].to_numpy(), bearing_bins)\n    np_bins = se_bins.value_counts().to_numpy()\n    bins =  height_scale * np.array(np_bins) / np.max(np_bins)\n    \n    # Uncomment to debug figure:\n    # bins = range(directions)\n    \n    # plotting    \n    ax_bars = ax.bar(theta, bins, width=width, bottom=bottom, color=\"blue\")\n\n    ax.set_yticklabels([])\n    ax.set_xticks(np.linspace(0, 2 * np.pi, 8, endpoint=False))\n    ax.set_xticklabels(['E', '', 'N', '', 'W', '', 'S', ''])\n    ax.grid(False)\n\n    return ax\n\ndef radar_histogram_3_levels(ax, df):\n    \"\"\"\n    Input: \n        df with at least 2 columns distance_km and bearing_deg.\n    Output: radar histogram plot.\n    \"\"\"\n    # Figures parameter\n    directions = 40\n    height_scale = 2\n\n    bottom_inner = 2\n    bottom_betw = 5\n    bottom_outer = 8\n    \n    # bearing: degrees from nort pole clockwise\n    bearing_bins = np.linspace(0, 360, directions+1, endpoint=False)\n    # angle visualisation: rad from east counterclockwise\n    theta = - 1 * np.linspace(0, 2 * np.pi, directions, endpoint=False) + np.pi/2\n    width = (2*np.pi) / directions\n    \n    # data binning\n    \n    df_inner = df[df[\"distance_km\"] <= 10]\n    se_bins_inner = pd.cut(df_inner[\"bearing_deg\"].to_numpy(), bearing_bins)\n    np_bins_inner = se_bins_inner.value_counts().to_numpy()\n    bins_inner =  height_scale * np.array(np_bins_inner) / np.max(np_bins_inner)\n    \n    df_betw = df[(df[\"distance_km\"] > 10) & (df[\"distance_km\"] <= 20)]\n    se_bins_betw = pd.cut(df_betw[\"bearing_deg\"].to_numpy(), bearing_bins)\n    np_bins_betw = se_bins_betw.value_counts().to_numpy()\n    bins_betw =  height_scale * np.array(np_bins_betw) / np.max(np_bins_betw)\n    \n    df_outer = df[df[\"distance_km\"] > 20]\n    se_bins_outer = pd.cut(df_outer[\"bearing_deg\"].to_numpy(), bearing_bins)\n    np_bins_outer = se_bins_outer.value_counts().to_numpy()\n    bins_outer =  height_scale * np.array(np_bins_outer) / np.max(np_bins_outer)\n    \n    # plotting\n    \n    ax_bars_inner = ax.bar(theta, bins_inner, width=width, bottom=bottom_inner, color=\"blue\")\n    ax_bars_betw = ax.bar(theta, bins_betw, width=width, bottom=bottom_betw, color=\"blue\")\n    ax_bars_outer = ax.bar(theta, bins_outer, width=width, bottom=bottom_outer, color=\"blue\")\n\n    \n    ax.set_yticklabels([])\n    # uncomment to add values on radius axis\n    # ax.set_yticks(np.arange(0,10,1.0))\n    # ax.set_yticklabels(['', '', '', '<=10Km ', '', '', '>10Km\\n <=20Km', '', '', '>20Km'])\n\n    ax.set_xticks(np.linspace(0, 2 * np.pi, 8, endpoint=False))\n    ax.set_xticklabels(['E', '', 'N', '', 'W', '', 'S', ''])\n    ax.grid(False)\n\n    return ax\n\n\nfig = plt.figure(figsize=(12, 12))\n\nax11 = fig.add_subplot(221, polar=True)\nax11 = radar_histogram(ax11, df_commuters_st_luke_office)\nax11.set_title(\"Saint Luke Office\", y=1.08, x=1.1)\n\nax12 = fig.add_subplot(222, polar=True)\nax12 = radar_histogram_3_levels(ax12, df_commuters_st_luke_office)\n\n\nax21 = fig.add_subplot(223, polar=True)\nax21 = radar_histogram(ax21, df_commuters_albert_road)\nax21.set_title(\"Albert Road Office\", y=1.08, x=1.1)\n\nax22 = fig.add_subplot(224, polar=True)\nax22 = radar_histogram_3_levels(ax22, df_commuters_albert_road)\n\n\nplt.plot()\n\nFrom the graphs above we can see that the office in Saint Luke is reasonably well balance across the location of the employees, overall, and splitting the commuters into three categories based on radial distance.\nThe same can not be said for the office located in Albert road office, whose employees should consider to relocate to an office further North-East."
  },
  {
    "objectID": "posts/gds-2022-07-29-office-pos/index.html#can-we-do-better",
    "href": "posts/gds-2022-07-29-office-pos/index.html#can-we-do-better",
    "title": "How well positioned is your office?",
    "section": "Can we do better?",
    "text": "Can we do better?\nFrom the question “Is your company office optimally located in respect to the position of its employees?”, we developed a small example of the geospatial data science capabilities to visualise the employees distribution around in respect to the position of their office, via the computation of bearing and distance, to see how off-center it can be, and in which direction it should be relocated to reduce the commuting distance for each employee. In doing so we showed how to download city boundaries from the OSM python API, how to intersect points in polygons, and how visualise geospatial data with Keplerl GL.\nThere are several limitations that are worth mentioning. The first and most obvious one is that the bearing and distance between office and residence is not a the single metric to justify an office relocation. From the point of view of the employee, there are other factors that have not been considered, such as commuting time, cost, frequency of commute, as well as the employee position in the company hierarchy and its “can’t bother” factor. This last metric, is an empirical one of the will (or lack thereof) to make a change, it is entirely based upon the individual opinion, taking into account for example possible facilities in the new office, traffic, proximity to children’s schools and so on. All these parameters has to be considered for the current office location and the potential new office location.\nFrom the point of view of the employer, there is of course the cost of the new office, as well as the cost of the move, as well as prestige of location in respect to possible investors.\nThe last limitation is obviously the dataset. The whole post was written around the toy dataset downloaded from the of Kepler GL examples page. Is it realistic enough or reliable? Can we for example make further analysis on the dataset and obtain some statistics and insights about commuters’ habits? We already noticed that some, if not all commuting locations coincided with the location of the offices. Can we do some further analysis to know how little we should trust the data? To answer this question, we can end up with some minimal data analysis to the dataset to see if the ratio of number of offices in respect to the number of employees is convincing.\n\nnumber_of_offices = len(df_commuter.groupby([\"workplace_lng\", \"workplace_lat\"]).count())\nnumber_of_residences = len(df_commuter.groupby([\"residence_lng\", \"residence_lat\"]).count())\n\nnumber_of_offices_in_london_and_city = len(df_commuter_london_office.groupby([\"workplace_lng\", \"workplace_lat\"]).count())\nnumber_of_residences_commuting_to_london_and_city = len(df_commuter_london_office.groupby([\"residence_lng\", \"residence_lat\"]).count())\n\ncommuters_office_ratio = number_of_residences / number_of_offices\ncommuters_office_ratio_in_london_and_city = number_of_residences_commuting_to_london_and_city / number_of_offices_in_london_and_city\n\nprint(f\"Number of offices in london and the city {number_of_offices_in_london_and_city} ({number_of_offices_in_london_and_city / number_of_offices} %)\")\nprint(f\"Number of residences commuting to london and the city {number_of_residences_commuting_to_london_and_city} ({number_of_residences_commuting_to_london_and_city / number_of_residences} %)\")\nprint(f\"Number of commuters residences per office {commuters_office_ratio}\")\nprint(f\"Number of commuters residences per office in london and the city {commuters_office_ratio_in_london_and_city}\")\n\nCertainly the ratio of commuter’s residences per office can tell us that there is something wrong with the dataset. We could speculated about the fact that the dataset is synthetically generated, or that the arrows direction was swapped when generating the dataset, or both. With no further information, we can only say that no analysis on this dataset can provide us with any reasonable answers or statistics to questions related to commuters in the UK. Nonetheless it is a useful dataset for visualistaion and toy exercises such as the one just presented."
  },
  {
    "objectID": "posts/gds-2022-07-29-office-pos/index.html#resources",
    "href": "posts/gds-2022-07-29-office-pos/index.html#resources",
    "title": "How well positioned is your office?",
    "section": "Resources:",
    "text": "Resources:\n\nhttps://geopandas.org/en/stable/index.html\nhttps://stackoverflow.com/questions/65064351/python-osmnx-how-to-download-a-city-district-map-from-openstreetmap-based-on-t\nhttps://gis.stackexchange.com/questions/343725/convert-geojson-to-geopandas-geodataframe\nhttps://stackoverflow.com/questions/4913349/haversine-formula-in-python-bearing-and-distance-between-two-gps-points\nhttps://anitagraser.github.io/movingpandas/#:~:text=MovingPandas%20is%20a%20Python%20library,movement%20data%20exploration%20and%20analysis\nhttps://stackoverflow.com/questions/17624310/geopy-calculating-gps-heading-bearing\nhttps://geodesy.noaa.gov/CORS/\nhttps://stackoverflow.com/questions/22562364/circular-polar-histogram-in-python\nhttps://stackoverflow.com/questions/12750355/python-matplotlib-figure-title-overlaps-axes-label-when-using-twiny\nhttps://www.dexplo.org/jupyter_to_medium/"
  },
  {
    "objectID": "posts/gds-2022-07-29-office-pos/index.html#also-source-of-inspiration-for-writing-this-blog-post",
    "href": "posts/gds-2022-07-29-office-pos/index.html#also-source-of-inspiration-for-writing-this-blog-post",
    "title": "How well positioned is your office?",
    "section": "Also source of inspiration for writing this blog post:",
    "text": "Also source of inspiration for writing this blog post:\n\nMaxime Labonne\nKhuyen Tran\nAbdishakur\nHerbert Lui"
  },
  {
    "objectID": "posts/gds-2022-10-03-polygons-part-1/index.html",
    "href": "posts/gds-2022-10-03-polygons-part-1/index.html",
    "title": "Using polygons - part 1",
    "section": "",
    "text": "In this article, the first of a series of three, you will find a few facts and examples about an ubiquitous object lying at the very foundation of geospatial data science: nothing less than the polygon!\nWether used to segment raster data, such as satellite images or regular point grids, or in conjunction with other vectorial objects, such as points and lines, polygons are geometrical figures omnipresent in almost every operations and measurements involving maps. Knowing how to manipulate polygons quickly is one of the things telling apart the pro data scientist from the rest.\nIn their simplicity, the power of polygons lies in being a general purpose model for anything that has a surface, and in their extensibility, via associating meta-data or features, as a lists of attributes with collected data in relation to the modelled surface. For example to know where to place a bus stop, roads become segments, existing bus stops become points, and neighborhoods become polygons; to each of these entities we can associate the collected data, that can be static such as lengths, average traveling time, population density, building typology, such as offices, residential or shops, with their capacity, as well as dynamics such as temporal series of traffic, flow of people, opening and closing time etc… We can even consider moving polygons, to model for example icebergs, or shrinking and expanding polygons to model rivers and lakes.\nSimplifying the model and selecting the optimal attributes to the geometries involved, is the step that would make most of the subsequent analysis straightforward, pending the ability to manipulate and measure the objects involved.\nThere are in fact a series of experimental or at least intuitive facts about polygons, and some slightly less intuitive, such as how to measure the distance between two polygons on a surface (and on a curved surface), or how to measure the overlap of two polygons, or the difference between two polygons aimed at modelling the same object. In wanting to keep a pragmatic approach to the matter, we will omit the even less intuitive measures entailing the pathological cases of polygons, such as fractals and limits of curves in general, that rarely arise in the practice of geospatial data science."
  },
  {
    "objectID": "posts/gds-2022-10-03-polygons-part-1/index.html#setting-up-the-environment",
    "href": "posts/gds-2022-10-03-polygons-part-1/index.html#setting-up-the-environment",
    "title": "Using polygons - part 1",
    "section": "0. Setting up the environment",
    "text": "0. Setting up the environment\nCreate a python 3.9 environment called venv and activate it. There are several options, the code below is to create an environment via virtualenv, as quicker than conda, and more than enough for these small experiments:\nvirtualenv venv -p python3.9\nsource venv/bin/activate\nWith the environment activated, install the requirements below. You can install each library individually with pip pip install <copy paste each line here>, or you can copy paste all the requirements in a file in the root of your project requirements.txt and install them all in one go via pip install -r requirements.txt.\n# requirements.txt\ngeopandas==0.11.1\njupyter==1.0.0\nkeplergl==0.3.2\nmatplotlib==3.6.0\nosmnx==1.2.2\nshapely==1.8.4"
  },
  {
    "objectID": "posts/gds-2022-10-03-polygons-part-1/index.html#polygons-intersecting-water-and-land.",
    "href": "posts/gds-2022-10-03-polygons-part-1/index.html#polygons-intersecting-water-and-land.",
    "title": "Using polygons - part 1",
    "section": "1.1 Polygons intersecting water and land.",
    "text": "1.1 Polygons intersecting water and land.\nLand and water on a map are typically modelled by polygons through the segmentation of the underlying geographical features. Given a polygon or a bounding box drawn on a map as input, the goal is to know want to know what are the polygons of land and water this polygon intersects, and, after trimming them to the given geometry, we want to measure the percentage of land respect to the percentage of water. For the example below we will use a polygon drawn around the city of Genova, Italy, (44.405551, 8.904023).\nYou are invited to reproduce the results using a different city.\n\n\nfrom copy import deepcopy\n\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport osmnx\nimport pandas as pd\nimport shapely\n\nfrom geopy import distance\nfrom keplergl import KeplerGl\nfrom shapely.geometry import Polygon, Point, shape\n\n\n# creates an empty map where to draw the polygon\nmap_0 = KeplerGl(height=800)\ndisplay(map_0)\n\n\n# you can create a different polygon with kepler, using the drawing pencil (top right), then right click on it to copy the values and paste it in a dictionary.\ndict_roi = {\"type\":\"Polygon\",\"coordinates\":[[[8.987382009236233,44.54680601507832],[8.841126105836173,44.550231465826634],[8.723022747221052,44.53065474626796],[8.621398927017932,44.48511343018655],[8.591873087363503,44.3865638718846],[8.64817817693631,44.307506289856086],[8.75666847147733,44.33796344890003],[8.83906616353428,44.35613196160054],[8.950989695244745,44.3531860988552],[9.023087675794352,44.30554076888227],[9.079392765366244,44.274083482671955],[9.120591611394707,44.288339652995674],[9.143937624144632,44.33108740800724],[9.141877681842741,44.39784910785984],[9.090379124307875,44.49344084451141],[8.987382009236233,44.54680601507832]]]}\n\nsh_roi = shape(dict_roi)\n\nconfig_map1 = {\n    'version': 'v1',\n    'config': {\n        'mapState': {\n            \"bearing\": 0,\n            \"dragRotate\": False,\n            \"latitude\": 44.41404004333898,\n            \"longitude\": 8.871549089955757,\n            \"pitch\": 0,\n            \"zoom\": 10.347869665266995,\n            \"isSplit\": False,\n        }\n    }\n}\n\ngdf_roi =  gpd.GeoDataFrame({\"name\":[\"ROI\"], \"geometry\": [sh_roi]})\ngdf_roi = gdf_roi.set_crs('4326')  # reproject to WGS84 (the only one supported by kepler)\n\n# To reproduce exactly the same images shown in the article you will have to copy the config\n# file from the linked repo and save it in the save it in a configs.py file.\n# You can still follow the tutorial and run all the lines of code without the config.\ntry:\n    from configs import config_map1\nexcept ImportError:\n    config_map1 = dict()\n\nmap_1 = KeplerGl(data=deepcopy({\"roi\": gdf_roi}), config=config_map1, height=800)\ndisplay(map_1)\n\nThe copy pasted geometry from the kepler app is a dictionary with a type and a list of coordinates following a conventional GeoJSON object. GeoJSON is a format for encoding data about geographic features using JavaScript Object Notation (json), established in 2015.\nA geojson to model a single point appears as:\n{\n  \"type\": \"Feature\",\n  \"geometry\": {\n    \"type\": \"Point\",\n    \"coordinates\": [125.6, 10.1]\n  },\n  \"properties\": {\n    \"name\": \"Dinagat Islands\"\n  }\n}\nWhere \"type\" can be Feature for single objects, or FeatureCollections for multiple objects, and where the geometry.type can be a Point, LineString, Polygon, MultiPoint, MultiLineString, and MultiPolygon.\nTo get the “water” within the selected geometry, we will use the osmnx library.\nThere are a few options to obtain land and water intersecting a region. It is possible to get the rivers, the sea (or bay), and the administrative regions and then consider to subtract the rivers to the administrative regions, and their intersection with the selected ROI. This option may leave some empty spaces, as in OSM there may be multiple annotators and the boundaries may not collimate. A different approach is to assume that all what is not water is land. So we query the rivers and the sea, we consider their union and their intersection with the ROI, and we call this new region “water”. The land will be the set difference between water and the ROI.\n\\[\n\\text{water}_{\\text{roi}} = (\\text{sea} \\cup \\text{river}) \\cap \\text{roi}\\\\\n\\text{land}_{\\text{roi}} = \\text{roi} \\setminus \\text{water}_{\\text{roi}}\n\\]\nSo we need to perform the binary polygon operations of union, intersection and subtraction of polygons, as well as to get the polygons of sea and rivers involved in the selected roi.\n\n# how to get tags:\n# https://wiki.openstreetmap.org/wiki/Map_features#Water_related\n\n# First run would take ~5 mins (2 seconds after the first run, as results are stored in the local folder `cache`).\ngdf_rivers = osmnx.geometries_from_polygon(sh_roi, tags={'natural': 'water'})\ngdf_sea = osmnx.geometries_from_polygon(sh_roi, tags={'natural': 'bay'})\n\n# check crs is already set to WGS84\nassert gdf_rivers.crs == 4326\nassert gdf_sea.crs == 4326\n\n# Dissolve the \"geodataframe\" so that all of its rows are conflated into a single observation\ngdf_rivers = gdf_rivers.dissolve()\ngdf_sea = gdf_sea.dissolve()\n\n# Compute water_roi and land_roi\ngse_sea_union_river = gdf_rivers.union(gdf_sea)  # The union of two geodataframes is a geoseries (the union is only on the geometries)\n\ngse_water_roi = gse_sea_union_river.intersection(gdf_roi)\ngse_land_roi = gdf_roi.difference(gse_water_roi)\n\nyou can also explore the administrative boundaries for an alternative route to find the polygons segmenting land.\ngdf_region = ox.geometries_from_polygon(sh_poly, tags={'boundary': 'administrative'})\nBut this route is not explored here, and it is left for the reader to investigate and plot.\n\n# Call deepcopy when passing an object prevents issue \"AttributeError: 'str' object has no attribute '_geom'\".\n# https://github.com/keplergl/kepler.gl/issues/1240\n\ngdf_genova = gpd.GeoDataFrame(\n    {\n        \"name\": [\"roi\", \"water\", \"land\"], \n        \"geometry\":[deepcopy(gdf_roi.geometry.iloc[0]), deepcopy(gse_water_roi.iloc[0]), deepcopy(gse_land_roi.iloc[0])]\n    }\n)\n\ngdf_genova = gdf_genova.set_crs('4326')\n\n\nkepler_data = {}\nfor _, row in gdf_genova.iterrows():\n    kepler_data.update({row[\"name\"]: gdf_genova[gdf_genova[\"name\"] == row[\"name\"]].copy()})\n\n\ntry:\n    from configs import config_map2\nexcept ImportError:\n    config_map2 = dict()\n\n\nmap_2 = KeplerGl(\n    data=deepcopy(kepler_data), \n    height=800,\n    config=config_map2,\n)\nmap_2\n\n\n# Reproject to cylindrical equal area projectcion\ngdf_genova[\"areas (Km2)\"] = gdf_genova.to_crs({'proj':'cea'}).area/ 10**6\ngdf_genova[[\"name\", \"areas (Km2)\"]]  # Table 1"
  },
  {
    "objectID": "posts/gds-2022-10-03-polygons-part-1/index.html#polygons-intersecting-water-and-land-nearby",
    "href": "posts/gds-2022-10-03-polygons-part-1/index.html#polygons-intersecting-water-and-land-nearby",
    "title": "Using polygons - part 1",
    "section": "1.2 Polygons intersecting water and land… nearby!",
    "text": "1.2 Polygons intersecting water and land… nearby!\nThere are situations when the manually delineated region may not be as accurate as it is required. It may be required to get some information of the areas “nearby” as well. “Nearby” can have different definitions, and here we get three of them: 1. Convex hull 2. Bounding Box 3. Buffer\nThe convex hull of the polygon \\(\\Omega\\) is the smallest convex polygon containing \\(\\Omega\\). The bounding box is the smallest rectangle containing \\(\\Omega\\) with edges parallel to the axis of the reference system. The buffer consists of enlarging the boundaries of \\(\\Omega\\) in a direction perpendicular to its sides. The analogous operation for raster objects is called dilation and it is one of the two most common morphological operations along with erosion.\nBut first we transform the operations we wrote so far into a function, taking dict_roi as input and returning the equivalent of gdf_genova for the selected area, which we can try for a different geometry. Then we test that the functions work for a different region. We chose the Viverone Lake, you are invited to select another one.\n\ndef water_and_land(roi: dict) -> gpd.GeoDataFrame:\n    \"\"\"from a polygon embedded in a geojson dict to a geodataframe with water, land and respective areas\"\"\"\n    sh_roi = shape(roi)\n    gdf_roi =  gpd.GeoDataFrame({\"name\":[\"ROI\"], \"geometry\": [sh_roi]}).set_crs('4326')\n    gdf_rivers = osmnx.geometries_from_polygon(sh_roi, tags={'natural': 'water'}).dissolve()\n    gdf_sea = osmnx.geometries_from_polygon(sh_roi, tags={'natural': 'bay'}).dissolve()\n    gse_sea_union_river = gdf_rivers.union(gdf_sea) if len(gdf_sea) else gdf_rivers\n    gse_water_roi = gse_sea_union_river.intersection(gdf_roi)\n    gse_land_roi = gdf_roi.difference(gse_water_roi)\n    \n    gdf = gpd.GeoDataFrame(\n        {\n            \"name\": [\"roi\", \"water\", \"land\"], \n            \"geometry\":[deepcopy(gdf_roi.geometry.iloc[0]), deepcopy(gse_water_roi.iloc[0]), deepcopy(gse_land_roi.iloc[0])]\n        }\n    ).set_crs('4326')\n    \n    gdf[\"areas (Km2)\"] = gdf.to_crs({'proj':'cea'}).area/ 10**6\n    \n    return gdf\n\n\ndef split_gdf_to_layers(gdf: gpd.GeoDataFrame, column_name: str = \"name\") -> dict:\n    kepler_data = {}\n    for _, row in gdf.iterrows():\n        kepler_data.update({row[column_name]: gdf[gdf[column_name] == row[column_name]].copy()})\n        \n    return kepler_data\n\n\ntry:\n    from configs import config_map3\nexcept ImportError:\n    config_map3 = dict()\n\nif True:\n    dict_viverone = {\"type\":\"Polygon\",\"coordinates\":[[[8.098304589093475,45.44917237554611],[8.061826507846979,45.42337850571526],[8.086238762219843,45.3926467614535],[8.015246804101336,45.37550064157892],[7.985783738478874,45.42121207166915],[8.02282302097566,45.44838495097526],[8.098304589093475,45.44917237554611]]]}\n\n    # ~ 5 minutes first run\n    map_3 = KeplerGl(\n        data=deepcopy(split_gdf_to_layers(water_and_land(dict_viverone))), \n        height=800,\n        config =config_map3,\n    )\nmap_3\n\nNow we can consider the three “nearby” operations. For simplicity all the three operations we are interested in are embedded in a class.\n\n\nclass GeoOperations:\n\n    @staticmethod\n    def _data_to_gdf(roi: dict) -> gpd.GeoDataFrame:\n        \"\"\" input value `data` is a dumped geojson to a string format \"\"\"\n        sh_roi = shape(roi)\n        return gpd.GeoDataFrame({\"name\":[\"ROI\"], \"geometry\": [sh_roi]}).set_crs(4326)\n    \n    @staticmethod\n    def _add_area_column(gdf: gpd.GeoDataFrame) -> None:\n        gdf[\"areas (Km2)\"] = gdf.to_crs({'proj':'cea'}).area/ 10**6\n        \n    @staticmethod\n    def _get_polygon_box(row):\n        c_hull = row.convex_hull  # make sure the input is a polygon \n        x, y, X, Y = c_hull.bounds\n        return shapely.geometry.Polygon([(x, y), (X, y), (X, Y), (x, Y), (x, y)])\n    \n    def buffer(self, data: str, buffer_meters:float = 10) -> gpd.GeoDataFrame:\n        gdf = self._data_to_gdf(data)\n        gdf.geometry = gdf.to_crs(crs=3857)['geometry'].buffer(buffer_meters).to_crs(4326)\n        self._add_area_column(gdf)\n        return gdf\n\n    def convex_hull(self, data: str) -> gpd.GeoDataFrame:\n        gdf = self._data_to_gdf(data)\n        gdf.geometry = gdf['geometry'].convex_hull\n        self._add_area_column(gdf)\n        return gdf\n        \n    def bbox(self, data: str) -> gpd.GeoDataFrame:\n        gdf = self._data_to_gdf(data)\n        gdf['geometry'] = gdf['geometry'].apply(lambda x: self._get_polygon_box(x))\n        self._add_area_column(gdf)\n        return gdf\n\n\nkepler_data = split_gdf_to_layers(water_and_land(dict_roi))\n\nge_ops = GeoOperations()\nkepler_data.update({\"buffer\": ge_ops.buffer(dict_roi, 5_000), \"convex hull\": ge_ops.convex_hull(dict_roi), \"bbox\": ge_ops.bbox(dict_roi)})\n\n\ntry:\n    from configs import config_map4\nexcept ImportError:\n    config_map4 = dict()\n\n\nmap_4 = KeplerGl(\n    data=deepcopy(kepler_data),\n    height=800,\n    config=deepcopy(config_map4),\n)\nmap_4\n\nIn the picture above the bounding box, buffer and convex hull were added to the selected geometry, but the land and sea was not computed for these new polygons. This is left for the reader for practice. For example the water and land within the bounding box should look like the figure below:\n\ndict_bbox = ge_ops.bbox(dict_roi)[\"geometry\"].iloc[0].__geo_interface__\n\nmap_5 = KeplerGl(\n    data=split_gdf_to_layers(water_and_land(dict_bbox)), \n    height=800,\n    config=deepcopy(config_map4), # we can re-use the same config as the previous map\n)\nmap_5"
  },
  {
    "objectID": "posts/gds-2022-10-03-polygons-part-1/index.html#which-country-intersects-this-polygon",
    "href": "posts/gds-2022-10-03-polygons-part-1/index.html#which-country-intersects-this-polygon",
    "title": "Using polygons - part 1",
    "section": "1.3 Which country intersects this polygon?",
    "text": "1.3 Which country intersects this polygon?\nImagine now to draw a polygon on the map with the goal of wanting to know a list with the names of all the countries intersecting it.\nThere are two options we can see think of to solve this problem with osmnx: 1. Create a GeoDataFrame in memory with all the countries in the world, and then compute the intersection between the dataframe and the given polygon. 2. Feed the given polygon to osmnx directly, and use the right tag to get all the countries intersecting it.\nThe first one is very slow to initialise, though fast each time a new polygon is passed to the intersect method. The second one is relatively slow each time a new polygon is given.\nHere, out of laziness we will not use any of the methods above. The toy dataset provided geopandas contains already all the information that we need.\nAs usual the first step is to draw a polygon on the first empty map of the notebook. Then copy paste the polygon geojson below, to be saved in the variable dict_roi. We selected a rectangular region around Myanmar, the reader is free to act according to preference.\n\ndict_roi = {\"type\":\"Polygon\",\"coordinates\":[[[95.27405518168942,28.464895473156595],[88.19328456832459,26.69900877882346],[87.15109322263424,21.154320639876016],[91.6876908450501,14.831845554981104],[98.52323996531177,13.52423467533735],[103.97941818686449,15.157545455928199],[104.43920848643478,20.868173454079436],[104.0713762467781,26.45228108415989],[95.27405518168942,28.464895473156595]]]}\n\nsh_roi = shape(dict_roi)\n\ngdf_roi =  gpd.GeoDataFrame({\"name\":[\"ROI\"], \"geometry\": [sh_roi]})\ngdf_roi = gdf_roi.set_crs('4326')\n\ntry:\n    from configs import config_map6\nexcept ImportError:\n    config_map6 = dict()\n\nmap_6 = KeplerGl(data=deepcopy({\"roi\": gdf_roi}), height=800, config=config_map6)\ndisplay(map_6)\n\n\nimport geopandas as gpd\n\ngdf_countries = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n\ngdf_countries[\"areas (Km2)\"] = gdf_countries.to_crs({'proj':'cea'}).area/ 10**6\n\nax = gdf_countries.plot(figsize=(15, 15), column='areas (Km2)', cmap='Greens')\nax.axis('off')  # map_naturalearth\n\n\ndict_roi = {\"type\":\"Polygon\",\"coordinates\":[[[95.27405518168942,28.464895473156595],[88.19328456832459,26.69900877882346],[87.15109322263424,21.154320639876016],[91.6876908450501,14.831845554981104],[98.52323996531177,13.52423467533735],[103.97941818686449,15.157545455928199],[104.43920848643478,20.868173454079436],[104.0713762467781,26.45228108415989],[95.27405518168942,28.464895473156595]]]}\n\nsh_roi = shape(dict_roi)\n\n# gdf_countries.intersects(sh_roi) returns a boolean series.\n\ngdf_intersection_countries = gdf_countries[gdf_countries.intersects(sh_roi)].reset_index(drop=True)\n\ngdf_intersection_countries[[\"name\", 'areas (Km2)']].head(10)  # Table 2\n\nLeft to the reader is to add a column with the information of the the percentage of the area covered by the selected roi over each country (e.g. Myanmar will be 100% covered by the ROI. What about China?)."
  },
  {
    "objectID": "posts/gds-2022-10-03-polygons-part-1/index.html#distances-between-countries",
    "href": "posts/gds-2022-10-03-polygons-part-1/index.html#distances-between-countries",
    "title": "Using polygons - part 1",
    "section": "1.3 Distances between countries",
    "text": "1.3 Distances between countries\nNow let’s say you have two polygons representing two countries. How to compute their distance in Km on the surface of the earth?\n\n# Show list countries:\nlist_countries = sorted(list(gdf_countries.name))\n\n\"\"\"\n['Afghanistan',\n 'Albania',\n 'Algeria',\n 'Angola',\n 'Antarctica',\n 'Argentina',\n 'Armenia',\n 'Australia',\n 'Austria',\n 'Azerbaijan'\n ... ]\n\"\"\"\n\nlist_countries[:10]\n\n\ngdf_countries.to_crs(epsg=27700,inplace=True) # more about this number on the next article! The impatient reader can look under https://epsg.io/27700\n\nsh_france = gdf_countries[gdf_countries.name == \"France\"].geometry.iloc[0]\nsh_italy = gdf_countries[gdf_countries.name == 'Italy'].geometry.iloc[0]\nsh_uk = gdf_countries[gdf_countries.name == 'United Kingdom'].geometry.iloc[0]\n\nprint(f\"Distance between France and UK polygons: {round(sh_france.distance(sh_uk) / 1_000, 3)} Km\")\n# Distance between France and UK polygons: 37.14 Km\n\nA quick investigation using google seems to not confirm this number. Google says the distance is approx 32Km. Did we do something wrong? Let’s use google maps and pick the two point that are visually the closest to each others on the coasts, copy paste them to the notebook and see what is their distance:\n\nfrom geopy import distance\n\na = (51.108648, 1.286524)\nb = (50.869575, 1.583608)\n\nprint(f\"Distance between France and UK by naked eye: {round(distance.distance(a, b).km, 3)} Km\")\n# Distance between France and UK by naked eye: 33.801 Km\n\nThis number looks way closer to the one found with the shapely distance method. So Why the discrepancy? Is it a problem with the coordinates? Maybe the 27700 is not the optimal one in this case?\nLooking at the data we can find that the accuracy of the countries segmentation is way less that the actual underlying coast. The accuracy is so low that the distance between the polygon is actually wider!\n\ntry:\n    from configs import config_map7\nexcept ImportError:\n    config_map7 = dict()\n\n\nmap_7 = KeplerGl(data=deepcopy({\"rois\": gdf_countries.to_crs('4326')}), height=800, config=config_map7)\ndisplay(map_7)\n\nNOTE: the artefacts that you can see in the map above are caused by polygons crossing the antimeridian. This topic will be discussed in the next blog post of the polygon series “reference systems for polygons”.\nThis section also confirmed numerically the importance of the quality of the segmentation in taking measurements, which lead us straight to the next (and last!) section of this first tutorial about polygons."
  },
  {
    "objectID": "posts/gds-2022-10-03-polygons-part-1/index.html#same-country-different-borders-how-different-are-they",
    "href": "posts/gds-2022-10-03-polygons-part-1/index.html#same-country-different-borders-how-different-are-they",
    "title": "Using polygons - part 1",
    "section": "Same country, different borders: how different are they?",
    "text": "Same country, different borders: how different are they?\nThe problem we want to address in this section is to quantify the differences between segmentations of the same region. As often happens in geospatial data science (and not only there) a ground truth is not available. Also, as in the case of natural phenomena, the true segmentation varies over time, and an approximation is all what we can get. Quantifying the differences between two shapes consists of reducing the dimensionality of the problem, from 2D to 1D. The methods here presented can be generalised from ND to 1D, which can happen in case altitude and time are taken into account in the segmentation process.\nWe will take three segmentations of a the Laguna Honda reservoir in San Francisco, CA: a most accurate, a less accurate, and an absurdly inaccurate one, then we measure their distances with a range of four metrics.\n\ndict_lh_accurate = {\"type\":\"Polygon\",\"coordinates\":[[[-122.46336870734235,37.75382312075932],[-122.46361657885608,37.75373534452417],[-122.4635831171001,37.75365833165909],[-122.46316396885612,37.75301643279386],[-122.46300520664897,37.752840487963525],[-122.46230161881246,37.7522463019286],[-122.46211503445602,37.75203693263166],[-122.46178081988387,37.75186782984793],[-122.46148855443528,37.75184019740165],[-122.46133288449474,37.75201341062547],[-122.4612061599761,37.752084737992796],[-122.46103218224685,37.75214417741294],[-122.46077014171657,37.752179841041794],[-122.46037947391036,37.752188392148845],[-122.46021718629451,37.75217343769624],[-122.4600201930734,37.75211950941181],[-122.45991172291727,37.75207281219638],[-122.45983991818758,37.752077610017345],[-122.45994274713122,37.75230912513596],[-122.45996851594319,37.75240437677724],[-122.46006708164848,37.7524522572354],[-122.46084756619754,37.752511150412346],[-122.46122218790283,37.75259302382214],[-122.46131143485962,37.75262966326064],[-122.46167022567394,37.7526676568636],[-122.46188775902073,37.75272257149743],[-122.46205025380969,37.752816858793715],[-122.462376261119,37.75308710607375],[-122.46251137863547,37.753166819591506],[-122.46262051201403,37.75320708721178],[-122.46268905680037,37.75326534110075],[-122.46270650192626,37.75330849493771],[-122.46273109980764,37.753422092415555],[-122.46277517530947,37.75345753879545],[-122.4628296309809,37.7534771096768],[-122.46294915057157,37.75346704465253],[-122.4630682867433,37.753516920096686],[-122.46336870734235,37.75382312075932]]]}\n\ndict_lh_less_accurate = {\"type\":\"Polygon\",\"coordinates\":[[[-122.46336953589098,37.753832862738975],[-122.4636233738959,37.75373251334907],[-122.4631072366194,37.75293640336153],[-122.46177458709447,37.75186264481159],[-122.46148267338904,37.75183922934248],[-122.4612246047509,37.752080073813296],[-122.46047155200345,37.75218711554916],[-122.4599004164929,37.752066693585995],[-122.45984118762517,37.752080073813296],[-122.45996387599406,37.752411233681165],[-122.46131767868594,37.752618625974755],[-122.46194804306447,37.75272232190355],[-122.46268840391157,37.75324748937671],[-122.46272647961233,37.753431464361086],[-122.46314954295359,37.75359536896148],[-122.46336953589098,37.753832862738975]]]}\n\n\ndict_lh_inaccurate = {\"type\":\"Polygon\",\"coordinates\":[[[-122.46176576455443,37.75368924488866],[-122.46242948841775,37.75334575268072],[-122.4631052799878,37.752296183276115],[-122.46372073302484,37.75144697531703],[-122.46259843631039,37.75127522420765],[-122.46204332180642,37.75167597617579],[-122.46192264474033,37.752410682480374],[-122.46095722821178,37.752954551279636],[-122.4605348584804,37.75364153773237],[-122.46176576455443,37.75368924488866]]]}\n\n\nsh_lh_accurate = shape(dict_lh_accurate)\nsh_lh_less_accurate = shape(dict_lh_less_accurate)\nsh_lh_inaccurate = shape(dict_lh_inaccurate)\n\ngdf_lh_accurate = gpd.GeoDataFrame({\"name\":[\"Accurate\"], \"geometry\": [sh_lh_accurate]}).set_crs('4326')\ngdf_lh_less_accurate = gpd.GeoDataFrame({\"name\":[\"Less accurate\"], \"geometry\": [sh_lh_less_accurate]}).set_crs('4326')\ngdf_lh_inaccurate = gpd.GeoDataFrame({\"name\":[\"Inaccurate\"], \"geometry\": [sh_lh_inaccurate]}).set_crs('4326')\n\n\ntry:\n    from configs import config_map8\nexcept ImportError:\n    config_map8 = dict()\n\nmap_8 = KeplerGl(\n    data={\n        \"Laguna Honda accurate\": deepcopy(gdf_lh_accurate),\n        \"Laguna Honda less accurate\": deepcopy(gdf_lh_less_accurate),\n        \"Laguna Honda inaccurate\": deepcopy(gdf_lh_inaccurate),\n    }, \n    height=800,\n    config=config_map8\n)\ndisplay(map_8)\n\n\nDice’s Score\nAfter Lee R. Dice, the Dice’s score measures the area of the overlap between two polygons normalised to the sum of the individual areas of the two polygons. Therefore if the polygon are perfectly congruent, then the dice equals 1, if there is no intersection between the two elements, the measure is zero.\nFormally: let \\(P_1\\) and \\(P_2\\) be two simple (i.e. where their area can be computed uniquely) polygons drawn on a map, \\(\\mathcal{A}(P_1)\\) and \\(\\mathcal{A}(P_2)\\) their area, and \\(P_1 \\cap P_2\\) their intersection. The Dice’s score of \\(P_1\\) and \\(P_2\\) is defined as: \\[\n\\text{Dice}(P_1, P_2) = \\frac{2 \\mathcal{A}(P_1 \\cap P_2) }{ \\mathcal{A}(P_1) + \\mathcal{A}(P_2)}~.\n\\]\nThis definition does not correspond to the intuitive definition of a distance that is zero when the measured object as as close to each others as possible.\n\n\nDice’s Distance\nThe Dice’s score can be turned into a distance simply modifying its formula as: \\[\n\\text{DiceDist}(P_1, P_2) = 1 - \\frac{2 \\mathcal{A}(P_1 \\cap P_2) }{ \\mathcal{A}(P_1) + \\mathcal{A}(P_2)}~,\n\\] so that the distance between two identical polygons is zero.\n\n\nCovariance Distance\nWe can consider the polygon’s vertex as a cloud of points, and compare the principal components of their distribution encoded as a Symmetric Positive Definite matrix through the covariance matrix.\nThe covariance distance is given by the sum of the 2 eigenvalues of the product of the covariance matrices, normalised by the sum of the products of squared eigenvalues of each covariance matrix. As done for the dice score, to have zero when the two shapes are identica, we take \\(1\\) minus the value obtained.\nFormally: with the same notation as above the covariance distance is defined as \\[\n\\text{CovDist}(P_1, P_2) = \\alpha \\left( 1 - \\frac{ \\text{Tr}( \\text{c}(P_1) \\text{c}(P_2) )  }{ \\text{Fro}(P_1) + \\text{Fro}(P_2)} \\right) ~,\n\\] where \\(\\alpha\\) is a multiplicative factor to scale the data (it can be one if the points are normalised with standard deviation = 1, or computed as the reciprocal of the average standard deviations of the clouds distributions), and \\(\\text{Tr}\\) and \\(\\text{Fro}\\) are the trace and Frobenius norm respectively.\n\n\nHausdoroff Distance\nThe main feature of the covariance distance is that only the principal components of the two geometries are considered, and the noise is not taken into account. The Hausdoroff distance instead aims at being very sensitive to the misplacement of a single vertex between the two geometries.\nIt’s definition is based on as maximal distance between the vertex of one polygon the distance between the \\[\n\\text{HausDist}(P_1, P_2) = \\text{max} \\left\\{ \\text{max}_{p_i \\in \\partial P_1} d(p_i, \\partial P_2) , \\text{max}_{p_j \\in \\partial P_2} d(p_j, \\partial P_1) \\right\\} ~,\n\\] where \\(\\partial P_1\\) is the contour, or perimeter of the polygon \\(P_i\\) and \\(d\\) is the Euclidean distance (or in the case of the curved surface, the geodesic distance).\n\n\nNormalised Symmetric Contour distance\nThe contour distance between \\(P_1\\) and \\(P_2\\) is given by the the sum of the distances of all the vertex of \\(P_1\\) to the contour of \\(P_2\\).\nAs contour distance so defined is not symmetric (the contour distance between \\(P_1\\) and \\(P_2\\) is not the same of the contour distance between \\(P_2\\) and \\(P_1\\)), we can make it symmetric considering the sum of the contour distance between \\(P_1\\) and \\(P_2\\) with the contour distance between \\(P_2\\) and \\(P_1\\).\nThe distance can be normalised considering a distance factor encompassing both geometries in respect to their distances, which can be the lengths of both perimeters.\nFormally: \\[\n\\text{NSCD}(P_1, P_2) = \\frac{  S(P_1, P_2) + S(P_1, P_2) }{ \\vert \\partial P_1 \\vert + \\vert \\partial P_2 \\vert } ~,\n\\] where the length of the contour is indicated with \\(\\vert \\partial P_i \\vert\\), and \\(S(P_1, P_2)\\) is the contour distance, computed as: \\[\nS(P_2, P_1) = \\sum_{p_i \\in \\partial P_1} d(p_i, \\partial P_2)~.\n\\]\nBelow we implement the four distances in a class and then we use this tool to measure the distances between the polygon we created above: sh_lh_accurate, sh_lh_less_accurate and sh_lh_inaccurate.\nAs usual you are invited to try it out on your own, on different geometries, to check the behaviours of the distances.\n\nclass Dist:\n    def __init__(self, p1: Polygon, p2: Polygon):\n        self.p1 = p1\n        self.p2 = p2\n        self.prec = 6\n        \n    def dice(self) -> float:\n        assert self.p1.area + self.p2.area > 0, \"Polygons must have positive areas.\"\n        return np.round(1 - 2 * self.p1.intersection(self.p2).area / (self.p1.area + self.p2.area), self.prec)\n    \n    def cov(self) -> float:\n        x1, y1 = self.p1.exterior.coords.xy[0], self.p1.exterior.coords.xy[1]\n        x2, y2 = self.p2.exterior.coords.xy[0], self.p2.exterior.coords.xy[1]\n        cov1 = np.cov(np.array([(x1 - np.mean(x1)) /  np.std(x1), (y1 - np.mean(y1)) / np.std(y1) ]))\n        cov2 = np.cov(np.array([(x2 - np.mean(x2)) /  np.std(x2), (y2 - np.mean(y2)) / np.std(y2) ]))\n        return np.round((1 - (np.trace(cov1.dot(cov2)) / (np.linalg.norm(cov1, ord='fro') * np.linalg.norm(cov2, ord='fro')))), self.prec)\n    \n    def hau(self) -> float:\n        \"\"\" measured in degrees !!\"\"\"\n        return np.round(self.p1.hausdorff_distance(self.p2), self.prec)\n    \n    def nsc(self) -> float:\n        \"\"\" measured in degrees !!\"\"\"\n        def bd(p1, p2):\n            return np.round(sum([p1.boundary.distance(Point(x, y)) for x,y in zip(p2.exterior.coords.xy[0], p2.exterior.coords.xy[1])]), self.prec)\n        \n        return (bd(self.p1, self.p2) + bd(self.p2, self.p1)) / (self.p1.length + self.p2.length)\n    \n    def d(self, selected_measure: str) -> float:\n        map_measures = {\n            \"dice\": self.dice,\n            \"cov\": self.cov,\n            \"haus\": self.hau,\n            \"nsc\": self.nsc,\n        }\n        return map_measures[selected_measure]()\n        \n        \n\n\n# simplify the name according to the polygons colour in the image above\ngreen = sh_lh_accurate\nyellow = sh_lh_less_accurate\nred = sh_lh_inaccurate\n\nmeas = [ \"dice\", \"cov\", \"haus\", \"nsc\"]\n\npd.DataFrame(\n    {\n        \"green - green\": [Dist(green, green).d(m) for m in meas],\n        \"green - yellow\": [Dist(green, yellow).d(m) for m in meas],\n        \"green - red\": [Dist(green, red).d(m) for m in meas],\n        \"yellow - red\": [Dist(yellow, red).d(m) for m in meas],    \n    },\n    index=meas\n)  # Table 3\n\n\nNote 1:\nThe considered Hausdorff and nsc distances are in degrees and not in meters. As we want to have a measure of the differences for geometries (specifically geometries within comparable latitude intervals) this is not invalidating the results. Though there are cases when these metrics should return the results in meters. This is left as an exercise for the reader (you can leave a comment below for a discussion about possible solutions!).\n\n\nNote 2:\nThe same methods can be used for comparing the differences in shape of two objects that are expected to be similar or two measure the variability of two shapes that are changing over time, assuming that their segmentation is accurate and unbiased.\nAlso the same method can be used to measure the accuracy of a segmentation of two different people segmenting the same object (inter-rater variability) or the same person segmenting twice the same object (intra-rater variability). If the variability is assessed without the rater knowing he is segmenting twice the same shape (out of a stack of different shapes to undergo segmentation, one is repeated twice, re-labelled and re-shuffled in the stack), then this experiment is called test re-test."
  },
  {
    "objectID": "posts/gds-2022-10-03-polygons-part-1/index.html#appendix-list-of-topics-and-further-theoretical-ideas",
    "href": "posts/gds-2022-10-03-polygons-part-1/index.html#appendix-list-of-topics-and-further-theoretical-ideas",
    "title": "Using polygons - part 1",
    "section": "Appendix: list of topics and further theoretical ideas",
    "text": "Appendix: list of topics and further theoretical ideas\nTo summarize this appendix reports the list of the topics of this post. It also provides some hints to dig further into some of the topics were not treated directly.\n\nPolygons and sets\n\npolygon as a list of vectors\npolygon as a set on a surface\noperations between sets: union, intersection, difference, disjoint union.\nCan you express union and difference as combinations of intersection and disjoint union?\nDe Morgan laws\nMathematical concept of Algebra and sigma algebra. Are the polygons on the surface of a sphere with the operation of intersection and disjoint union a sigma algebra?\nGeojson format to represent a polygon\nGeojson to shapely, and shapely operations\nAdding metadata to a polygon with geopandas\nOsmnx library\nVisualisation with KeplerGl and WGS84\n\nMorphological operations in GIS\n\nDilation (or buffer) and erosion\nBounding box\nConvex hull\nClass as a toolbox\n\nDistance between polygons and projections\n\n“Optimal Algorithms for Computing the Minimum Distance Between Two Finite Planar Sets” Toussaint, Bhattacharya\nMinkovsky sum\nHaversine distance\nVincentry distance\n\nShape differences\n\nSegmentation\nDifference quantification as a dimensionality reduction problem\nIntra-rater and inter-rater variability\nDice’s score\nCovariance of a polygon (of a set of 2d points) and covariance distance\nHausdoroff distance\nNormalized symmetric contour distance"
  },
  {
    "objectID": "posts/gds-2022-10-03-polygons-part-1/index.html#resources",
    "href": "posts/gds-2022-10-03-polygons-part-1/index.html#resources",
    "title": "Using polygons - part 1",
    "section": "Resources",
    "text": "Resources\n\nGeopandas and shapely basics https://www.learndatasci.com/tutorials/geospatial-data-python-geopandas-shapely/\nShapely manual https://shapely.readthedocs.io/en/stable/manual.html\nGeopandas crs usage https://geopandas.org/en/stable/docs/user_guide/projections.html\nintersection polygon algorithm https://www.swtestacademy.com/intersection-convex-polygons-algorithm/\nBuffer (dilation) to remove holes https://gis.stackexchange.com/questions/409340/removing-small-holes-from-the-polygon/409398\nmaps projections https://automating-gis-processes.github.io/2017/lessons/L2/projections.html\nTrace and covariance https://online.stat.psu.edu/stat505/lesson/1/1.5"
  },
  {
    "objectID": "posts/gds-2022-10-03-polygons-part-1/index.html#inspired-by",
    "href": "posts/gds-2022-10-03-polygons-part-1/index.html#inspired-by",
    "title": "Using polygons - part 1",
    "section": "Inspired by",
    "text": "Inspired by\n\nQiusheng Wu\nMaxime Labonne"
  },
  {
    "objectID": "posts/bl-2022-10-30-quarto/index.html",
    "href": "posts/bl-2022-10-30-quarto/index.html",
    "title": "Create your blog with Quarto",
    "section": "",
    "text": "A couple of months ago I transitioned my newly clreated blog from medium to Hashnode and I thought there was no better option!\nThen, after talking about some issues in LaTeX formatting and jupyter notebook conversion to markdown with Maxime Labonne (also documented here), Maxime suggested me to give quarto a try.\nIn this post you will find some tips about how to create your blog with quarto and why of the three options that I have tried so far, quarto is by far the best one."
  },
  {
    "objectID": "posts/bl-2022-10-30-quarto/index.html#credits-and-further-resources",
    "href": "posts/bl-2022-10-30-quarto/index.html#credits-and-further-resources",
    "title": "Create your blog with Quarto",
    "section": "Credits and further resources",
    "text": "Credits and further resources\n\nBea Milz\nAlbert Rapp\nLearn enough git to be dangerous by Michael Hartl\nLearn enough command line to be dangerous by Michael Hartl\nQuarto guide and documentation\nMaxime Labonne\nLearn \\(\\LaTeX\\) in 30 minutes"
  },
  {
    "objectID": "posts/bl-2022-10-30-quarto/index.html#pre-requisites",
    "href": "posts/bl-2022-10-30-quarto/index.html#pre-requisites",
    "title": "Create your blog with Quarto",
    "section": "Pre-requisites",
    "text": "Pre-requisites\nUnlike with medium where it is not needed, and with hashnode where you can not know, it is not possible to start a blog with quarto without any familiarity with command line and git.\nIf you are here and are new to you then it may be too early to start using with quarto.\nIf you have any\nMy favourite learning resources, and the one I recommend most often to friends, family and students are from the Learn enough series, - Learn enough git to be dangerous and Learn enough command line to be dangerous."
  },
  {
    "objectID": "posts/bl-2022-10-30-quarto/index.html#why-another-blog",
    "href": "posts/bl-2022-10-30-quarto/index.html#why-another-blog",
    "title": "Create your blog with Quarto",
    "section": "Why another blog?",
    "text": "Why another blog?"
  },
  {
    "objectID": "posts/bl-2022-10-30-quarto/index.html#install-and-setup-quarto",
    "href": "posts/bl-2022-10-30-quarto/index.html#install-and-setup-quarto",
    "title": "Create your blog with Quarto",
    "section": "Install and setup quarto",
    "text": "Install and setup quarto"
  },
  {
    "objectID": "posts/bl-2022-10-30-quarto/index.html#create-your-thematic-blog-posts",
    "href": "posts/bl-2022-10-30-quarto/index.html#create-your-thematic-blog-posts",
    "title": "Create your blog with Quarto",
    "section": "Create your thematic blog posts",
    "text": "Create your thematic blog posts"
  },
  {
    "objectID": "posts/bl-2022-10-30-quarto/index.html#create-your-about-page",
    "href": "posts/bl-2022-10-30-quarto/index.html#create-your-about-page",
    "title": "Create your blog with Quarto",
    "section": "Create your about page",
    "text": "Create your about page"
  },
  {
    "objectID": "posts/bl-2022-10-30-quarto/index.html#create-your-cv",
    "href": "posts/bl-2022-10-30-quarto/index.html#create-your-cv",
    "title": "Create your blog with Quarto",
    "section": "Create your CV",
    "text": "Create your CV"
  },
  {
    "objectID": "posts/bl-2022-10-30-quarto/index.html#finally-publish-your-work",
    "href": "posts/bl-2022-10-30-quarto/index.html#finally-publish-your-work",
    "title": "Create your blog with Quarto",
    "section": "Finally, publish your work",
    "text": "Finally, publish your work"
  },
  {
    "objectID": "posts/bl-2022-10-30-quarto/index.html#appendix-write-mathematical-formulas",
    "href": "posts/bl-2022-10-30-quarto/index.html#appendix-write-mathematical-formulas",
    "title": "Create your blog with Quarto",
    "section": "Appendix: write mathematical formulas",
    "text": "Appendix: write mathematical formulas"
  },
  {
    "objectID": "posts/bl-2022-10-30-quarto/index.html#appendix-my-workflow-with-quarto-and-how-it-improved-over-medium-and-hashnode",
    "href": "posts/bl-2022-10-30-quarto/index.html#appendix-my-workflow-with-quarto-and-how-it-improved-over-medium-and-hashnode",
    "title": "Create your blog with Quarto",
    "section": "Appendix: my workflow with Quarto and how it improved over Medium and Hashnode",
    "text": "Appendix: my workflow with Quarto and how it improved over Medium and Hashnode"
  },
  {
    "objectID": "posts/bl-2022-10-30-quarto/index.html#appendix-what-else-you-can-do-with-quarto",
    "href": "posts/bl-2022-10-30-quarto/index.html#appendix-what-else-you-can-do-with-quarto",
    "title": "Create your blog with Quarto",
    "section": "Appendix: what else you can do with quarto",
    "text": "Appendix: what else you can do with quarto"
  },
  {
    "objectID": "about/about.html",
    "href": "about/about.html",
    "title": "About",
    "section": "",
    "text": "Here I am sharing some tutorials and small projects about geospatial data science with particular focus on algorithms, mathematics and best (in a very relative sense) coding practices.\nAll my posts aim to be as practical and hands-on as possible, so to give you the possibility of reproduce all what is shown.\nI’ve been working in algorithms development and data analysis for more then 10 years in a range of industries, from automotive, to medical imaging to hospitality industry. Now at General System I work and learn with a great team of scientists and engineers how to tackle the challenges posed by geospatial sensor data processing.\nYou can find more about me on linkedin and my open source projects on github. You will certainly not find me on twitter!\nThis blog started on Medium, then moved to hashnode to finally land to quarto. The source code is available on github, where you can find the code resources, images, jupyter notebooks and more."
  },
  {
    "objectID": "z_draft/bl-2022-07-30-to-hashnode/index.html",
    "href": "z_draft/bl-2022-07-30-to-hashnode/index.html",
    "title": "Medium to Hashnode",
    "section": "",
    "text": "This post documents the transition of my newly born blog dedicated to geospatal data science from Medium to Hashnode. Here you will find the pros and cons of both platforms I have found so far and how my workflow have changed after this choice. In between there is the description of two issues with hashnode when articles are sources from github, and at the end you will find a list of possible alternatives for creating a blog."
  },
  {
    "objectID": "z_draft/bl-2022-07-30-to-hashnode/index.html#content",
    "href": "z_draft/bl-2022-07-30-to-hashnode/index.html#content",
    "title": "Medium to Hashnode",
    "section": "Content",
    "text": "Content\n\nPros and Cons\n\nMedium\nHashnode\n\nCurrent issues with Hashnode\nWorkflow\n\nMedium\nHashnode\n\nOther alternatives\nConclusions\nLinks\nCredits"
  },
  {
    "objectID": "z_draft/bl-2022-07-30-to-hashnode/index.html#pros-and-cons",
    "href": "z_draft/bl-2022-07-30-to-hashnode/index.html#pros-and-cons",
    "title": "Medium to Hashnode",
    "section": " 1. Pros and cons",
    "text": "1. Pros and cons\nThe two reasons why hashnode is so attractive for tech writing are the possibility to source the articles directly from github with the github integration (like the article that you are reading right now!), and the possibility of collaborating with other users. But these are only the main reasons. Let’s see a list of 4 pros and cons for each platform, starting with Medium.\n\n Medium\n\nPros:\n\nPopularity: Medium had been around since 2012, it has 200000 writers adn 54 million users and thanks to the quality of its content, whatever you are looking for on google, a medium article is likely to appear in the top 20 results.\nSimplicity: it is very simple to use, and does not require coding experience. For plain text Medium has the shortest possible pathway to connect your content with potential readers.\nMinimalism: the UI for the Medium’s readers is easily recognisable, iconic, clutter free, and simple to interact with.\nWide community: Being widely used and popular, you are very likely to find an expert to ask questions to about something you want to know more about on Medium than anywhere else. As a writer, you can be sure your articles will receive immediate attention from other users with similar interests.\n\nCons:\nI already wrote about the limitations of Medium on a post with the emphatic title The 7 reasons why I am not writing on Medium. Here you can find a condensed version, in only 4 points.\n\nWYSIWYG: the intention of being usable by the widest possible audience has some limitations. The text editor follows the WYSIWYG (what you see is what you get) paradigm: what you write is exactly what you will see in the page, as it happens in other tools, such as in Microsoft Word, and FrontPage. This feature reduces the flexibility in formatting and customisation, and a technical writer typically used to compiled markup languages and text editors with shortcuts, multiple cursors, custom colorschemes, integrated terminals, and other goodies, will have the sensation of writing with handcuffs.\nNot flexible or programmable: The interface of Medium has minimal editability. You can import third party contents, such as images, import code from github gists, and cusomise the graphic. Here as well, all is done via UI and not via programming language, which reduces the flexibility developers are used to have.\nNo version control: also providing the shortest path between writers and readers has a costs. As it would probably complicate the interface beyond the scopes of Medium and it would require some knowledge with version control systems, Medium lacks the possibility of having multiple branches for the same draft and the idea of checkpoint the work to be able to go back in the past at a tagged point.\nNo collaboration: probably for a similar reason why there is no version control, it is also not possible to collaborate with another author while writing an article. Git and its hosts such as github and gitlab have solved the problem of writing collaboration (as well as version control), though the learning curve may discourage the writer who is only looking for a simple platform.\n\n\n\n\n Hashnode Pros\n\nPros:\n\nProgrammable: articles on hashnode are written in markdown. It is not a WYSIWYG language, but it is simple enough to allow the author to focus on the content, without loosing the benefits of a markup language. Moreover hashnode can be customised directly in CSS, can have integration with github.\nCollaborative: collaboration on hashnode can happen either on github integration or adding directly multiple team members to a single blog. The paradigm of the solitary developer in a dark room it’s a thing of the past, and more and more devs are investing a percentage of their time in looking at other people work and sharing theirs, for learning, contaminations, and for finding direct collaborators. Imagine a novel writer reads only his own work, and does not even publish it!\nTech community and support: probably hashnode is not explicitly targeting only tech experts, though these are the writers and readers who will find themselves more at home here than in many other blog platforms I know of. Not only tech articles are encouraged and you will easily find personal blogs of developer talking about various topics such as Kotlin, XML and how to talk confidently in public.\nGithub integration: that’s the main reason why hashnode is so attractive to me (and possibly to many others who have joined). The idea of writing a post, version control and collaborate on it on github, and having it automatically up to date on the community facing blog. There are currently a couple of issues that I have delineated below, though they will hopefully be soon solved to make the integration seamless.\n\nCons:\n\nNot as popular: being more user friendly (for the writer and reader not in for technical content) medium certainly wins in popularity, and therefore in visibility. I expect hashnode will never be as popular, or will have as many users as medium, though it is certainly destined to improve, and to become the go-to choice for developer looking for a quick way of posting content and for a community to interact with.\nNot as minimalistic: compared to the now iconic Medium UI, the dashboard of hashnode looks a crammed and still has to find its personality. Zooming out and going dark mode helps, though there is sometimes too much information in one go for my taste.\nToo many emoji: emoji are like swearing 🤬, they are cool 🆒 and funny 🤣 only if not abused 🔫 or misplaced 🚽. An excessive 📈 amount would infantilise 👶 the reader 👓, visually saturate the page 📃, and divert the attention ㊟ from the content 🔃 (yes, you got me 🥇, I am showing you an example here 🤣🤣🤣🤣🤣, how funny 🤪). Also there is something odd in having 10 different ways of showing appreciation to an article (unlike the single black-and-white claps of Medium). A single option encourages the reader to give kudos, 10 options to chose from so far steered me away from upvoting."
  },
  {
    "objectID": "z_draft/bl-2022-07-30-to-hashnode/index.html#current-issues-with-hashnode",
    "href": "z_draft/bl-2022-07-30-to-hashnode/index.html#current-issues-with-hashnode",
    "title": "Medium to Hashnode",
    "section": " 2. Current issues with Hashnode",
    "text": "2. Current issues with Hashnode\nThis post is also the opportunity to document a couple of issues found (29 July 2022) sourcing articles from github.\n\nAll the underscores appearing in the markdown in the source code are escaped with a backslash. For example the following url that contains underscores appears on github as: txt     https://github.com/SebastianoF/GeoBlog/blob/master/office_positioning/office_positioning.md though when it is formatted as a link, the underscores are escaped, and the link results to be broken.\n\nhttps://github.com/SebastianoF/GeoBlog/blob/master/office_positioning/office_positioning.md\n\nThis happens also when linking images. I raised this issue on stackoverflow as well and communicated it to the hashnode development team. Hopefully it will be only a matter of time before the bug will be fixed.\nLaTeX code is also not formatted, remaining raw across the article. For example, I’m adding below the formula for the bearing:\n\\[\n\\mathcal{B} = \\arctan\\left(\n     \\frac{\n         \\sin(\\Delta \\text{lon}) \\cos(\\text{lat2})\n     }{\n         \\cos(\\text{lat1}) \\sin(\\text{lat2}) - \\sin(\\text{lat1}) \\cos(\\text{lat2}) \\cos\\left( \\Delta \\text{lon} \\right)\n     }\n\\right)\n\\]\nAnd here the integral of the Gaussian:\n\\[\n\\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^{\\infty} e^{-\\frac{1}{2}\\xi^2} \\, d\\xi = 1\n\\]\n\nTo see how the correct formatting should look like (admitting that the bug had not been fixed in the meantime), the source of this article can be found here (NOTE: you will have to remove the slash before each underscores manually from the url after clicking on the link)."
  },
  {
    "objectID": "z_draft/bl-2022-07-30-to-hashnode/index.html#workflows",
    "href": "z_draft/bl-2022-07-30-to-hashnode/index.html#workflows",
    "title": "Medium to Hashnode",
    "section": " 3. Workflows",
    "text": "3. Workflows\nThe platform you are using changes your workflow, and so has an influence on the content, exactly as the mean used to write influences the writing quality. If the workflow is too convoluted there will be detrimental effects on the results, and it would diminish the enthusiasm in starting a new article.\n\n 3. Medium\nSince my technical writing involves producing content with a text editor (or a jupyter notebook) versioned controlled and stored on github, to publish on Medium this is what I would be doing:\n\nCreate, revise and improve content on a jupyter notebook in a versioned controlled repository on github, with images stored in a folder in the same repo, and latex formulae saved in markdown in the same place.\nManually copy paste each cell over to Medium, and re-format in place, transforming link, bold, italic, itemisation etc…\nManually copy each code snippet on github (with some naming that would allow to insert snippets in between snippets already created, and to find the snippets after a few months of not having worked on a post).\nManually create the images from latex formulae, and save them in an external folder somewhere (as it makes no sense to save them on the repository).\nManually upload the images, via drag and drop on the article, then copy paste the caption.\nRe-read, find issues and correct in both places.\n\n\n\n 3. Hashnode\nWith hashnode I found two different workflows, depending on if I want to source the code from github or not.\n\nCreate, revise and improve content on a jupyter notebook in a versioned controlled repository on github, with images stored in a folder in the same repo, and latex formulae saved in markdown in the same place.\nAutomatically convert the Jupyter notebook to markdown with nbconvert.\n\nWith github integration:\n\nAdd the hashnode header to the markdown file, then commit, and merge to master, as you would do anyway with github.\nRe-read, find issues on hashnode and correct directly on github (look at the log if something went wrong in the uploading).\n\nWithout github integration:\n\nCreate a new article on hashnode and copy paste the created markdown.\nRe-read, find issues on hashnode, correct directly on github (look at the log if something went wrong in the uploading), and re-do copy paste of the full file from the repository to hashnode."
  },
  {
    "objectID": "z_draft/bl-2022-07-30-to-hashnode/index.html#alternatives",
    "href": "z_draft/bl-2022-07-30-to-hashnode/index.html#alternatives",
    "title": "Medium to Hashnode",
    "section": " 4. Alternatives",
    "text": "4. Alternatives\nHere we are in the not tried yet zone, so the following list of possible alternatives is based on google searches only (please leave a comment if you have hints and recommendations):\n\nCreate it from scratch with HTML, CSS and JavaScript. if not already knowledgeable about it, it may be worth digging into HTML, CSS and JavaScript at least once in life. A blog can be the right opportunity for it. If you want to pursue this path, the blog by Alex Nim telling you about his journey and secrets may be a good starting point.\nWordpress is certainly a valid option. It has a very simple interface, has a wide range of customisation, and most importantly, it supports LaTeX. There are several technical blogs out there worth mentioning written in wordpress, one for all the blog by Terence Tao\nGhost is another valid alternative, allowing to inject content directly in HTML or render LaTeX equations\nFastpages a “wrapper” of Jekyll hosted on github pages and developed by volunteers, it allows the developers to create a blog directly from the jupyter notebooks. A cool example is the ML blog by Maxime Labonne."
  },
  {
    "objectID": "z_draft/bl-2022-07-30-to-hashnode/index.html#conclusions",
    "href": "z_draft/bl-2022-07-30-to-hashnode/index.html#conclusions",
    "title": "Medium to Hashnode",
    "section": " 5. Conclusions",
    "text": "5. Conclusions\nDespite the markdown and LaTeX formatting issues documented above for when sourcing an article from github, hashnode has plenty to offer for the technical writer. Using markdown instead of a WYSIWYG interface greatly reduces the editing time, it gives the possiblity of having collaborators, and set your blog into an ecosystem of bloggers producing content for developers."
  },
  {
    "objectID": "z_draft/bl-2022-07-30-to-hashnode/index.html#links",
    "href": "z_draft/bl-2022-07-30-to-hashnode/index.html#links",
    "title": "Medium to Hashnode",
    "section": " 6. Links",
    "text": "6. Links\n(All the links below are underscore-free, so they should work despite the documented issue, even if this article is sourced from github)\n\nConnect a repository to hashnode\nHow to publish articles on hashnode\nList of tags\nFiles uploader\nTemplate repository\nGive feedback to hashnode"
  },
  {
    "objectID": "z_draft/bl-2022-07-30-to-hashnode/index.html#credits",
    "href": "z_draft/bl-2022-07-30-to-hashnode/index.html#credits",
    "title": "Medium to Hashnode",
    "section": " 7. Credits",
    "text": "7. Credits\n\nQiusheng Wu: his blog also had moved from Medium to Hashnode, and he introduced me to this platform.\nAlex Nim for the tutorial about how to build your own blog from scratch.\nMaxime Labonne for his very inspiring blog, and the insights about how to transition from a blog in fastpages to Medium.\nHashnode support team, in particular to uncle-big-bay, of the hashnode support team, for promptly answering my messages and looking into the issue."
  }
]