{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Spatiotemporal join with Geopandas\n",
    "subtitle: Filter geospatial events with polygons + time intervals\n",
    "date: 2025-07-16\n",
    "categories: [geospatial, H3, KeplerGl, pings binning, Geolife, tutorial]\n",
    "image: images/cover.png\n",
    "toc: true\n",
    "draft: false\n",
    "colab: <a href=\"https://colab.research.google.com/github/SebastianoF/GeoDsBlog/blob/master/posts/gds-2025-07-16-spatiotemporal-join/index.ipynb\" target=\"_blank\"><img src=\"images/colab.svg\"></a>\n",
    "github: <a href=\"https://github.com/SebastianoF/GeoDsBlog/blob/master/posts/gds-2025-07-16-spatiotemporal-join/index.ipynb\" target=\"_blank\">  <img src=\"images/github.svg\"> </a>\n",
    "twitter-card:\n",
    "  image: images/cover.png\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spatiotemporal join intro - in progress\n",
    "\n",
    "\n",
    "\n",
    "### Outline\n",
    "\n",
    "Goal: combine the spatial and the temporal join with geopandas.\n",
    "\n",
    "\n",
    "- Datasets\n",
    "  - Pings (id, lat, lon, timestamp)  (map)\n",
    "  - Events (id, polygon, start_time, end_time)  (map)\n",
    "  - Temporised polygons  (map)\n",
    "- Joins\n",
    "  - Pings in polygon  (illustration and map)\n",
    "  - Events in polygons  (illustration and map)\n",
    "  - Pings in events  (illustration and map)\n",
    "- References and acknowledgements\n",
    "\n",
    "\n",
    "THE END. \n",
    "\n",
    "References:\n",
    "# https://stackoverflow.com/questions/63369715/filter-a-geopandas-dataframe-within-a-polygon-and-remove-from-the-dataframe-the\n",
    "https://www.youtube.com/watch?v=y85IKthrV-s&t=3s&ab_channel=JonathanSoma\n",
    "\n",
    "\n",
    "- @sec-python-env Python environment setup\n",
    "- @sec-download-the-dataset Download geolife dataset\n",
    "- @sec-load-dataset Load the dataset\n",
    "- @sec-eda Exploratory data analysis (EDA)\n",
    "- @sec-space-time-binning Space-time binning with H3\n",
    "- @sec-dataset-profile-visualisation Dataset profile visualisation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.callout-tip}\n",
    "To create the `.gif` animation on mac, take a scree recording with a software like QuickTime player `video.mov`, then following [this guide](https://gist.github.com/SheldonWangRJT/8d3f44a35c8d1386a396b9b49b43c385), run the command:\n",
    "\n",
    "```bash\n",
    "ffmpeg -i video.mov -pix_fmt rgb8 -r 10 output.gif && gifsicle -O3 output.gif -o output.gif\n",
    "```\n",
    "\n",
    "Making sure that you have `ffmpeg` and `gifsicle` installed beforehand:\n",
    "\n",
    "```bash\n",
    "brew install ffmpeg\n",
    "brew install gifsicle\n",
    "```\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python environment setup {#sec-python-env}\n",
    "\n",
    "Create a virtualenvironment and install the required libraries.\n",
    "\n",
    "Suggested lightweight method:\n",
    "\n",
    "```bash\n",
    "virtualenv venv -p python3.11\n",
    "source venv/bin/activate\n",
    "pip install -r https://raw.githubusercontent.com/SebastianoF/GeoDsBlog/master/posts/gds-2024-06-20-dataset-profiling/requirements.txt\n",
    "```\n",
    "\n",
    "Where the requirement file is sourced directly from the repository, and contains the following libraries and their pinned dependencies:\n",
    "\n",
    "```text\n",
    "altair==5.3.0\n",
    "geopandas==1.0.1\n",
    "gitpython==3.1.43\n",
    "h3==3.7.7\n",
    "keplergl==0.3.2\n",
    "matplotlib==3.9.0\n",
    "pyarrow==16.1.0\n",
    "tqdm==4.66.4\n",
    "```\n",
    "\n",
    "You can look under `requirements.txt` [in the repository](https://github.com/SebastianoF/GeoDsBlog/blob/master/posts/gds-2024-06-20-dataset-profiling/requirements.txt) for the pinned dependency tree.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import io\n",
    "import zipfile\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "\n",
    "import altair as alt\n",
    "import geopandas as gpd\n",
    "import git\n",
    "import h3\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from IPython.display import Image\n",
    "from shapely import Polygon\n",
    "from tqdm import tqdm\n",
    "\n",
    "plt.style.use(\"dark_background\")\n",
    "alt.renderers.set_embed_options(theme=\"dark\")\n",
    "\n",
    "KEPLER_OUTPUT = False  # for blog visualisation: set to true if running on jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the dataset {#sec-download-the-dataset}\n",
    "\n",
    "The dataset can be downloaded manually, or running the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ready\n"
     ]
    }
   ],
   "source": [
    "url_download_geolife_dataset = \"https://download.microsoft.com/download/F/4/8/F4894AA5-FDBC-481E-9285-D5F8C4C4F039/Geolife%20Trajectories%201.3.zip\"\n",
    "\n",
    "try:\n",
    "    path_root = Path(git.Repo(Path().cwd(), search_parent_directories=True).git.rev_parse(\"--show-toplevel\"))\n",
    "except (git.exc.InvalidGitRepositoryError, ModuleNotFoundError):\n",
    "    path_root = Path().cwd()\n",
    "\n",
    "path_data_folder = path_root / \"z_data\"\n",
    "path_data_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "path_unzipped_dataset = path_data_folder / \"Geolife Trajectories 1.3\"\n",
    "\n",
    "if not path_unzipped_dataset.exists():\n",
    "    r = requests.get(url_download_geolife_dataset)\n",
    "    z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "    z.extractall(path_data_folder)\n",
    "\n",
    "print(\"Dataset ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset {#sec-load-dataset}\n",
    "\n",
    "The folder structure of the Geolife dataset is not as linear as it could be.\n",
    "\n",
    "- The trajectories are divided into 182 folders numbered from `000` to `181`. \n",
    "- Each folder contains a subfolder called `Trajectory` containing a series of `.plt` files.\n",
    "- Some folders also contain a file called `labels.txt`, containing time intervals and transportation modes.\n",
    "- There are two different datetime formats.\n",
    "- Within the zip folder there is also a pdf file with the dataset description.\n",
    "\n",
    "To load the files I implemented a class to load the files and enrich them with the labels when they are available. The time formats are converted to pandas datetimes[^1].\n",
    "\n",
    "The main parser method has also a boolean flag `parse_only_if_labels`. `False` by default, when set to `True` only the files with the labels are parsed.\n",
    "\n",
    "[^1]: For a refresher about datetime manipulation, [here](https://sebastianof.github.io/GeoDsBlog/posts/gds-2024-01-24-timestamp/) is an detailed tutorial on this topic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns names\n",
    "\n",
    "COLS_TRAJECTORY = [\n",
    "    \"latitude\",\n",
    "    \"longitude\",\n",
    "    \"none\",\n",
    "    \"altitude\",\n",
    "    \"date_elapsed\",\n",
    "    \"date\",\n",
    "    \"time\",\n",
    "]\n",
    "\n",
    "COLS_TRAJECTORY_TO_LOAD = [\n",
    "    \"latitude\",\n",
    "    \"longitude\",\n",
    "    \"altitude\",\n",
    "    \"date\",\n",
    "    \"time\",\n",
    "]\n",
    "\n",
    "COLS_LABELS = [\n",
    "    \"start_time\",\n",
    "    \"end_time\",\n",
    "    \"transportation_mode\",\n",
    "]\n",
    "\n",
    "COLS_RESULTS = [\n",
    "    \"entity_id\",\n",
    "    \"latitude\",  # degrees\n",
    "    \"longitude\",  # degrees\n",
    "    \"altitude\",  # meters (int)\n",
    "    \"timestamp\",  # UNIX (int)\n",
    "    \"transport\",  # only if the labels.txt file is there\n",
    "]\n",
    "\n",
    "# Format codes\n",
    "\n",
    "LABELS_FC = \"%Y/%m/%d %H:%M:%S\"\n",
    "TRAJECTORY_FC = \"%Y-%m-%d %H:%M:%S\"\n",
    "\n",
    "\n",
    "class GeoLifeDataLoader:\n",
    "    def __init__(self, path_to_geolife_folder: str | Path) -> None:\n",
    "        self.pfo_geolife = path_to_geolife_folder\n",
    "        pfo_data = Path(self.pfo_geolife) / \"Data\"\n",
    "        self.dir_per_subject: dict[int, Path] = {int(f.name): f for f in pfo_data.iterdir() if f.is_dir()}\n",
    "\n",
    "    def to_pandas_per_device(\n",
    "        self,\n",
    "        device_number: int,\n",
    "        leave_progressbar=True,\n",
    "    ) -> tuple[pd.DataFrame]:\n",
    "        path_to_device_folder = self.dir_per_subject[device_number]\n",
    "        pfo_trajectory = path_to_device_folder / \"Trajectory\"\n",
    "        pfi_labels = path_to_device_folder / \"labels.txt\"\n",
    "        df_labels = None\n",
    "        df_trajectory = None\n",
    "\n",
    "        list_trajectories = [plt_file for plt_file in pfo_trajectory.iterdir() if str(plt_file).endswith(\".plt\")]\n",
    "        list_dfs = []\n",
    "        for traj in tqdm(list_trajectories, leave=leave_progressbar):\n",
    "            df_sourced = pd.read_csv(traj, skiprows=6, names=COLS_TRAJECTORY, usecols=COLS_TRAJECTORY_TO_LOAD)\n",
    "            df_sourced[\"altitude\"] = df_sourced[\"altitude\"].apply(lambda x: x * 0.3048)  # feets to meters\n",
    "            df_sourced[\"timestamp\"] = df_sourced.apply(\n",
    "                lambda x: dt.datetime.strptime(x[\"date\"] + \" \" + x[\"time\"], TRAJECTORY_FC),\n",
    "                axis=1,\n",
    "            )\n",
    "            df_sourced = df_sourced.assign(entity_id=f\"device_{device_number}\", transport=None)\n",
    "            df_sourced = df_sourced[COLS_RESULTS]\n",
    "            list_dfs.append(df_sourced)\n",
    "        df_trajectory = pd.concat(list_dfs)\n",
    "\n",
    "        if pfi_labels.exists():\n",
    "            df_labels = pd.read_csv(pfi_labels, sep=\"\\t\")\n",
    "            df_labels.columns = COLS_LABELS\n",
    "            df_labels[\"start_time\"] = df_labels[\"start_time\"].apply(lambda x: dt.datetime.strptime(x, LABELS_FC))\n",
    "            df_labels[\"end_time\"] = df_labels[\"end_time\"].apply(lambda x: dt.datetime.strptime(x, LABELS_FC))\n",
    "        if df_labels is not None:\n",
    "            for _, row in tqdm(df_labels.iterrows(), leave=leave_progressbar):\n",
    "                mask = (df_trajectory.timestamp > row.start_time) & (df_trajectory.timestamp <= row.end_time)\n",
    "                df_trajectory.loc[mask, \"transport\"] = row[\"transportation_mode\"]\n",
    "\n",
    "        return df_trajectory, df_labels\n",
    "\n",
    "    def to_pandas(self, parse_only_if_labels: bool = False) -> pd.DataFrame:\n",
    "        list_dfs_final = []\n",
    "        for idx in tqdm(self.dir_per_subject.keys()):\n",
    "            df_trajectory, df_labels = self.to_pandas_per_device(idx, leave_progressbar=False)\n",
    "            if parse_only_if_labels:\n",
    "                if df_labels is not None:\n",
    "                    list_dfs_final.append(df_trajectory)\n",
    "                else:\n",
    "                    pass\n",
    "            else:\n",
    "                list_dfs_final.append(df_trajectory)\n",
    "\n",
    "        return pd.concat(list_dfs_final)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and save to parquet \n",
    "\n",
    "To speed up next loading phase, I save to parquet after loading from the `.plt` files the first time.\n",
    "\n",
    "In this way it will take less time to reload the dataset to continue the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_complete_parquet = path_data_folder / \"GeolifeTrajectories.parquet\"\n",
    "\n",
    "if not path_complete_parquet.exists():\n",
    "    # this takes about 5 minutes\n",
    "    gdl = GeoLifeDataLoader(path_unzipped_dataset)\n",
    "    df_geolife = gdl.to_pandas(parse_only_if_labels=True)\n",
    "    df_geolife = df_geolife.reset_index(drop=True)\n",
    "    df_geolife.to_parquet(path_complete_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24876977, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity_id</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>altitude</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>transport</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>device_135</td>\n",
       "      <td>39.974294</td>\n",
       "      <td>116.399741</td>\n",
       "      <td>149.9616</td>\n",
       "      <td>2009-01-03 01:21:34</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>device_135</td>\n",
       "      <td>39.974292</td>\n",
       "      <td>116.399592</td>\n",
       "      <td>149.9616</td>\n",
       "      <td>2009-01-03 01:21:35</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>device_135</td>\n",
       "      <td>39.974309</td>\n",
       "      <td>116.399523</td>\n",
       "      <td>149.9616</td>\n",
       "      <td>2009-01-03 01:21:36</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>device_135</td>\n",
       "      <td>39.974320</td>\n",
       "      <td>116.399588</td>\n",
       "      <td>149.9616</td>\n",
       "      <td>2009-01-03 01:21:38</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>device_135</td>\n",
       "      <td>39.974365</td>\n",
       "      <td>116.399730</td>\n",
       "      <td>149.6568</td>\n",
       "      <td>2009-01-03 01:21:39</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    entity_id   latitude   longitude  altitude           timestamp transport\n",
       "0  device_135  39.974294  116.399741  149.9616 2009-01-03 01:21:34      None\n",
       "1  device_135  39.974292  116.399592  149.9616 2009-01-03 01:21:35      None\n",
       "2  device_135  39.974309  116.399523  149.9616 2009-01-03 01:21:36      None\n",
       "3  device_135  39.974320  116.399588  149.9616 2009-01-03 01:21:38      None\n",
       "4  device_135  39.974365  116.399730  149.6568 2009-01-03 01:21:39      None"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this takes 2 seconds (MAC book air, 8GB RAM)\n",
    "df_geolife = pd.read_parquet(path_complete_parquet)\n",
    "print(df_geolife.shape)\n",
    "df_geolife.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
