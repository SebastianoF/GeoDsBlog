{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: H3 to Vect\n",
    "subtitle: Location prediction with H3\n",
    "date: 2024-06-25\n",
    "categories: [geospatial, H3, KeplerGl, word2vect]\n",
    "image: images/cover.png\n",
    "toc: true\n",
    "draft: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Into here\n",
    "\n",
    "### Outline\n",
    "\n",
    "sections there\n",
    "\n",
    "- @sec-python-env python environment setup\n",
    "- @sec-download-the-dataset Download geolife dataset\n",
    "- @sec-load-dataset Load the dataset\n",
    "- @sec-method Method \n",
    "\n",
    "More sections here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python environment setup {#sec-python-env}\n",
    "\n",
    "Create a virtualenvironment and install the required libraries.\n",
    "\n",
    "Suggested lightweight method:\n",
    "\n",
    "```bash\n",
    "virtualenv venv -p python3.11\n",
    "source venv/bin/activate\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "The requirement file contains\n",
    "```text\n",
    "proper requirements list here\n",
    "```\n",
    "\n",
    "Please look under `requirements.txt` in the repository for the complete pinned dependency tree.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the dataset {#sec-download-the-dataset}\n",
    "\n",
    "The dataset can be downloaded manually, or running the following python command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import io\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "import altair as alt\n",
    "import git\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "plt.style.use('dark_background')\n",
    "alt.renderers.set_embed_options(theme=\"dark\")\n",
    "\n",
    "KEPLER_OUTPUT = False  # for blog visualisation: set to true if running on jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ready\n"
     ]
    }
   ],
   "source": [
    "url_download_geolife_dataset = \"https://download.microsoft.com/download/F/4/8/F4894AA5-FDBC-481E-9285-D5F8C4C4F039/Geolife%20Trajectories%201.3.zip\"\n",
    "\n",
    "try:\n",
    "    path_root = Path(git.Repo(Path().cwd(), search_parent_directories=True).git.rev_parse(\"--show-toplevel\"))\n",
    "except (git.exc.InvalidGitRepositoryError, ModuleNotFoundError):\n",
    "    path_root = Path().cwd()\n",
    "    \n",
    "path_data_folder = path_root / \"z_data\" \n",
    "path_data_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "path_unzipped_dataset = path_data_folder / \"Geolife Trajectories 1.3\"\n",
    "\n",
    "if not path_unzipped_dataset.exists():\n",
    "    r = requests.get(url_download_geolife_dataset)\n",
    "    z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "    z.extractall(path_data_folder)\n",
    "\n",
    "print(\"Dataset ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset {#sec-load-dataset}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns names\n",
    "\n",
    "COLS_TRAJECTORY = [\n",
    "    \"latitude\",\n",
    "    \"longitude\",\n",
    "    \"none\",\n",
    "    \"altitude\",\n",
    "    \"date_elapsed\",\n",
    "    \"date\",\n",
    "    \"time\",\n",
    "]\n",
    "\n",
    "COLS_TRAJECTORY_TO_LOAD = [\n",
    "    \"latitude\",\n",
    "    \"longitude\",\n",
    "    \"altitude\",\n",
    "    \"date\",\n",
    "    \"time\",\n",
    "]\n",
    "\n",
    "COLS_LABELS = [\n",
    "    \"start_time\",\n",
    "    \"end_time\",\n",
    "    \"transportation_mode\",\n",
    "]\n",
    "\n",
    "COLS_RESULTS = [\n",
    "    \"entity_id\",\n",
    "    \"latitude\",  # degrees\n",
    "    \"longitude\",  # degrees\n",
    "    \"altitude\",  # meters (int)\n",
    "    \"timestamp\",  # UNIX (int)\n",
    "    \"transport\",  # only if the labels.txt file is there\n",
    "]\n",
    "\n",
    "# Format codes\n",
    "\n",
    "LABELS_FC = \"%Y/%m/%d %H:%M:%S\"\n",
    "TRAJECTORY_FC = \"%Y-%m-%d %H:%M:%S\"\n",
    "\n",
    "\n",
    "class GeoLifeDataLoader:\n",
    "    def __init__(self, path_to_geolife_folder: str | Path) -> None:\n",
    "        self.pfo_geolife = path_to_geolife_folder\n",
    "        pfo_data = Path(self.pfo_geolife) / \"Data\"\n",
    "        self.dir_per_subject: dict[int, Path] = {int(f.name): f for f in pfo_data.iterdir() if f.is_dir()}\n",
    "\n",
    "    def to_pandas_per_device(\n",
    "        self,\n",
    "        device_number: int,\n",
    "        leave_progressbar=True,\n",
    "    ) -> tuple[pd.DataFrame]:\n",
    "        path_to_device_folder = self.dir_per_subject[device_number]\n",
    "        pfo_trajectory = path_to_device_folder / \"Trajectory\"\n",
    "        pfi_labels = path_to_device_folder / \"labels.txt\"\n",
    "        df_labels = None\n",
    "        df_trajectory = None\n",
    "\n",
    "        list_trajectories = [plt_file for plt_file in pfo_trajectory.iterdir() if str(plt_file).endswith(\".plt\")]\n",
    "        list_dfs = []\n",
    "        for traj in tqdm(list_trajectories, leave=leave_progressbar):\n",
    "            df_sourced = pd.read_csv(traj, skiprows=6, names=COLS_TRAJECTORY, usecols=COLS_TRAJECTORY_TO_LOAD)\n",
    "            df_sourced[\"altitude\"] = df_sourced[\"altitude\"].apply(lambda x: x * 0.3048)  # feets to meters\n",
    "            df_sourced[\"timestamp\"] = df_sourced.apply(\n",
    "                lambda x: dt.datetime.strptime(x[\"date\"] + \" \" + x[\"time\"], TRAJECTORY_FC),\n",
    "                axis=1,\n",
    "            )\n",
    "            df_sourced = df_sourced.assign(entity_id=f\"device_{device_number}\", transport=None)\n",
    "            df_sourced = df_sourced[COLS_RESULTS]\n",
    "            list_dfs.append(df_sourced)\n",
    "        df_trajectory = pd.concat(list_dfs)\n",
    "\n",
    "        if pfi_labels.exists():\n",
    "            df_labels = pd.read_csv(pfi_labels, sep=\"\\t\")\n",
    "            df_labels.columns = COLS_LABELS\n",
    "            df_labels[\"start_time\"] = df_labels[\"start_time\"].apply(lambda x: dt.datetime.strptime(x, LABELS_FC))\n",
    "            df_labels[\"end_time\"] = df_labels[\"end_time\"].apply(lambda x: dt.datetime.strptime(x, LABELS_FC))\n",
    "        if df_labels is not None:\n",
    "            for _, row in tqdm(df_labels.iterrows(), leave=leave_progressbar):\n",
    "                mask = (df_trajectory.timestamp > row.start_time) & (df_trajectory.timestamp <= row.end_time)\n",
    "                df_trajectory.loc[mask, \"transport\"] = row[\"transportation_mode\"]\n",
    "\n",
    "        return df_trajectory, df_labels\n",
    "\n",
    "    def to_pandas(self, parse_only_if_labels: bool = False) -> pd.DataFrame:\n",
    "        list_dfs_final = []\n",
    "        for idx in tqdm(self.dir_per_subject.keys()):\n",
    "            df_trajectory, df_labels = self.to_pandas_per_device(idx, leave_progressbar=False)\n",
    "            if parse_only_if_labels:\n",
    "                if df_labels is not None:\n",
    "                    list_dfs_final.append(df_trajectory)\n",
    "                else: \n",
    "                    pass\n",
    "            else:\n",
    "                list_dfs_final.append(df_trajectory)\n",
    "            \n",
    "        return pd.concat(list_dfs_final)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and save to parquet \n",
    "\n",
    "To speed up next loading phase, I save to parquet after loading from the `.plt` files the first time.\n",
    "\n",
    "In this way it will take less time to reload the dataset to continue the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_complete_parquet = path_data_folder / \"GeolifeTrajectories.parquet\"\n",
    "\n",
    "if not path_complete_parquet.exists():\n",
    "    # this takes about 5 minutes\n",
    "    gdl = GeoLifeDataLoader(path_unzipped_dataset)\n",
    "    df_geolife = gdl.to_pandas(parse_only_if_labels=False)\n",
    "    # move outliers\n",
    "    df_geolife = df_geolife[(df_geolife['longitude'] <= 180) & (df_geolife['longitude'] >= -180) & (df_geolife['latitude'] <= 90) & (df_geolife['latitude'] >= -90)].reset_index(drop=True)\n",
    "    # save to parquet\n",
    "    df_geolife.to_parquet(path_complete_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24876977, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity_id</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>altitude</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>transport</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>device_135</td>\n",
       "      <td>39.974294</td>\n",
       "      <td>116.399741</td>\n",
       "      <td>149.9616</td>\n",
       "      <td>2009-01-03 01:21:34</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>device_135</td>\n",
       "      <td>39.974292</td>\n",
       "      <td>116.399592</td>\n",
       "      <td>149.9616</td>\n",
       "      <td>2009-01-03 01:21:35</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>device_135</td>\n",
       "      <td>39.974309</td>\n",
       "      <td>116.399523</td>\n",
       "      <td>149.9616</td>\n",
       "      <td>2009-01-03 01:21:36</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>device_135</td>\n",
       "      <td>39.974320</td>\n",
       "      <td>116.399588</td>\n",
       "      <td>149.9616</td>\n",
       "      <td>2009-01-03 01:21:38</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>device_135</td>\n",
       "      <td>39.974365</td>\n",
       "      <td>116.399730</td>\n",
       "      <td>149.6568</td>\n",
       "      <td>2009-01-03 01:21:39</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    entity_id   latitude   longitude  altitude           timestamp transport\n",
       "0  device_135  39.974294  116.399741  149.9616 2009-01-03 01:21:34      None\n",
       "1  device_135  39.974292  116.399592  149.9616 2009-01-03 01:21:35      None\n",
       "2  device_135  39.974309  116.399523  149.9616 2009-01-03 01:21:36      None\n",
       "3  device_135  39.974320  116.399588  149.9616 2009-01-03 01:21:38      None\n",
       "4  device_135  39.974365  116.399730  149.6568 2009-01-03 01:21:39      None"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this takes 2 seconds (MAC book air, 8GB RAM)\n",
    "df_geolife = pd.read_parquet(path_complete_parquet)\n",
    "print(df_geolife.shape)\n",
    "df_geolife.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
